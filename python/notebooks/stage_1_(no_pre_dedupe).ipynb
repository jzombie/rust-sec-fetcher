{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceaa3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# This snippet ensures consistent import paths across environments.\n",
    "# When running notebooks via JupyterLab's web UI, the current working\n",
    "# directory is often different (e.g., /notebooks) compared to VS Code,\n",
    "# which typically starts at the project root. This handles that by \n",
    "# retrying the import after changing to the parent directory.\n",
    "# \n",
    "# Include this at the top of every notebook to standardize imports\n",
    "# across development environments.\n",
    "\n",
    "try:\n",
    "    from utils.os import chdir_to_git_root\n",
    "except ModuleNotFoundError:\n",
    "    os.chdir(Path.cwd().parent)\n",
    "    print(f\"Retrying import from: {os.getcwd()}\")\n",
    "    from utils.os import chdir_to_git_root\n",
    "\n",
    "chdir_to_git_root(\"python\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98055128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UMAP visualization\n",
    "# umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, metric=\"cosine\")\n",
    "# umap_2d = umap_model.fit_transform(compressed)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(umap_2d[:, 0], umap_2d[:, 1], c=labels, cmap=\"tab10\", s=5)\n",
    "# plt.title(\"Concept/UOM Embeddings Clustered\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e52bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({\n",
    "#     \"concept\": [c for c, _ in concept_unit_pairs],\n",
    "#     \"unit\": [u for _, u in concept_unit_pairs],\n",
    "#     \"cluster\": labels\n",
    "# })\n",
    "# grouped = df.groupby(\"cluster\")\n",
    "\n",
    "# for cluster_id, group in grouped:\n",
    "#     print(f\"\\nCluster {cluster_id} ({len(group)} items):\")\n",
    "#     print(group.head(10).to_string(index=False))\n",
    "\n",
    "# noise = df[df[\"cluster\"] == -1]\n",
    "\n",
    "# print(f\"Noise points: {len(noise)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_points = df[df[\"cluster\"] == -1][[\"concept\", \"unit\"]].reset_index(drop=True)\n",
    "\n",
    "# noise_points.to_csv(\"noise_points.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a051cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Save both embeddings and tuples\n",
    "# np.savez_compressed(\n",
    "#     \"data/stage1_latents.npz\",\n",
    "#     keys=np.array([f\"{c}::{u}\" for c, u in concept_unit_pairs]),\n",
    "#     embeddings=compressed,\n",
    "#     concept_unit_value_tuples=np.array(concept_unit_value_tuples, dtype=object)\n",
    "# )\n",
    "\n",
    "# print(f\"✅ Saved {len(concept_unit_value_tuples):,} tuples and {len(compressed):,} embeddings to 'stage1_latents.npz'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc170ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load saved latent data\n",
    "data = np.load(\"data/stage1_latents.npz\", allow_pickle=True)\n",
    "\n",
    "# Build embedding map\n",
    "embedding_map = {\n",
    "    tuple(key.split(\"::\", 1)): vec\n",
    "    for key, vec in zip(data[\"keys\"], data[\"embeddings\"])\n",
    "}\n",
    "\n",
    "# Load concept-unit-value tuples\n",
    "concept_unit_value_tuples = data[\"concept_unit_value_tuples\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# NOTE: Scaling is performed before train/val split to ensure that every\n",
    "# (concept, unit) pair receives a fitted StandardScaler. If we split first,\n",
    "# some (concept, unit) groups might not appear in the training set at all,\n",
    "# making it impossible to fit their scalers later — leading to missing\n",
    "# or unscalable entries downstream.\n",
    "#\n",
    "# IMPORTANT: The goal of this Stage 1 encoder is not to \"predict\" values,\n",
    "# but to learn meaningful latent representations of (concept, unit, value)\n",
    "# tuples. These embeddings are intended for use in downstream models and\n",
    "# alignment stages, not for direct forecasting or regression tasks.\n",
    "\n",
    "# Step 1: Group values per (concept, unit)\n",
    "grouped = defaultdict(list)\n",
    "for concept, unit, value in concept_unit_value_tuples:\n",
    "    grouped[(concept, unit)].append(value)\n",
    "\n",
    "# Step 2: Fit individual scalers and transform\n",
    "scalers = {}\n",
    "scaled_tuples = []\n",
    "\n",
    "for key, vals in tqdm(grouped.items(), desc=\"Scaling per concept/unit\"):\n",
    "    vals_np = np.array(vals).reshape(-1, 1)\n",
    "\n",
    "    n_quantiles_val = min(len(vals), 1000)\n",
    "    if n_quantiles_val < 2 and len(vals) >= 2:\n",
    "        n_quantiles_val = 2\n",
    "\n",
    "    scaler = QuantileTransformer(\n",
    "        output_distribution='uniform',\n",
    "        n_quantiles=n_quantiles_val,\n",
    "        subsample=len(vals),\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    scaled_vals = scaler.fit_transform(vals_np).flatten()\n",
    "    scalers[key] = scaler\n",
    "\n",
    "    scaled_tuples.extend((key[0], key[1], v) for v in scaled_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b584bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset\n",
    "from utils.pytorch import seed_everything\n",
    "import numpy as np\n",
    "from torch.nn.functional import cosine_similarity, l1_loss\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# from torchmetrics.regression import R2Score\n",
    "from collections import defaultdict\n",
    "\n",
    "class AggregateStats:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self._eps = 1e-8\n",
    "        self._per_tag = defaultdict(lambda: {\n",
    "            \"mae_sum\": 0.0,\n",
    "            \"abs_sum\": 0.0,\n",
    "            # old: \"r2\": R2Score().to(device),\n",
    "            \"n\": 0,\n",
    "            \"sum_y_true\": 0.0,\n",
    "            \"sum_y_pred\": 0.0,\n",
    "            \"sum_y_true2\": 0.0,\n",
    "            \"sum_y_pred2\": 0.0,\n",
    "            \"sum_y_true_y_pred\": 0.0,\n",
    "        })\n",
    "        self.z_sum = 0.0\n",
    "        self.z_sq_sum = 0.0\n",
    "        self.z_count = 0\n",
    "\n",
    "    def update(self, tags, y_pred_batch, y_true_batch, z_norm_batch):\n",
    "        \"\"\"\n",
    "        tags: List[Tuple[str, str]]\n",
    "        y_pred_batch, y_true_batch, z_norm_batch: Tensors of shape [B]\n",
    "        \"\"\"\n",
    "        y_pred_batch = y_pred_batch.detach().cpu()\n",
    "        y_true_batch = y_true_batch.detach().cpu()\n",
    "        z_norm_batch = z_norm_batch.detach().cpu()\n",
    "\n",
    "        for i, tag in enumerate(tags):\n",
    "            stats = self._per_tag[tag]\n",
    "            abs_err = torch.abs(y_pred_batch[i] - y_true_batch[i]).item()\n",
    "            abs_target = torch.abs(y_true_batch[i]).item()\n",
    "\n",
    "            stats[\"mae_sum\"] += abs_err\n",
    "            stats[\"abs_sum\"] += abs_target\n",
    "\n",
    "            # stats[\"r2\"].update(y_pred_batch[i].unsqueeze(0), y_true_batch[i].unsqueeze(0))\n",
    "            stats[\"n\"] += 1\n",
    "            yt = y_true_batch[i].item()\n",
    "            yp = y_pred_batch[i].item()\n",
    "            stats[\"sum_y_true\"] += yt\n",
    "            stats[\"sum_y_pred\"] += yp\n",
    "            stats[\"sum_y_true2\"] += yt * yt\n",
    "            stats[\"sum_y_pred2\"] += yp * yp\n",
    "            stats[\"sum_y_true_y_pred\"] += yt * yp\n",
    "\n",
    "        self.z_sum += z_norm_batch.sum().item()\n",
    "        self.z_sq_sum += (z_norm_batch ** 2).sum().item()\n",
    "        self.z_count += z_norm_batch.size(0)\n",
    "\n",
    "    def median_relative_mae(self):\n",
    "        vals = []\n",
    "        for v in self._per_tag.values():\n",
    "            if v[\"abs_sum\"] > 0:\n",
    "                vals.append(v[\"mae_sum\"] / (v[\"abs_sum\"] + self._eps))\n",
    "        return float(np.median(vals)) if vals else 0.0\n",
    "\n",
    "    def worst_median_relative_mae(self, top_frac=0.05):\n",
    "        vals = []\n",
    "        for v in self._per_tag.values():\n",
    "            if v[\"abs_sum\"] > 0:\n",
    "                rel_mae = v[\"mae_sum\"] / (v[\"abs_sum\"] + self._eps)\n",
    "                vals.append(rel_mae)\n",
    "\n",
    "        if not vals:\n",
    "            return 0.0\n",
    "\n",
    "        vals.sort(reverse=True)  # higher MAE is worse\n",
    "        k = max(1, int(len(vals) * top_frac))\n",
    "        return float(np.median(vals[:k]))\n",
    "\n",
    "    def median_r2(self):\n",
    "        vals = []\n",
    "        for v in self._per_tag.values():\n",
    "            n = v[\"n\"]\n",
    "            if n < 2:\n",
    "                continue\n",
    "            mean_y = v[\"sum_y_true\"] / n\n",
    "            ss_tot = v[\"sum_y_true2\"] - n * mean_y**2\n",
    "            ss_res = (\n",
    "                v[\"sum_y_pred2\"]\n",
    "                - 2 * v[\"sum_y_true_y_pred\"]\n",
    "                + v[\"sum_y_true2\"]\n",
    "            )\n",
    "            r2 = 1.0 - (ss_res / (ss_tot + self._eps))\n",
    "            vals.append(r2)\n",
    "        return float(np.median(vals)) if vals else 0.0\n",
    "\n",
    "\n",
    "    def worst_median_r2(self, bottom_frac=0.05):\n",
    "        vals = []\n",
    "        for v in self._per_tag.values():\n",
    "            n = v[\"n\"]\n",
    "            if n < 2:\n",
    "                continue\n",
    "            mean_y = v[\"sum_y_true\"] / n\n",
    "            ss_tot = v[\"sum_y_true2\"] - n * mean_y**2\n",
    "            ss_res = (\n",
    "                v[\"sum_y_pred2\"]\n",
    "                - 2 * v[\"sum_y_true_y_pred\"]\n",
    "                + v[\"sum_y_true2\"]\n",
    "            )\n",
    "            r2 = 1.0 - (ss_res / (ss_tot + self._eps))\n",
    "            vals.append(r2)\n",
    "        if not vals:\n",
    "            return 0.0\n",
    "        vals.sort()\n",
    "        k = max(1, int(len(vals) * bottom_frac))\n",
    "        return float(np.median(vals[:k]))\n",
    "\n",
    "\n",
    "    def z_norm_mean_std(self):\n",
    "        if self.z_count == 0:\n",
    "                return 0.0, 0.0\n",
    "\n",
    "        mean = self.z_sum / max(self.z_count, 1)\n",
    "        mean_sq = self.z_sq_sum / max(self.z_count, 1)\n",
    "        var = max(mean_sq - mean**2, 0.0)\n",
    "        return mean, var**0.5\n",
    "\n",
    "    def reset(self):\n",
    "        self.__init__(self.device)\n",
    "\n",
    "\n",
    "\n",
    "# Stage 1 dataset: concept+uom embedding + value\n",
    "class ConceptValueDataset(Dataset):\n",
    "    def __init__(self, scaled_tuples, embedding_lookup, device: torch.tensor, scalers=None, return_scaler=False):\n",
    "        \"\"\"\n",
    "        Dataset for (concept, unit, value) triplets with optional per-sample scaler.\n",
    "\n",
    "        :param scaled_tuples: List of (concept, unit, scaled_value) tuples\n",
    "        :param embedding_lookup: Dict[(concept, unit)] -> embedding np.array\n",
    "        :param device: torch device tensor to place tensors on\n",
    "        :param scalers: Optional dict of (concept, unit) -> QuantileTransformer\n",
    "        :param return_scaler: If True, return the scaler used per sample\n",
    "        \"\"\"\n",
    "        self.rows = scaled_tuples\n",
    "        self.lookup = embedding_lookup\n",
    "        self.device = device\n",
    "        self.scalers = scalers\n",
    "        self.return_scaler = return_scaler\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        concept, unit, value = self.rows[idx]\n",
    "\n",
    "        try:\n",
    "            embedding = self.lookup[(concept, unit)]\n",
    "        except KeyError:\n",
    "            raise ValueError(f\"Missing embedding for ({concept}, {unit})\")\n",
    "\n",
    "        x = torch.tensor(np.concatenate([embedding, [value]]), dtype=torch.float32,\n",
    "                         device=self.device)\n",
    "\n",
    "        # For autoencoders, target y is typically the same as input x\n",
    "        y = x.clone() \n",
    "\n",
    "        if self.return_scaler:\n",
    "            scaler_obj = self.scalers.get((concept, unit))\n",
    "            return x, y, scaler_obj, (concept, unit)\n",
    "        return x, y, (concept, unit)\n",
    "\n",
    "\n",
    "def collate_with_scaler(batch):\n",
    "    xs, ys, scalers_list, concept_units = zip(*batch)\n",
    "    return torch.stack(xs), torch.stack(ys), scalers_list, list(concept_units)\n",
    "\n",
    "\n",
    "# LightningModule\n",
    "class Stage1Autoencoder(pl.LightningModule):\n",
    "    EPSILON = torch.finfo(torch.float32).eps \n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim=244,\n",
    "            latent_dim=128,\n",
    "            encoder_dropout_rate=0.0,\n",
    "            value_dropout_rate=0.0,\n",
    "            # lr=0.00023072200683712404,\n",
    "            lr=5e-5,\n",
    "            min_lr=1e-6,\n",
    "            # lr=0.00002307,\n",
    "            batch_size=64,\n",
    "            gradient_clip=0.5,\n",
    "            alpha_embed=1.0,\n",
    "            alpha_value=1.0,\n",
    "            embedding_noise_std=0.0, # 0.02 is roughly ~0.951 cosine sim difference for 243 dimeensions; 0.01 is roughly ~0.99\n",
    "            weight_decay=5.220603379116996e-07,\n",
    "            lr_annealing_epochs=40\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters(ignore=['median_scaled_val', 'mean_emb'])\n",
    "\n",
    "         # --> Register mean_s and mean_emb as buffers <--\n",
    "        # self.register_buffer(\"median_scaled_val\", median_scaled_val)\n",
    "        # self.register_buffer(\"mean_emb\", mean_emb)\n",
    "\n",
    "        # self.value_proj = nn.Sequential(\n",
    "        #     nn.Linear(1, 32),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(32, self.hparams.latent_dim),\n",
    "        #     nn.LayerNorm(self.hparams.latent_dim)\n",
    "        # )\n",
    "\n",
    "        # May 1, 2025 original\n",
    "        self.value_proj = nn.Sequential(\n",
    "            nn.Linear(1, 32),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, latent_dim),\n",
    "            nn.LayerNorm(latent_dim)\n",
    "        )\n",
    "\n",
    "        # self.value_proj = nn.Sequential(\n",
    "        #     nn.Linear(1, 32),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(32, 64),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(64, latent_dim),\n",
    "        #     nn.LayerNorm(latent_dim)\n",
    "        # )\n",
    "\n",
    "        # self.attended_interaction = nn.Sequential(\n",
    "        #     nn.Linear(latent_dim * 2, latent_dim * 2),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(latent_dim * 2, latent_dim),\n",
    "        #     nn.LayerNorm(latent_dim)\n",
    "        # )\n",
    "\n",
    "        # self.encoder = nn.Sequential(\n",
    "        #     nn.Linear(input_dim - 1 + self.hparams.latent_dim, 256),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Dropout(p=dropout_rate),\n",
    "        #     nn.Linear(256, latent_dim)\n",
    "        # )\n",
    "\n",
    "        # self.gate = nn.Sequential(\n",
    "        #     nn.Linear(latent_dim * 2, latent_dim),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(latent_dim, latent_dim),\n",
    "        #     nn.Sigmoid()\n",
    "        # )\n",
    "\n",
    "        # self.fusion_logits = nn.Parameter(torch.zeros(3))\n",
    "        # self.fusion_dim = latent_dim * 3\n",
    "        # self.post_fusion_norm = nn.LayerNorm(self.fusion_dim)\n",
    "\n",
    "        self.joint_input_dim = input_dim - 1 + latent_dim\n",
    "        # self.joint_input_norm = nn.LayerNorm(self.joint_input_dim)\n",
    "\n",
    "        # self.encoder = nn.Sequential(\n",
    "        #     nn.Linear(self.joint_input_dim, latent_dim * 4),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Dropout(p=dropout_rate),\n",
    "        #     nn.Linear(latent_dim * 4, latent_dim * 2),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Dropout(p=dropout_rate),\n",
    "        #     nn.Linear(latent_dim * 2, latent_dim)\n",
    "        # )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            # nn.Linear(self.joint_input_dim, latent_dim * 4),\n",
    "            # nn.GELU(),\n",
    "            # nn.LayerNorm(latent_dim * 4),\n",
    "            # nn.Dropout(p=encoder_dropout_rate),\n",
    "\n",
    "            nn.Linear(self.joint_input_dim, latent_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(latent_dim * 2),\n",
    "            nn.Dropout(p=encoder_dropout_rate),\n",
    "\n",
    "            nn.Linear(latent_dim * 2, latent_dim)  # Bottleneck\n",
    "        )\n",
    "        \n",
    "        self.embedding_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, (input_dim - 1) * 2),\n",
    "            nn.GELU(),\n",
    "            # nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear((input_dim - 1) * 2, input_dim - 1)\n",
    "        )\n",
    "\n",
    "        # May 1, 2025 original\n",
    "        self.value_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=value_dropout_rate),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "        # self.value_decoder = nn.Sequential(\n",
    "        #     nn.Linear(latent_dim, 64),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Dropout(p=value_dropout_rate),\n",
    "        #     nn.Linear(64, 32),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(32, 1)\n",
    "        # )\n",
    "\n",
    "        # self.loss_fn = nn.MSELoss()\n",
    "        self.loss_fn = nn.L1Loss() # MAELoss\n",
    "\n",
    "        self._agg_train_stats = self.create_aggregate_stats()\n",
    "        self._agg_val_stats = self.create_aggregate_stats()\n",
    "\n",
    "    def create_aggregate_stats(self):\n",
    "        return AggregateStats(self.device)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # x shape: [batch_size, input_dim]\n",
    "        x_emb = x[:, :-1] # Non-scaled embeddings\n",
    "        x_val = x[:, -1].unsqueeze(1) # Scaled values\n",
    "\n",
    "        # Inject Gaussian noise into embedding (during training only)\n",
    "        if self.training:\n",
    "            x_emb = x_emb + torch.randn_like(x_emb) * self.hparams.embedding_noise_std\n",
    "\n",
    "       \n",
    "        val_proj = self.value_proj(x_val)\n",
    "\n",
    "        joint_input = torch.cat([x_emb, val_proj], dim=1)\n",
    "        # joint_input = self.joint_input_norm(joint_input)\n",
    "\n",
    "        z = self.encoder(joint_input)\n",
    "\n",
    "        # Apply L2 normalization along the feature dimension (dim=1)\n",
    "        # p=2 is the default for L2 norm, but explicitly stated for clarity\n",
    "        z = F.normalize(z, p=2, dim=1)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        recon_emb = self.embedding_decoder(z)\n",
    "        recon_val = self.value_decoder(z)\n",
    "        \n",
    "        return recon_emb, recon_val\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        \n",
    "        recon_emb, recon_val = self.decode(z)\n",
    "        return recon_emb, recon_val, z\n",
    "    \n",
    "    def compute_losses(self, x, target, scaler, concept_units, train=False):\n",
    "        recon_emb, recon_val, z = self(x)\n",
    "\n",
    "        target_emb = target[:, :-1]\n",
    "        target_val = target[:, -1].unsqueeze(1)\n",
    "\n",
    "        if scaler and isinstance(scaler, (list, tuple)):\n",
    "            recon_val_np = recon_val.detach().cpu().numpy()\n",
    "            target_val_np = target_val.detach().cpu().numpy()\n",
    "\n",
    "            # Inverse transform per sample\n",
    "            recon_val_orig = np.stack([\n",
    "                s.inverse_transform(r.reshape(-1, 1)).flatten()\n",
    "                for s, r in zip(scaler, recon_val_np)\n",
    "            ])\n",
    "            target_val_orig = np.stack([\n",
    "                s.inverse_transform(t.reshape(-1, 1)).flatten()\n",
    "                for s, t in zip(scaler, target_val_np)\n",
    "            ])\n",
    "\n",
    "            recon_val_orig = torch.tensor(recon_val_orig, dtype=torch.float32,\n",
    "                                        device=recon_val.device)\n",
    "            target_val_orig = torch.tensor(target_val_orig, dtype=torch.float32,\n",
    "                                        device=target_val.device)\n",
    "        else:\n",
    "            raise Exception(\"Scaler not implemented\")\n",
    "\n",
    "        # non-scaled\n",
    "        embedding_loss = self.loss_fn(recon_emb, target_emb)\n",
    "\n",
    "        # scaled\n",
    "        value_loss = self.loss_fn(recon_val, target_val)\n",
    "    \n",
    "        total_loss = (\n",
    "            self.hparams.alpha_embed * embedding_loss +\n",
    "            self.hparams.alpha_value * value_loss\n",
    "        )\n",
    "\n",
    "        # non-scaled\n",
    "        cos_sim_emb = cosine_similarity(recon_emb, target_emb, dim=1).mean()\n",
    "        euclidean_dist_emb = torch.norm(recon_emb - target_emb, dim=1).mean()\n",
    "\n",
    "        # non-scaled\n",
    "        z_norm = torch.norm(z, dim=1)\n",
    "\n",
    "        agg_stats = self._agg_train_stats if train else self._agg_val_stats\n",
    "        agg_stats.update(\n",
    "            tags=concept_units,\n",
    "            y_pred_batch=recon_val_orig.view(-1),\n",
    "            y_true_batch=target_val_orig.view(-1),\n",
    "            z_norm_batch=z_norm\n",
    "        )\n",
    "\n",
    "        relative_mae_value = agg_stats.median_relative_mae()\n",
    "        worst_relative_mae_value = agg_stats.worst_median_relative_mae()\n",
    "\n",
    "        r2_value = agg_stats.median_r2()\n",
    "        worst_r2_value = agg_stats.worst_median_r2()\n",
    "\n",
    "        z_norm_mean, z_norm_std = agg_stats.z_norm_mean_std()\n",
    "\n",
    "        return total_loss, embedding_loss, value_loss, cos_sim_emb, euclidean_dist_emb, relative_mae_value, worst_relative_mae_value, r2_value, worst_r2_value, z_norm_mean, z_norm_std\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        if len(batch) == 4:\n",
    "            x, target, scaler, concept_units = batch\n",
    "        elif len(batch) == 3:\n",
    "            x, target, scaler = batch\n",
    "            concept_units = None\n",
    "        else:\n",
    "            x, target = batch\n",
    "            scaler = None\n",
    "            concept_units = None\n",
    "\n",
    "        total_loss, embedding_loss, value_loss, cos_sim_emb, euclidean_dist_emb, relative_mae_value, worst_relative_mae_value, r2_value, worst_r2_value, z_norm_mean, z_norm_std = (\n",
    "            self.compute_losses(x, target, scaler, concept_units, train=True)\n",
    "        )\n",
    "\n",
    "        self.log(\"train_loss\", total_loss, prog_bar=True, batch_size=self.hparams.batch_size)\n",
    "        # self.log(\"train_overlap_loss\", overlap_loss, prog_bar=True, batch_size=self.hparams.batch_size)\n",
    "        self.log(\"train_embedding_loss\", embedding_loss, batch_size=self.hparams.batch_size)\n",
    "        self.log(\"train_value_loss\", value_loss, batch_size=self.hparams.batch_size)\n",
    "        self.log(\"train_embedding_cos_sim\", cos_sim_emb, batch_size=self.hparams.batch_size)\n",
    "        self.log(\"train_embedding_euclidean\", euclidean_dist_emb, batch_size=self.hparams.batch_size)\n",
    "        self.log(\"train_value_relative_mae_running\", relative_mae_value, batch_size=self.hparams.batch_size)\n",
    "        self.log(\"train_worst_value_relative_mae_running\", worst_relative_mae_value, batch_size=self.hparams.batch_size)\n",
    "        self.log(\"train_value_r2_running\", r2_value, batch_size=self.hparams.batch_size)\n",
    "        self.log(\"train_worst_value_r2_running\", worst_r2_value, batch_size=self.hparams.batch_size)\n",
    "        self.log(\"train_z_norm_mean\", z_norm_mean, batch_size=self.hparams.batch_size)\n",
    "        self.log(\"train_z_norm_std\", z_norm_std, batch_size=self.hparams.batch_size)\n",
    "\n",
    "        self.log(\"train_loss_epoch\", total_loss, prog_bar=True, on_step=False, on_epoch=True, logger=True, batch_size=self.hparams.batch_size)\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        if len(batch) == 4:\n",
    "            x, target, scaler, concept_units = batch\n",
    "        elif len(batch) == 3:\n",
    "            x, target, scaler = batch\n",
    "            concept_units = None\n",
    "        else:\n",
    "            x, target = batch\n",
    "            scaler = None\n",
    "            concept_units = None\n",
    "\n",
    "        total_loss, embedding_loss, value_loss, cos_sim_emb, euclidean_dist_emb, relative_mae_value, worst_relative_mae_value, r2_value, worst_r2_value, z_norm_mean, z_norm_std = (\n",
    "            self.compute_losses(x, target, scaler, concept_units, train=False)\n",
    "        )\n",
    "\n",
    "        self.log(\"val_loss\", total_loss, prog_bar=True, batch_size=self.hparams.batch_size)\n",
    "        # self.log(\"val_overlap_loss\", overlap_loss, prog_bar=True, batch_size=self.hparams.batch_size)\n",
    "        self.log(\"val_embedding_loss\", embedding_loss, batch_size=self.hparams.batch_size)\n",
    "        self.log(\"val_value_loss\", value_loss, batch_size=self.hparams.batch_size)\n",
    "        self.log(\"val_embedding_cos_sim\", cos_sim_emb, batch_size=self.hparams.batch_size)\n",
    "        self.log(\"val_embedding_euclidean\", euclidean_dist_emb, batch_size=self.hparams.batch_size)\n",
    "        self.log(\"val_value_relative_mae_running\", relative_mae_value, batch_size=self.hparams.batch_size)\n",
    "        self.log(\"val_worst_value_relative_mae_running\", worst_relative_mae_value, batch_size=self.hparams.batch_size)\n",
    "        self.log(\"val_value_r2_running\", r2_value, batch_size=self.hparams.batch_size)\n",
    "        self.log(\"val_worst_value_r2_running\", worst_r2_value, batch_size=self.hparams.batch_size)\n",
    "        self.log(\"val_z_norm_mean\", z_norm_mean, batch_size=self.hparams.batch_size)\n",
    "        self.log(\"val_z_norm_std\", z_norm_std, batch_size=self.hparams.batch_size)\n",
    "\n",
    "        self.log(\"val_loss_epoch\", total_loss, prog_bar=True, on_step=False, on_epoch=True, logger=True, batch_size=self.hparams.batch_size)\n",
    "        return total_loss\n",
    "\n",
    "    # Note: PyTorch Lightning doesn't support logging from `on_train_epoch_start`. Use `on_train_epoch_end` for logging, instead.\n",
    "    def on_train_epoch_start(self):\n",
    "        self._agg_train_stats.reset()\n",
    "\n",
    "        print(\"Current LR: \", self.get_current_lr())\n",
    "\n",
    "    def on_validation_start(self):\n",
    "        self._agg_val_stats.reset()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        # Log learning rate of first param group\n",
    "        current_lr = self.get_current_lr()\n",
    "        self.log(\"lr_adjusted\", current_lr, prog_bar=True)\n",
    "\n",
    "    def get_current_lr(self):\n",
    "        current_lr = self.trainer.optimizers[0].param_groups[0][\"lr\"]\n",
    "        return current_lr\n",
    "\n",
    "    # def configure_optimizers(self):\n",
    "    #     return torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        \n",
    "        # Use CosineAnnealingLR with T_max=15 and eta_min=1e-6 (matches your 15 epochs)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=self.hparams.lr_annealing_epochs, eta_min=self.hparams.min_lr)\n",
    "\n",
    "        # TODO: Replace scheduler with CosineAnnealingWarmRestarts(optimizer, T_0=15, T_mult=1)?\n",
    "        \n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccdb991-b887-4847-968b-8d9fb4b9517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning\n",
    "\n",
    "# import os\n",
    "# import optuna\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "# from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from torch.utils.data import DataLoader\n",
    "# from utils.pytorch import get_device\n",
    "\n",
    "# device = get_device()\n",
    "\n",
    "# # === CONFIG ===\n",
    "# OUTPUT_PATH = \"data/stage1\"\n",
    "# os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "# OPTUNA_DB_PATH = os.path.join(OUTPUT_PATH, \"optuna_study.db\")\n",
    "# EPOCHS = 3\n",
    "# PATIENCE = 5\n",
    "# VAL_SPLIT = 0.2\n",
    "\n",
    "# def objective(trial):\n",
    "#     batch_size = trial.suggest_int(\"batch_size\", 8, 64, step=8)\n",
    "#     lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "#     latent_dim = trial.suggest_int(\"latent_dim\", 32, 128, step=32)\n",
    "#     dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.2, step=0.1)\n",
    "#     weight_decay = trial.suggest_float(\"weight_decay\", 1e-8, 1e-4, log=True)\n",
    "#     gradient_clip = trial.suggest_float(\"gradient_clip\", 0.0, 1.0, step=0.1)\n",
    "\n",
    "#     # # 80/20 Train/Val Split\n",
    "#     # split = int(len(scaled_tuples) * (1 - VAL_SPLIT))\n",
    "#     # train_data = scaled_tuples[:split]\n",
    "#     # val_data = scaled_tuples[split:]\n",
    "\n",
    "#      # === Sample Subset for Faster Debugging ===\n",
    "#     SAMPLE_SIZE = 500_000\n",
    "#     subset = scaled_tuples[:SAMPLE_SIZE]\n",
    "    \n",
    "#     # 80/20 Train/Val Split\n",
    "#     split = int(len(subset) * (1 - VAL_SPLIT))\n",
    "#     train_data = subset[:split]\n",
    "#     val_data = subset[split:]\n",
    "\n",
    "#     train_loader = DataLoader(\n",
    "#         ConceptValueDataset(train_data, embedding_map, device=device, value_noise_std=0.005, train=True),\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=True\n",
    "#     )\n",
    "    \n",
    "#     val_loader = DataLoader(\n",
    "#         ConceptValueDataset(val_data, embedding_map, device=device, value_noise_std=0.00, train=False),\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=False\n",
    "#     )\n",
    "\n",
    "#     input_dim = len(next(iter(embedding_map.values()))) + 1\n",
    "\n",
    "#     model = Stage1Autoencoder(\n",
    "#         input_dim=input_dim,\n",
    "#         latent_dim=latent_dim,\n",
    "#         dropout_rate=dropout_rate,\n",
    "#         lr=lr,\n",
    "#         batch_size=batch_size,\n",
    "#         weight_decay=weight_decay,\n",
    "#         gradient_clip=gradient_clip\n",
    "#     )\n",
    "\n",
    "#     early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, verbose=True, mode=\"min\")\n",
    "\n",
    "#     model_checkpoint = ModelCheckpoint(\n",
    "#         dirpath=OUTPUT_PATH,\n",
    "#         filename=\"best_model_trial_{trial.number}\",\n",
    "#         monitor=\"val_loss\",\n",
    "#         mode=\"min\",\n",
    "#         save_top_k=1,\n",
    "#         verbose=True\n",
    "#     )\n",
    "\n",
    "#     trainer = pl.Trainer(\n",
    "#         max_epochs=EPOCHS,\n",
    "#         logger=TensorBoardLogger(OUTPUT_PATH, name=\"stage1_autoencoder\"),\n",
    "#         callbacks=[early_stop_callback, model_checkpoint],\n",
    "#         accelerator=\"auto\",\n",
    "#         devices=1,\n",
    "#         gradient_clip_val=gradient_clip\n",
    "#     )\n",
    "\n",
    "#     trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "#     return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "# # === Optuna Study ===\n",
    "# study = optuna.create_study(direction=\"minimize\",\n",
    "#                             storage=f\"sqlite:///{OPTUNA_DB_PATH}\",\n",
    "#                             load_if_exists=True)\n",
    "# study.optimize(objective, n_trials=25)\n",
    "\n",
    "# print(\"Best params:\", study.best_params)\n",
    "# print(\"Best trial value:\", study.best_trial.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c278a0d0-19d7-47ca-95ba-5d972553b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Instantiate dataset\n",
    "# dataset = ConceptValueDataset(scaled_tuples, embedding_map)\n",
    "\n",
    "# # Sample inspection\n",
    "# sample_x, sample_y = dataset[0]\n",
    "# print(\"Sample input:\", sample_x)\n",
    "# print(\"Min:\", sample_x.min().item(), \"Max:\", sample_x.max().item())\n",
    "# print(\"Mean:\", sample_x.mean().item(), \"Std:\", sample_x.std().item())\n",
    "# print(\"Input dim:\", sample_x.shape[0], \"Target dim:\", sample_y.shape[0])\n",
    "\n",
    "# # Optional: test batch loading\n",
    "# loader = DataLoader(dataset, batch_size=4)\n",
    "# for xb, yb in loader:\n",
    "#     print(\"Batch shape:\", xb.shape)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c250f332-069a-4a69-af69-fc73b521529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split # Make sure this is imported\n",
    "import numpy as np # For potential use, though train_test_split handles counts\n",
    "\n",
    "# Target fraction for the validation set from each group\n",
    "VALIDATION_FRACTION = 0.25\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Assume `scaled_tuples` is the list of (concept, unit, scaled_value)\n",
    "# generated by your previous preprocessing script. It contains ALL your scaled data.\n",
    "\n",
    "# 1. Training data will be 100% of scaled_tuples.\n",
    "# We create a list copy for clarity, though direct use is also possible.\n",
    "# The DataLoader for training will typically shuffle this.\n",
    "train_data = list(scaled_tuples)\n",
    "\n",
    "# 2. Validation data will be a stratified sample, aiming for ~25% of each group's items.\n",
    "val_data = []\n",
    "\n",
    "# Group all tuples by (concept, unit) to perform stratified sampling for the validation set.\n",
    "# This grouping is based on the full scaled_tuples list.\n",
    "grouped_for_val_sampling = defaultdict(list)\n",
    "for concept, unit, value in scaled_tuples: # Iterate over the full dataset\n",
    "    grouped_for_val_sampling[(concept, unit)].append(value)\n",
    "\n",
    "# For each group, select a VALIDATION_FRACTION sample of its values to contribute to val_data.\n",
    "# These samples will also be part of the 100% train_data.\n",
    "for (concept, unit), values_in_group in grouped_for_val_sampling.items():\n",
    "    if not values_in_group: # Skip if group is empty\n",
    "        continue\n",
    "\n",
    "    num_samples_in_group = len(values_in_group)\n",
    "    group_val_values = [] # To store validation values from this specific group\n",
    "\n",
    "    if num_samples_in_group == 1:\n",
    "        # If a group has only one sample, and we want validation data (VALIDATION_FRACTION > 0),\n",
    "        # we include this single sample in the validation set.\n",
    "        # This means all single-sample groups will be represented in this stratified validation set.\n",
    "        if VALIDATION_FRACTION > 0:\n",
    "            group_val_values = list(values_in_group) # Take the single value\n",
    "    elif num_samples_in_group > 1:\n",
    "        # For groups with more than one sample, use train_test_split to get\n",
    "        # the desired fraction for the validation set from this group.\n",
    "        # The '_group_train_dummy' part is not used for constructing val_data here.\n",
    "        # We ensure shuffling before split if not done by train_test_split, but it shuffles by default.\n",
    "        _group_train_dummy, sampled_val_values_for_group = train_test_split(\n",
    "            values_in_group,       # Values from the current (concept, unit) group\n",
    "            test_size=VALIDATION_FRACTION,\n",
    "            random_state=RANDOM_STATE,\n",
    "            shuffle=True          # Ensure shuffling for random sampling within the group\n",
    "        )\n",
    "        group_val_values = sampled_val_values_for_group\n",
    "    \n",
    "    # Extend the global val_data list with the (concept, unit, value) tuples from this group's sample\n",
    "    if group_val_values: # If any values were selected for validation from this group\n",
    "        val_data.extend([(concept, unit, v) for v in group_val_values])\n",
    "\n",
    "print(f\"Total items for training (100%): {len(train_data)}\")\n",
    "print(f\"Total items for validation (stratified ~25% sample): {len(val_data)}\")\n",
    "\n",
    "# Now you have:\n",
    "# - `train_data`: A list of all your (concept, unit, scaled_value) tuples (100%).\n",
    "# - `val_data`: A list containing a stratified sample, where each (concept, unit) group\n",
    "#               contributes approximately 25% of its items to this validation set.\n",
    "#               The total size will be roughly 25% of the original dataset, but may vary\n",
    "#               slightly due to per-group integer rounding and handling of small groups.\n",
    "\n",
    "# You would then use `train_data` and `val_data` in your PyTorch DataLoaders.\n",
    "# The `train_loader` will use `train_data` (100%) with shuffle=True.\n",
    "# The `val_loader` will use `val_data` (the ~25% stratified sample) with shuffle=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c5e738-ff3a-4e99-bdfb-0479288faed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging, only\n",
    "\n",
    "# Example: keep only 10,000 training and 2,000 validation samples\n",
    "# train_data = train_data[:10_000]\n",
    "# val_data = val_data[:2_000]\n",
    "\n",
    "# print(f\"Truncated train_data: {len(train_data)}\")\n",
    "# print(f\"Truncated val_data: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd4f758-e1d6-4e8b-bbb2-0060ffbdadc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from utils.pytorch import get_device # Assuming you have this\n",
    "\n",
    "# device = get_device() # Make sure device is defined\n",
    "\n",
    "# # Define Median Scaled Value (which is 0.0 after RobustScaler)\n",
    "# median_scaled_val_tensor = torch.tensor(0.0, dtype=torch.float32, device=device)\n",
    "# print(f\"Using median_scaled_val: {median_scaled_val_tensor.item()}\")\n",
    "\n",
    "# # === Calculate Mean Embedding (mean_emb) ===\n",
    "# # (Calculation for mean_emb_tensor remains the same as before)\n",
    "# # Get all unique (concept, unit) keys from train_data\n",
    "# train_keys = set((item[0], item[1]) for item in train_data)\n",
    "# # Get the corresponding embeddings\n",
    "# train_embeddings = [embedding_map[key] for key in train_keys if key in embedding_map]\n",
    "\n",
    "# if not train_embeddings:\n",
    "#     raise ValueError(\"No valid embeddings found for training data keys!\")\n",
    "\n",
    "# # Stack embeddings into a numpy array and calculate the mean vector\n",
    "# train_embeddings_np = np.stack(train_embeddings)\n",
    "# mean_emb_numpy = np.mean(train_embeddings_np, axis=0)\n",
    "# # Convert to a tensor on the correct device\n",
    "# mean_emb_tensor = torch.tensor(mean_emb_numpy, dtype=torch.float32, device=device)\n",
    "# print(f\"Calculated mean_emb shape: {mean_emb_tensor.shape}\")\n",
    "# print(f\"Calculated mean_emb value (first 10 elements): {mean_emb_tensor[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47484ee4-1c73-48be-8024-e368d977c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "import os\n",
    "import optuna\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.pytorch import get_device\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "# === CONFIG ===\n",
    "OUTPUT_PATH = \"data/stage1_23_(no_pre_dedupe)\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "OPTUNA_DB_PATH = os.path.join(OUTPUT_PATH, \"optuna_study.db\")\n",
    "EPOCHS = 1000\n",
    "PATIENCE = 20 # Compensate for long annealing period + some\n",
    "\n",
    "# ckpt_path = f\"{OUTPUT_PATH}/stage1_resume.ckpt\"\n",
    "# ckpt_path = f\"{OUTPUT_PATH}/manual_resumed_checkpoint.ckpt\"\n",
    "ckpt_path = None\n",
    "\n",
    "# model = Stage1Autoencoder.load_from_checkpoint(ckpt_path)\n",
    "model = Stage1Autoencoder()\n",
    "\n",
    "batch_size = model.hparams.batch_size\n",
    "gradient_clip = model.hparams.gradient_clip\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     ConceptValueDataset(train_data, embedding_map, device=device, value_noise_std=0.005, train=True),\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# val_loader = DataLoader(\n",
    "#     ConceptValueDataset(val_data, embedding_map, device=device, value_noise_std=0.00, train=False),\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False\n",
    "# )\n",
    "train_loader = DataLoader(\n",
    "    ConceptValueDataset(\n",
    "        train_data,\n",
    "        embedding_map,\n",
    "        device=device,\n",
    "        scalers=scalers,\n",
    "        return_scaler=True\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_with_scaler\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    ConceptValueDataset(\n",
    "        val_data,\n",
    "        embedding_map,\n",
    "        device=device,\n",
    "        scalers=scalers,\n",
    "        return_scaler=True\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_with_scaler\n",
    ")\n",
    "\n",
    "\n",
    "input_dim = len(next(iter(embedding_map.values()))) + 1\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss_epoch\", patience=PATIENCE, verbose=True, mode=\"min\")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    dirpath=OUTPUT_PATH,\n",
    "    filename=\"stage1_resume\",\n",
    "    monitor=\"val_loss_epoch\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    logger=TensorBoardLogger(OUTPUT_PATH, name=\"stage1_autoencoder\"),\n",
    "    callbacks=[early_stop_callback, model_checkpoint],\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    gradient_clip_val=gradient_clip,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    #\n",
    "    # ckpt_path=ckpt_path # TODO: Uncomment if resuming training\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5769c4-97ea-4638-a2bc-de631ce1f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_checkpoint(f\"{OUTPUT_PATH}/manual_resumed_checkpoint.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca87a09-65c9-42ee-9310-42558e6df0dc",
   "metadata": {},
   "source": [
    "# Conceptual Draft\n",
    "\n",
    "Stage 1 learns semantic+quantitative embeddings for individual concept/unit/value triplets.\n",
    "\n",
    "Stage 2 learns how to aggregate and contextualize those embeddings into higher-order units (i.e., financial statements).\n",
    "\n",
    "Stage 3 learns how to model temporal dynamics and structural evolution across filings — a full hierarchy of understanding.\n",
    "\n",
    "This pipeline could encode an entire company's financial narrative into vector space.\n",
    "\n",
    "It’s structured like language modeling, but for accounting — and that’s what makes it potentially groundbreaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70426775-f40b-4b32-ac8f-2957ae841798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# # Where correlation matrix is on the full z, and the `corr_value` is derived specifically from the input value dimension\n",
    "# # IMPORTANT: This should only be used with this \"stage 1\" model\n",
    "# def analyze_latent_correlation_matrix_streaming(model, val_loader, device):\n",
    "#     model.eval()\n",
    "#     model.to(device)\n",
    "\n",
    "#     latent_dim = model.hparams.latent_dim\n",
    "\n",
    "#     count = 0\n",
    "#     mean_z = np.zeros(latent_dim)\n",
    "#     m2_z = np.zeros(latent_dim)\n",
    "#     cov_z = np.zeros((latent_dim, latent_dim))\n",
    "\n",
    "#     # For value correlation\n",
    "#     mean_val = 0.0\n",
    "#     m2_val = 0.0\n",
    "#     cov_val = np.zeros(latent_dim)\n",
    "\n",
    "#     for batch in val_loader:\n",
    "#         x, y, _ = batch\n",
    "#         x = x.to(device)\n",
    "#         y = y.to(device)\n",
    "\n",
    "#         z = model.encode(x).detach().cpu().numpy()  # [B, D]\n",
    "#         v = y[:, -1].detach().cpu().numpy()         # [B]\n",
    "\n",
    "#         for zi, vi in zip(z, v):\n",
    "#             count += 1\n",
    "\n",
    "#             # === Update latent stats (Welford) ===\n",
    "#             delta_z = zi - mean_z\n",
    "#             mean_z += delta_z / count\n",
    "#             m2_z += delta_z * (zi - mean_z)\n",
    "\n",
    "#             # === Update cov_z (outer product) ===\n",
    "#             cov_z += np.outer(delta_z, zi - mean_z)\n",
    "\n",
    "#             # === Update value stats ===\n",
    "#             delta_v = vi - mean_val\n",
    "#             mean_val += delta_v / count\n",
    "#             m2_val += delta_v * (vi - mean_val)\n",
    "\n",
    "#             # === Update cov_val ===\n",
    "#             cov_val += delta_z * (vi - mean_val)\n",
    "\n",
    "#         # break\n",
    "\n",
    "#     var_z = m2_z / (count - 1)\n",
    "#     var_val = m2_val / (count - 1)\n",
    "#     cov_z /= (count - 1)\n",
    "#     cov_val /= (count - 1)\n",
    "\n",
    "#     std_z = np.sqrt(var_z + 1e-8)\n",
    "#     std_val = np.sqrt(var_val + 1e-8)\n",
    "\n",
    "#     corr_matrix = cov_z / (std_z[:, None] * std_z[None, :])\n",
    "#     corr_value = cov_val / (std_z * std_val)\n",
    "\n",
    "#     return corr_matrix, corr_value\n",
    "\n",
    "\n",
    "# print(\"\\n=== Computing latent correlation matrix... ===\")\n",
    "# corr_matrix, corr_value = analyze_latent_correlation_matrix_streaming(model, val_loader, device=device)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_correlation_matrix(corr_matrix, title=\"Latent Dimension Correlation Matrix\"):\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     plt.imshow(corr_matrix, cmap='coolwarm', interpolation='nearest', vmin=-1, vmax=1)\n",
    "#     plt.colorbar(shrink=0.5)\n",
    "#     plt.title(title)\n",
    "#     plt.xlabel(\"Latent Dim\")\n",
    "#     plt.ylabel(\"Latent Dim\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# plot_correlation_matrix(corr_matrix)\n",
    "\n",
    "# top_k = 128\n",
    "\n",
    "# def print_top_latent_correlations(corr_matrix, top_k=top_k):\n",
    "#     dim = corr_matrix.shape[0]\n",
    "#     pairs = []\n",
    "\n",
    "#     for i in range(dim):\n",
    "#         for j in range(i + 1, dim):\n",
    "#             corr = corr_matrix[i, j]\n",
    "#             pairs.append(((i, j), corr))\n",
    "\n",
    "#     top_corrs = sorted(pairs, key=lambda x: -abs(x[1]))[:top_k]\n",
    "\n",
    "#     print(f\"\\nTop {top_k} most correlated latent dimension pairs:\")\n",
    "#     for (i, j), corr in top_corrs:\n",
    "#         print(f\"z[{i:03d}] ↔ z[{j:03d}]: corr = {corr:.4f}\")\n",
    "\n",
    "\n",
    "# print_top_latent_correlations(corr_matrix, top_k=top_k)\n",
    "\n",
    "\n",
    "# top_dims = sorted(enumerate(corr_value), key=lambda x: -abs(x[1]))[:top_k]\n",
    "# print(\"\\nTop latent dimensions most correlated with scaled value:\")\n",
    "# for i, c in top_dims:\n",
    "#     print(f\"z[{i:03d}]: corr = {c:.4f}\")\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(12, 4))\n",
    "# plt.plot(np.sort(np.abs(corr_value))[::-1], marker='o')\n",
    "# plt.title(\"Absolute Correlation of Latent Dims with Value\")\n",
    "# plt.xlabel(\"Sorted Latent Dimension\")\n",
    "# plt.ylabel(\"Absolute Correlation\")\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e1ffb0-31c4-4e71-9fc0-3411adf06d74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
