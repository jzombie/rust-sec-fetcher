{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceaa3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# This snippet ensures consistent import paths across environments.\n",
    "# When running notebooks via JupyterLab's web UI, the current working\n",
    "# directory is often different (e.g., /notebooks) compared to VS Code,\n",
    "# which typically starts at the project root. This handles that by \n",
    "# retrying the import after changing to the parent directory.\n",
    "# \n",
    "# Include this at the top of every notebook to standardize imports\n",
    "# across development environments.\n",
    "\n",
    "try:\n",
    "    from utils.os import chdir_to_git_root\n",
    "except ModuleNotFoundError:\n",
    "    os.chdir(Path.cwd().parent)\n",
    "    print(f\"Retrying import from: {os.getcwd()}\")\n",
    "    from utils.os import chdir_to_git_root\n",
    "\n",
    "chdir_to_git_root(\"python\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97813a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from db import DB\n",
    "\n",
    "# === Extraction Logic ===\n",
    "def extract_concept_unit_value_tuples(data_dir, valid_concepts):\n",
    "    rows = []\n",
    "    unit_values = defaultdict(list)\n",
    "    unit_concepts = defaultdict(set)\n",
    "    non_numeric_units = set()\n",
    "\n",
    "    csv_files = []\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "\n",
    "    for path in tqdm(csv_files, desc=\"Scanning CSV files\"):\n",
    "        try:\n",
    "            df = pd.read_csv(path, low_memory=False)\n",
    "            tag_columns = [col for col in df.columns if col in valid_concepts]\n",
    "            if not tag_columns:\n",
    "                continue\n",
    "\n",
    "            for col in tag_columns:\n",
    "                for val in df[col].dropna().astype(str):\n",
    "                    if \"::\" not in val:\n",
    "                        continue\n",
    "                    val_part, unit_part = val.split(\"::\", 1)\n",
    "                    unit_part = unit_part.strip().upper()\n",
    "                    try:\n",
    "                        num_val = float(val_part.strip())\n",
    "                        rows.append((col, unit_part, num_val))\n",
    "                        unit_values[unit_part].append(num_val)\n",
    "                        unit_concepts[unit_part].add(col)\n",
    "                    except ValueError:\n",
    "                        non_numeric_units.add(unit_part)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipped {path}: {e}\")\n",
    "\n",
    "    return rows, unit_values, unit_concepts, non_numeric_units, csv_files\n",
    "\n",
    "# === Main Execution ===\n",
    "data_dir = \"../data/us-gaap\"\n",
    "db = DB()\n",
    "concept_df = db.get(\"SELECT name FROM us_gaap_concept\", [\"name\"])\n",
    "valid_concepts = set(concept_df[\"name\"].values)\n",
    "\n",
    "concept_unit_value_tuples, unit_values, unit_concepts, non_numeric_units, csv_files = extract_concept_unit_value_tuples(data_dir, valid_concepts)\n",
    "\n",
    "print(f\"\\n‚úÖ Scanned {len(csv_files)} files.\")\n",
    "print(f\"üì¶ Found {len(unit_values)} numeric units and {len(non_numeric_units)} non-numeric units.\")\n",
    "\n",
    "for unit, values in sorted(unit_values.items()):\n",
    "    arr = np.array(values)\n",
    "    print(f\"üîπ {unit}\")\n",
    "    print(f\"   Count: {len(arr)}\")\n",
    "    print(f\"   Min:   {arr.min():,.4f}\")\n",
    "    print(f\"   Max:   {arr.max():,.4f}\")\n",
    "    print(f\"   Mean:  {arr.mean():,.4f}\")\n",
    "    print(f\"   Std:   {arr.std():,.4f}\")\n",
    "    print(f\"   Concepts: {', '.join(sorted(unit_concepts[unit]))}\")\n",
    "\n",
    "if non_numeric_units:\n",
    "    print(\"\\n‚ö†Ô∏è Non-numeric units encountered:\")\n",
    "    for unit in sorted(non_numeric_units):\n",
    "        print(f\"  - {unit}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d24251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüßÆ Total values extracted: {len(concept_unit_value_tuples):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b55b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Build concept/unit dataset\n",
    "concept_unit_pairs = []\n",
    "for unit, concepts in unit_concepts.items():\n",
    "    for concept in concepts:\n",
    "        concept_unit_pairs.append((concept, unit))\n",
    "\n",
    "# Convert to DataFrame\n",
    "# concept_unit_df = pd.DataFrame(concept_unit_pairs,\n",
    "#                                columns=[\"concept\", \"unit\"])\n",
    "# concept_unit_df.to_csv(\"data/concept_unit_pairs.csv\", index=False)\n",
    "# print(\"‚úÖ data/concept_unit_pairs.csv saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a50b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "from utils import generate_us_gaap_description\n",
    "\n",
    "input_texts = [f\"{generate_us_gaap_description(concept)} measured in {unit}\" for concept, unit in concept_unit_pairs]\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
    "model.to(device)\n",
    "\n",
    "def encode_on_device(texts, model, batch_size=64):\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        tokens = model.tokenize(batch)\n",
    "        tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "        with torch.no_grad():\n",
    "            output = model.forward(tokens)\n",
    "            embeddings = output[\"sentence_embedding\"]\n",
    "        all_embeddings.append(embeddings.cpu())\n",
    "    return torch.cat(all_embeddings).numpy()\n",
    "\n",
    "embeddings = encode_on_device(input_texts, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c5b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # embeddings: np.ndarray of shape (N, 1024)\n",
    "# pca = PCA()\n",
    "# pca.fit(embeddings)\n",
    "\n",
    "# explained = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# plt.plot(np.arange(1, len(explained)+1), explained)\n",
    "# plt.xlabel(\"Number of PCA components\")\n",
    "# plt.ylabel(\"Cumulative explained variance\")\n",
    "# plt.grid(True)\n",
    "# plt.axhline(0.95, color='red', linestyle='--')  # e.g. 95% threshold\n",
    "# plt.title(\"Explained Variance vs PCA Components\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a1a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "\n",
    "# Assuming `embeddings` is your (N, 1024) array\n",
    "n_components = 200  # or 128 if you're more memory-conscious\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Fit PCA and transform the embeddings\n",
    "compressed = pca.fit_transform(embeddings)\n",
    "\n",
    "# # Save PCA model and compressed embeddings\n",
    "# joblib.dump(pca, \"pca_model.joblib\")\n",
    "# np.save(\"concept_uom_embeddings_pca.npy\", compressed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab25306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "import numpy as np\n",
    "\n",
    "def plot_embeddings(embeddings, labels=None, title=\"Embedding Scatterplot\"):\n",
    "    \"\"\"\n",
    "    Display a 2D or 3D scatterplot of the compressed embeddings.\n",
    "\n",
    "    Parameters:\n",
    "        embeddings (np.ndarray): Array of shape (N, 2) or (N, 3)\n",
    "        labels (List[str], optional): Labels to annotate points (optional)\n",
    "        title (str): Plot title\n",
    "    \"\"\"\n",
    "    dim = embeddings.shape[1]\n",
    "    assert dim in (2, 3), \"Embeddings must be 2D or 3D for scatterplot\"\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "    if dim == 3:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(*embeddings.T, s=10, alpha=0.7)\n",
    "    else:\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(embeddings[:, 0], embeddings[:, 1], s=10, alpha=0.7)\n",
    "\n",
    "    if labels is not None:\n",
    "        for i, label in enumerate(labels):\n",
    "            if dim == 3:\n",
    "                ax.text(*embeddings[i], label, fontsize=6)\n",
    "            else:\n",
    "                ax.text(embeddings[i, 0], embeddings[i, 1], label, fontsize=6)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_embeddings(compressed[:, :2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5c762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hdbscan\n",
    "# import umap\n",
    "# import matplotlib.pyplot as plt\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # Cluster\n",
    "# # clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=5, cluster_selection_method=\"leaf\")\n",
    "# clusterer = hdbscan.HDBSCAN(min_cluster_size=20, min_samples=10)\n",
    "# labels = clusterer.fit_predict(compressed)  # PCA-reduced embeddings\n",
    "\n",
    "# # Group input_texts by cluster\n",
    "# clusters = defaultdict(list)\n",
    "# for idx, label in enumerate(labels):\n",
    "#     clusters[label].append(input_texts[idx])\n",
    "\n",
    "# # Print samples from each cluster\n",
    "# for cluster_id, examples in clusters.items():\n",
    "#     if cluster_id == -1:\n",
    "#         continue  # Skip noise\n",
    "#     print(f\"\\nüì¶ Cluster {cluster_id} ({len(examples)} samples):\")\n",
    "#     for e in examples[:10]:\n",
    "#         print(f\"  - {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98055128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UMAP visualization\n",
    "# umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, metric=\"cosine\")\n",
    "# umap_2d = umap_model.fit_transform(compressed)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(umap_2d[:, 0], umap_2d[:, 1], c=labels, cmap=\"tab10\", s=5)\n",
    "# plt.title(\"Concept/UOM Embeddings Clustered\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e52bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({\n",
    "#     \"concept\": [c for c, _ in concept_unit_pairs],\n",
    "#     \"unit\": [u for _, u in concept_unit_pairs],\n",
    "#     \"cluster\": labels\n",
    "# })\n",
    "# grouped = df.groupby(\"cluster\")\n",
    "\n",
    "# for cluster_id, group in grouped:\n",
    "#     print(f\"\\nCluster {cluster_id} ({len(group)} items):\")\n",
    "#     print(group.head(10).to_string(index=False))\n",
    "\n",
    "# noise = df[df[\"cluster\"] == -1]\n",
    "\n",
    "# print(f\"Noise points: {len(noise)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_points = df[df[\"cluster\"] == -1][[\"concept\", \"unit\"]].reset_index(drop=True)\n",
    "\n",
    "# noise_points.to_csv(\"noise_points.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f259b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = joblib.load(\"pca_model.joblib\")\n",
    "# compressed = np.load(\"concept_uom_embeddings_pca.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a051cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Save both embeddings and tuples\n",
    "np.savez_compressed(\n",
    "    \"data/stage1_latents.npz\",\n",
    "    keys=np.array([f\"{c}::{u}\" for c, u in concept_unit_pairs]),\n",
    "    embeddings=compressed,\n",
    "    concept_unit_value_tuples=np.array(concept_unit_value_tuples, dtype=object)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Saved {len(concept_unit_value_tuples):,} tuples and {len(compressed):,} embeddings to 'stage1_latents.npz'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc170ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load saved latent data\n",
    "data = np.load(\"data/stage1_latents.npz\", allow_pickle=True)\n",
    "\n",
    "# Build embedding map\n",
    "embedding_map = {\n",
    "    tuple(key.split(\"::\", 1)): vec\n",
    "    for key, vec in zip(data[\"keys\"], data[\"embeddings\"])\n",
    "}\n",
    "\n",
    "# Load concept-unit-value tuples\n",
    "concept_unit_value_tuples = data[\"concept_unit_value_tuples\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# NOTE: Scaling is performed before train/val split to ensure that every\n",
    "# (concept, unit) pair receives a fitted StandardScaler. If we split first,\n",
    "# some (concept, unit) groups might not appear in the training set at all,\n",
    "# making it impossible to fit their scalers later ‚Äî leading to missing\n",
    "# or unscalable entries downstream.\n",
    "#\n",
    "# IMPORTANT: The goal of this Stage 1 encoder is not to \"predict\" values,\n",
    "# but to learn meaningful latent representations of (concept, unit, value)\n",
    "# tuples. These embeddings are intended for use in downstream models and\n",
    "# alignment stages, not for direct forecasting or regression tasks.\n",
    "\n",
    "# Step 1: Group values per (concept, unit)\n",
    "grouped = defaultdict(list)\n",
    "for concept, unit, value in concept_unit_value_tuples:\n",
    "    grouped[(concept, unit)].append(value)\n",
    "\n",
    "# Step 2: Fit individual scalers and transform\n",
    "scalers = {}\n",
    "scaled_tuples = []\n",
    "\n",
    "for key, vals in tqdm(grouped.items(), desc=\"Scaling per concept/unit\"):\n",
    "    vals_np = np.array(vals).reshape(-1, 1)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_vals = scaler.fit_transform(vals_np).flatten()\n",
    "    scalers[key] = scaler\n",
    "\n",
    "    # Rebuild tuples\n",
    "    scaled_tuples.extend((key[0], key[1], v) for v in scaled_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65b584bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset\n",
    "from utils.pytorch import seed_everything\n",
    "import numpy as np\n",
    "from torch.nn.functional import cosine_similarity, l1_loss\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchmetrics.regression import R2Score\n",
    "\n",
    "\n",
    "# Stage 1 dataset: concept+uom embedding + value\n",
    "class ConceptValueDataset(Dataset):\n",
    "    def __init__(self, scaled_tuples, embedding_lookup, device: torch.tensor,\n",
    "                 value_noise_std=0.0, train=False, scalers=None, return_scaler=False):\n",
    "        \"\"\"\n",
    "        Dataset for (concept, unit, value) triplets with optional per-sample scaler.\n",
    "\n",
    "        :param scaled_tuples: List of (concept, unit, scaled_value) tuples\n",
    "        :param embedding_lookup: Dict[(concept, unit)] -> embedding np.array\n",
    "        :param device: torch device tensor to place tensors on\n",
    "        :param value_noise_std: Standard deviation of Gaussian noise to add (train only)\n",
    "        :param train: Whether this is training mode (controls noise)\n",
    "        :param scalers: Optional dict of (concept, unit) -> StandardScaler\n",
    "        :param return_scaler: If True, return the scaler used per sample\n",
    "        \"\"\"\n",
    "        self.rows = scaled_tuples\n",
    "        self.lookup = embedding_lookup\n",
    "        self.value_noise_std = value_noise_std\n",
    "        self.train = train\n",
    "        self.device = device\n",
    "        self.scalers = scalers or {}\n",
    "        self.return_scaler = return_scaler\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        concept, unit, value = self.rows[idx]\n",
    "\n",
    "        try:\n",
    "            embedding = self.lookup[(concept, unit)]\n",
    "        except KeyError:\n",
    "            raise ValueError(f\"Missing embedding for ({concept}, {unit})\")\n",
    "\n",
    "        if self.train and self.value_noise_std > 0:\n",
    "            value += np.random.normal(0, self.value_noise_std)\n",
    "\n",
    "        x = torch.tensor(np.concatenate([embedding, [value]]), dtype=torch.float32,\n",
    "                         device=self.device)\n",
    "        y = torch.tensor(np.concatenate([embedding, [value]]), dtype=torch.float32,\n",
    "                         device=self.device)\n",
    "\n",
    "        if self.return_scaler:\n",
    "            return x, y, self.scalers.get((concept, unit))\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def collate_with_scaler(batch):\n",
    "    \"\"\"Custom collate_fn that handles (x, y, scaler) samples.\"\"\"\n",
    "    xs, ys, scalers = zip(*batch)\n",
    "    return torch.stack(xs), torch.stack(ys), scalers\n",
    "\n",
    "\n",
    "# LightningModule\n",
    "class Stage1Autoencoder(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim=201,\n",
    "            latent_dim=128,\n",
    "            dropout_rate=0.0,\n",
    "            lr=0.00023072200683712404,\n",
    "            batch_size=64,\n",
    "            gradient_clip=0.7,\n",
    "            alpha_embed=0.5,\n",
    "            alpha_value=1.0,\n",
    "            weight_decay=5.220603379116996e-07\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # self.value_proj = nn.Sequential(\n",
    "        #     nn.Linear(1, 32),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(32, self.hparams.latent_dim),\n",
    "        #     nn.LayerNorm(self.hparams.latent_dim)\n",
    "        # )\n",
    "\n",
    "        self.value_proj = nn.Sequential(\n",
    "            nn.Linear(1, 32),               # Project from scalar (1) to 32\n",
    "            nn.GELU(),                      # Non-linearity for complex transformations\n",
    "            nn.Dropout(p=dropout_rate),     # Regularization\n",
    "            nn.Linear(32, 64),              # Expand to 64\n",
    "            nn.GELU(),                      # Non-linearity\n",
    "            nn.Dropout(p=dropout_rate),     # Regularization\n",
    "            nn.Linear(64, self.hparams.latent_dim),  # Final projection to latent_dim (128)\n",
    "            nn.LayerNorm(self.hparams.latent_dim)    # Normalize for stable training\n",
    "        )\n",
    "\n",
    "        # self.encoder = nn.Sequential(\n",
    "        #     nn.Linear(input_dim - 1 + self.hparams.latent_dim, 256),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Dropout(p=dropout_rate),\n",
    "        #     nn.Linear(256, latent_dim)\n",
    "        # )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim - 1 + latent_dim, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(256, latent_dim)\n",
    "        )\n",
    "\n",
    "        \n",
    "        # self.embedding_decoder = nn.Sequential(\n",
    "        #     nn.Linear(latent_dim, 256),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Dropout(p=dropout_rate),\n",
    "        #     nn.Linear(256, input_dim - 1)\n",
    "        # )\n",
    "\n",
    "        self.embedding_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(512, input_dim - 1)\n",
    "        )\n",
    "\n",
    "        # self.value_decoder = nn.Sequential(\n",
    "        #     nn.Linear(latent_dim, 128),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Dropout(p=dropout_rate),\n",
    "        #     nn.Linear(128, 1)\n",
    "        # )\n",
    "\n",
    "        self.value_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),       # Project from latent_dim (128) to 64\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(64, 32),               # Reduce to 32\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(32, 1)                 # Final output (single value)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.train_r2 = R2Score().to(self.device)\n",
    "        self.val_r2 = R2Score().to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, input_dim]\n",
    "        x_emb = x[:, :-1]\n",
    "        x_val = x[:, -1].unsqueeze(1)\n",
    "    \n",
    "        # project the value separately and then fuse\n",
    "        val_proj = self.value_proj(x_val)\n",
    "        fused_input = torch.cat([x_emb, val_proj], dim=1)\n",
    "    \n",
    "        z = self.encoder(fused_input)\n",
    "        \n",
    "        recon_emb = self.embedding_decoder(z)\n",
    "        recon_val = self.value_decoder(z)\n",
    "        \n",
    "        return recon_emb, recon_val\n",
    "    \n",
    "    # def compute_losses(self, x, target, train):\n",
    "    #     recon_emb, recon_val = self(x)\n",
    "\n",
    "    #     target_emb = target[:, :-1]\n",
    "    #     target_val = target[:, -1].unsqueeze(1)\n",
    "\n",
    "    #     embedding_loss = self.loss_fn(recon_emb, target_emb)\n",
    "    #     value_loss = self.loss_fn(recon_val, target_val)\n",
    "    #     loss = self.hparams.alpha_embed * embedding_loss + self.hparams.alpha_value * value_loss\n",
    "\n",
    "    #     cos_sim = cosine_similarity(recon_emb, target_emb, dim=1).mean()\n",
    "    #     mae_value = l1_loss(recon_val, target_val)\n",
    "    #     euclidean_dist = torch.norm(recon_emb - target_emb, dim=1).mean()\n",
    "\n",
    "    #     if train:\n",
    "    #         self.train_r2.update(recon_val.detach(), target_val.detach())\n",
    "    #         r2_value = self.train_r2.compute()\n",
    "    #     else:\n",
    "    #         self.val_r2.update(recon_val.detach(), target_val.detach())\n",
    "    #         r2_value = self.val_r2.compute()\n",
    "\n",
    "    #     return loss, embedding_loss, value_loss, cos_sim, mae_value, euclidean_dist, r2_value\n",
    "    def compute_losses(self, x, target, scaler=None, train=False):\n",
    "        recon_emb, recon_val = self(x)\n",
    "\n",
    "        target_emb = target[:, :-1]\n",
    "        target_val = target[:, -1].unsqueeze(1)\n",
    "\n",
    "        if scaler is not None and isinstance(scaler, (list, tuple)):\n",
    "            recon_val_np = recon_val.detach().cpu().numpy()\n",
    "            target_val_np = target_val.detach().cpu().numpy()\n",
    "\n",
    "            # Inverse transform per sample\n",
    "            recon_val_orig = np.stack([\n",
    "                s.inverse_transform(r.reshape(-1, 1)).flatten()\n",
    "                for s, r in zip(scaler, recon_val_np)\n",
    "            ])\n",
    "            target_val_orig = np.stack([\n",
    "                s.inverse_transform(t.reshape(-1, 1)).flatten()\n",
    "                for s, t in zip(scaler, target_val_np)\n",
    "            ])\n",
    "\n",
    "            recon_val_orig = torch.tensor(recon_val_orig, dtype=torch.float32,\n",
    "                                        device=recon_val.device)\n",
    "            target_val_orig = torch.tensor(target_val_orig, dtype=torch.float32,\n",
    "                                        device=target_val.device)\n",
    "        else:\n",
    "            raise Exception(\"Scaler not implemented\")\n",
    "            # If scaler is not implemented, use scaled values directly\n",
    "            # recon_val_orig = recon_val\n",
    "            # target_val_orig = target_val\n",
    "\n",
    "        # non-scaled\n",
    "        embedding_loss = self.loss_fn(recon_emb, target_emb)\n",
    "\n",
    "        # scaled\n",
    "        value_loss = self.loss_fn(recon_val, target_val)\n",
    "        loss = self.hparams.alpha_embed * embedding_loss + \\\n",
    "            self.hparams.alpha_value * value_loss\n",
    "\n",
    "        # non-scaled\n",
    "        cos_sim = cosine_similarity(recon_emb, target_emb, dim=1).mean()\n",
    "        euclidean_dist = torch.norm(recon_emb - target_emb, dim=1).mean()\n",
    "\n",
    "        # non-scaled\n",
    "        mae = l1_loss(recon_val_orig, target_val_orig)\n",
    "        relative_mae = (mae / (target_val_orig.abs().mean() + 1e-8)).clamp(0, 10)\n",
    "\n",
    "        if train:\n",
    "            self.train_r2.update(recon_val_orig, target_val_orig)\n",
    "            r2_value = self.train_r2.compute()\n",
    "        else:\n",
    "            self.val_r2.update(recon_val_orig, target_val_orig)\n",
    "            r2_value = self.val_r2.compute()\n",
    "\n",
    "        return loss, embedding_loss, value_loss, cos_sim, mae, relative_mae, \\\n",
    "            euclidean_dist, r2_value\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        if len(batch) == 3:\n",
    "            x, target, scaler = batch\n",
    "        else:\n",
    "            x, target = batch\n",
    "            scaler = None\n",
    "\n",
    "        loss, embedding_loss, value_loss, cos_sim, mae, relative_mae, euclidean_dist, r2_value = (\n",
    "            self.compute_losses(x, target, scaler, train=True)\n",
    "        )\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_embedding_loss\", embedding_loss)\n",
    "        self.log(\"train_value_loss\", value_loss)\n",
    "        self.log(\"train_embedding_cos_sim\", cos_sim)\n",
    "        self.log(\"train_value_mae\", mae)\n",
    "        self.log(\"train_value_relative_mae\", relative_mae)\n",
    "        self.log(\"train_embedding_euclidean\", euclidean_dist)\n",
    "        self.log(\"train_value_r2\", r2_value)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        if len(batch) == 3:\n",
    "            x, target, scaler = batch\n",
    "        else:\n",
    "            x, target = batch\n",
    "            scaler = None\n",
    "\n",
    "        loss, embedding_loss, value_loss, cos_sim, mae, relative_mae, euclidean_dist, r2_value = (\n",
    "            self.compute_losses(x, target, scaler, train=False)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_embedding_loss\", embedding_loss)\n",
    "        self.log(\"val_value_loss\", value_loss)\n",
    "        self.log(\"val_embedding_cos_sim\", cos_sim)\n",
    "        self.log(\"val_value_mae\", mae)\n",
    "        self.log(\"val_value_relative_mae\", relative_mae)\n",
    "        self.log(\"val_embedding_euclidean\", euclidean_dist)\n",
    "        self.log(\"val_value_r2\", r2_value)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.train_r2.reset()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.val_r2.reset()\n",
    "\n",
    "\n",
    "    # def configure_optimizers(self):\n",
    "    #     return torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        \n",
    "        # Use CosineAnnealingLR with T_max=15 and eta_min=1e-6 (matches your 15 epochs)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=15, eta_min=1e-6)\n",
    "        \n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccdb991-b887-4847-968b-8d9fb4b9517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning\n",
    "\n",
    "# import os\n",
    "# import optuna\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "# from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from torch.utils.data import DataLoader\n",
    "# from utils.pytorch import get_device\n",
    "\n",
    "# device = get_device()\n",
    "\n",
    "# # === CONFIG ===\n",
    "# OUTPUT_PATH = \"data/stage1\"\n",
    "# os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "# OPTUNA_DB_PATH = os.path.join(OUTPUT_PATH, \"optuna_study.db\")\n",
    "# EPOCHS = 3\n",
    "# PATIENCE = 5\n",
    "# VAL_SPLIT = 0.2\n",
    "\n",
    "# def objective(trial):\n",
    "#     batch_size = trial.suggest_int(\"batch_size\", 8, 64, step=8)\n",
    "#     lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "#     latent_dim = trial.suggest_int(\"latent_dim\", 32, 128, step=32)\n",
    "#     dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.2, step=0.1)\n",
    "#     weight_decay = trial.suggest_float(\"weight_decay\", 1e-8, 1e-4, log=True)\n",
    "#     gradient_clip = trial.suggest_float(\"gradient_clip\", 0.0, 1.0, step=0.1)\n",
    "\n",
    "#     # # 80/20 Train/Val Split\n",
    "#     # split = int(len(scaled_tuples) * (1 - VAL_SPLIT))\n",
    "#     # train_data = scaled_tuples[:split]\n",
    "#     # val_data = scaled_tuples[split:]\n",
    "\n",
    "#      # === Sample Subset for Faster Debugging ===\n",
    "#     SAMPLE_SIZE = 500_000\n",
    "#     subset = scaled_tuples[:SAMPLE_SIZE]\n",
    "    \n",
    "#     # 80/20 Train/Val Split\n",
    "#     split = int(len(subset) * (1 - VAL_SPLIT))\n",
    "#     train_data = subset[:split]\n",
    "#     val_data = subset[split:]\n",
    "\n",
    "#     train_loader = DataLoader(\n",
    "#         ConceptValueDataset(train_data, embedding_map, device=device, value_noise_std=0.005, train=True),\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=True\n",
    "#     )\n",
    "    \n",
    "#     val_loader = DataLoader(\n",
    "#         ConceptValueDataset(val_data, embedding_map, device=device, value_noise_std=0.00, train=False),\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=False\n",
    "#     )\n",
    "\n",
    "#     input_dim = len(next(iter(embedding_map.values()))) + 1\n",
    "\n",
    "#     model = Stage1Autoencoder(\n",
    "#         input_dim=input_dim,\n",
    "#         latent_dim=latent_dim,\n",
    "#         dropout_rate=dropout_rate,\n",
    "#         lr=lr,\n",
    "#         batch_size=batch_size,\n",
    "#         weight_decay=weight_decay,\n",
    "#         gradient_clip=gradient_clip\n",
    "#     )\n",
    "\n",
    "#     early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, verbose=True, mode=\"min\")\n",
    "\n",
    "#     model_checkpoint = ModelCheckpoint(\n",
    "#         dirpath=OUTPUT_PATH,\n",
    "#         filename=\"best_model_trial_{trial.number}\",\n",
    "#         monitor=\"val_loss\",\n",
    "#         mode=\"min\",\n",
    "#         save_top_k=1,\n",
    "#         verbose=True\n",
    "#     )\n",
    "\n",
    "#     trainer = pl.Trainer(\n",
    "#         max_epochs=EPOCHS,\n",
    "#         logger=TensorBoardLogger(OUTPUT_PATH, name=\"stage1_autoencoder\"),\n",
    "#         callbacks=[early_stop_callback, model_checkpoint],\n",
    "#         accelerator=\"auto\",\n",
    "#         devices=1,\n",
    "#         gradient_clip_val=gradient_clip\n",
    "#     )\n",
    "\n",
    "#     trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "#     return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "# # === Optuna Study ===\n",
    "# study = optuna.create_study(direction=\"minimize\",\n",
    "#                             storage=f\"sqlite:///{OPTUNA_DB_PATH}\",\n",
    "#                             load_if_exists=True)\n",
    "# study.optimize(objective, n_trials=25)\n",
    "\n",
    "# print(\"Best params:\", study.best_params)\n",
    "# print(\"Best trial value:\", study.best_trial.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c278a0d0-19d7-47ca-95ba-5d972553b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Instantiate dataset\n",
    "# dataset = ConceptValueDataset(scaled_tuples, embedding_map)\n",
    "\n",
    "# # Sample inspection\n",
    "# sample_x, sample_y = dataset[0]\n",
    "# print(\"Sample input:\", sample_x)\n",
    "# print(\"Min:\", sample_x.min().item(), \"Max:\", sample_x.max().item())\n",
    "# print(\"Mean:\", sample_x.mean().item(), \"Std:\", sample_x.std().item())\n",
    "# print(\"Input dim:\", sample_x.shape[0], \"Target dim:\", sample_y.shape[0])\n",
    "\n",
    "# # Optional: test batch loading\n",
    "# loader = DataLoader(dataset, batch_size=4)\n",
    "# for xb, yb in loader:\n",
    "#     print(\"Batch shape:\", xb.shape)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47484ee4-1c73-48be-8024-e368d977c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "import os\n",
    "import optuna\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.pytorch import get_device\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "# === CONFIG ===\n",
    "OUTPUT_PATH = \"data/stage1_23a\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "OPTUNA_DB_PATH = os.path.join(OUTPUT_PATH, \"optuna_study.db\")\n",
    "EPOCHS = 1000\n",
    "PATIENCE = 5\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "# 80/20 Train/Val Split\n",
    "split = int(len(scaled_tuples) * (1 - VAL_SPLIT))\n",
    "train_data = scaled_tuples[:split]\n",
    "val_data = scaled_tuples[split:]\n",
    "\n",
    "# model = Stage1Autoencoder.load_from_checkpoint(\"data/stage1.tuning/best_model_trial_trial.number=0-v23.ckpt\")\n",
    "model = Stage1Autoencoder()\n",
    "\n",
    "batch_size = model.hparams.batch_size\n",
    "gradient_clip = model.hparams.gradient_clip\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     ConceptValueDataset(train_data, embedding_map, device=device, value_noise_std=0.005, train=True),\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# val_loader = DataLoader(\n",
    "#     ConceptValueDataset(val_data, embedding_map, device=device, value_noise_std=0.00, train=False),\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False\n",
    "# )\n",
    "train_loader = DataLoader(\n",
    "    ConceptValueDataset(\n",
    "        train_data,\n",
    "        embedding_map,\n",
    "        device=device,\n",
    "        value_noise_std=0.005,\n",
    "        train=True,\n",
    "        scalers=scalers,\n",
    "        return_scaler=True\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_with_scaler\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    ConceptValueDataset(\n",
    "        val_data,\n",
    "        embedding_map,\n",
    "        device=device,\n",
    "        value_noise_std=0.00,\n",
    "        train=False,\n",
    "        scalers=scalers,\n",
    "        return_scaler=True\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_with_scaler\n",
    ")\n",
    "\n",
    "\n",
    "input_dim = len(next(iter(embedding_map.values()))) + 1\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, verbose=True, mode=\"min\")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    dirpath=OUTPUT_PATH,\n",
    "    filename=\"stage1_resume\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    logger=TensorBoardLogger(OUTPUT_PATH, name=\"stage1_autoencoder\"),\n",
    "    callbacks=[early_stop_callback, model_checkpoint],\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    gradient_clip_val=gradient_clip\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca87a09-65c9-42ee-9310-42558e6df0dc",
   "metadata": {},
   "source": [
    "# Conceptual Draft\n",
    "\n",
    "Stage 1 learns semantic+quantitative embeddings for individual concept/unit/value triplets.\n",
    "\n",
    "Stage 2 learns how to aggregate and contextualize those embeddings into higher-order units (i.e., financial statements).\n",
    "\n",
    "Stage 3 learns how to model temporal dynamics and structural evolution across filings ‚Äî a full hierarchy of understanding.\n",
    "\n",
    "This pipeline could encode an entire company's financial narrative into vector space.\n",
    "\n",
    "It‚Äôs structured like language modeling, but for accounting ‚Äî and that‚Äôs what makes it potentially groundbreaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d8512-2ade-484a-94d4-31d84a497782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
