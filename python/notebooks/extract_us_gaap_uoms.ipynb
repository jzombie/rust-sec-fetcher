{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceaa3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# This snippet ensures consistent import paths across environments.\n",
    "# When running notebooks via JupyterLab's web UI, the current working\n",
    "# directory is often different (e.g., /notebooks) compared to VS Code,\n",
    "# which typically starts at the project root. This handles that by \n",
    "# retrying the import after changing to the parent directory.\n",
    "# \n",
    "# Include this at the top of every notebook to standardize imports\n",
    "# across development environments.\n",
    "\n",
    "try:\n",
    "    from utils.os import chdir_to_git_root\n",
    "except ModuleNotFoundError:\n",
    "    os.chdir(Path.cwd().parent)\n",
    "    print(f\"Retrying import from: {os.getcwd()}\")\n",
    "    from utils.os import chdir_to_git_root\n",
    "\n",
    "chdir_to_git_root(\"python\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97813a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from db import DB\n",
    "\n",
    "# === Extraction Logic ===\n",
    "def extract_concept_unit_value_tuples(data_dir, valid_concepts):\n",
    "    rows = []\n",
    "    unit_values = defaultdict(list)\n",
    "    unit_concepts = defaultdict(set)\n",
    "    non_numeric_units = set()\n",
    "\n",
    "    csv_files = []\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "\n",
    "    for path in tqdm(csv_files, desc=\"Scanning CSV files\"):\n",
    "        try:\n",
    "            df = pd.read_csv(path, low_memory=False)\n",
    "            tag_columns = [col for col in df.columns if col in valid_concepts]\n",
    "            if not tag_columns:\n",
    "                continue\n",
    "\n",
    "            for col in tag_columns:\n",
    "                for val in df[col].dropna().astype(str):\n",
    "                    if \"::\" not in val:\n",
    "                        continue\n",
    "                    val_part, unit_part = val.split(\"::\", 1)\n",
    "                    unit_part = unit_part.strip().upper()\n",
    "                    try:\n",
    "                        num_val = float(val_part.strip())\n",
    "                        rows.append((col, unit_part, num_val))\n",
    "                        unit_values[unit_part].append(num_val)\n",
    "                        unit_concepts[unit_part].add(col)\n",
    "                    except ValueError:\n",
    "                        non_numeric_units.add(unit_part)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Skipped {path}: {e}\")\n",
    "\n",
    "    return rows, unit_values, unit_concepts, non_numeric_units, csv_files\n",
    "\n",
    "# === Main Execution ===\n",
    "data_dir = \"../data/us-gaap\"\n",
    "db = DB()\n",
    "concept_df = db.get(\"SELECT name FROM us_gaap_concept\", [\"name\"])\n",
    "valid_concepts = set(concept_df[\"name\"].values)\n",
    "\n",
    "concept_unit_value_tuples, unit_values, unit_concepts, non_numeric_units, csv_files = extract_concept_unit_value_tuples(data_dir, valid_concepts)\n",
    "\n",
    "print(f\"\\nâœ… Scanned {len(csv_files)} files.\")\n",
    "print(f\"ðŸ“¦ Found {len(unit_values)} numeric units and {len(non_numeric_units)} non-numeric units.\")\n",
    "\n",
    "for unit, values in sorted(unit_values.items()):\n",
    "    arr = np.array(values)\n",
    "    print(f\"ðŸ”¹ {unit}\")\n",
    "    print(f\"   Count: {len(arr)}\")\n",
    "    print(f\"   Min:   {arr.min():,.4f}\")\n",
    "    print(f\"   Max:   {arr.max():,.4f}\")\n",
    "    print(f\"   Mean:  {arr.mean():,.4f}\")\n",
    "    print(f\"   Std:   {arr.std():,.4f}\")\n",
    "    print(f\"   Concepts: {', '.join(sorted(unit_concepts[unit]))}\")\n",
    "\n",
    "if non_numeric_units:\n",
    "    print(\"\\nâš ï¸ Non-numeric units encountered:\")\n",
    "    for unit in sorted(non_numeric_units):\n",
    "        print(f\"  - {unit}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d24251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nðŸ§® Total values extracted: {len(concept_unit_value_tuples):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b55b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Build concept/unit dataset\n",
    "concept_unit_pairs = []\n",
    "for unit, concepts in unit_concepts.items():\n",
    "    for concept in concepts:\n",
    "        concept_unit_pairs.append((concept, unit))\n",
    "\n",
    "# Convert to DataFrame\n",
    "# concept_unit_df = pd.DataFrame(concept_unit_pairs,\n",
    "#                                columns=[\"concept\", \"unit\"])\n",
    "# concept_unit_df.to_csv(\"data/concept_unit_pairs.csv\", index=False)\n",
    "# print(\"âœ… data/concept_unit_pairs.csv saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a50b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "from utils import generate_us_gaap_description\n",
    "\n",
    "input_texts = [f\"{generate_us_gaap_description(concept)} measured in {unit}\" for concept, unit in concept_unit_pairs]\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
    "model.to(device)\n",
    "\n",
    "def encode_on_device(texts, model, batch_size=64):\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        tokens = model.tokenize(batch)\n",
    "        tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "        with torch.no_grad():\n",
    "            output = model.forward(tokens)\n",
    "            embeddings = output[\"sentence_embedding\"]\n",
    "        all_embeddings.append(embeddings.cpu())\n",
    "    return torch.cat(all_embeddings).numpy()\n",
    "\n",
    "embeddings = encode_on_device(input_texts, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c5b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # embeddings: np.ndarray of shape (N, 1024)\n",
    "# pca = PCA()\n",
    "# pca.fit(embeddings)\n",
    "\n",
    "# explained = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# plt.plot(np.arange(1, len(explained)+1), explained)\n",
    "# plt.xlabel(\"Number of PCA components\")\n",
    "# plt.ylabel(\"Cumulative explained variance\")\n",
    "# plt.grid(True)\n",
    "# plt.axhline(0.95, color='red', linestyle='--')  # e.g. 95% threshold\n",
    "# plt.title(\"Explained Variance vs PCA Components\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a1a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "\n",
    "# Assuming `embeddings` is your (N, 1024) array\n",
    "n_components = 200  # or 128 if you're more memory-conscious\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Fit PCA and transform the embeddings\n",
    "compressed = pca.fit_transform(embeddings)\n",
    "\n",
    "# # Save PCA model and compressed embeddings\n",
    "# joblib.dump(pca, \"pca_model.joblib\")\n",
    "# np.save(\"concept_uom_embeddings_pca.npy\", compressed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab25306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "import numpy as np\n",
    "\n",
    "def plot_embeddings(embeddings, labels=None, title=\"Embedding Scatterplot\"):\n",
    "    \"\"\"\n",
    "    Display a 2D or 3D scatterplot of the compressed embeddings.\n",
    "\n",
    "    Parameters:\n",
    "        embeddings (np.ndarray): Array of shape (N, 2) or (N, 3)\n",
    "        labels (List[str], optional): Labels to annotate points (optional)\n",
    "        title (str): Plot title\n",
    "    \"\"\"\n",
    "    dim = embeddings.shape[1]\n",
    "    assert dim in (2, 3), \"Embeddings must be 2D or 3D for scatterplot\"\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "    if dim == 3:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(*embeddings.T, s=10, alpha=0.7)\n",
    "    else:\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(embeddings[:, 0], embeddings[:, 1], s=10, alpha=0.7)\n",
    "\n",
    "    if labels is not None:\n",
    "        for i, label in enumerate(labels):\n",
    "            if dim == 3:\n",
    "                ax.text(*embeddings[i], label, fontsize=6)\n",
    "            else:\n",
    "                ax.text(embeddings[i, 0], embeddings[i, 1], label, fontsize=6)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_embeddings(compressed[:, :2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5c762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hdbscan\n",
    "# import umap\n",
    "# import matplotlib.pyplot as plt\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # Cluster\n",
    "# # clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=5, cluster_selection_method=\"leaf\")\n",
    "# clusterer = hdbscan.HDBSCAN(min_cluster_size=20, min_samples=10)\n",
    "# labels = clusterer.fit_predict(compressed)  # PCA-reduced embeddings\n",
    "\n",
    "# # Group input_texts by cluster\n",
    "# clusters = defaultdict(list)\n",
    "# for idx, label in enumerate(labels):\n",
    "#     clusters[label].append(input_texts[idx])\n",
    "\n",
    "# # Print samples from each cluster\n",
    "# for cluster_id, examples in clusters.items():\n",
    "#     if cluster_id == -1:\n",
    "#         continue  # Skip noise\n",
    "#     print(f\"\\nðŸ“¦ Cluster {cluster_id} ({len(examples)} samples):\")\n",
    "#     for e in examples[:10]:\n",
    "#         print(f\"  - {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98055128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UMAP visualization\n",
    "# umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, metric=\"cosine\")\n",
    "# umap_2d = umap_model.fit_transform(compressed)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(umap_2d[:, 0], umap_2d[:, 1], c=labels, cmap=\"tab10\", s=5)\n",
    "# plt.title(\"Concept/UOM Embeddings Clustered\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e52bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({\n",
    "#     \"concept\": [c for c, _ in concept_unit_pairs],\n",
    "#     \"unit\": [u for _, u in concept_unit_pairs],\n",
    "#     \"cluster\": labels\n",
    "# })\n",
    "# grouped = df.groupby(\"cluster\")\n",
    "\n",
    "# for cluster_id, group in grouped:\n",
    "#     print(f\"\\nCluster {cluster_id} ({len(group)} items):\")\n",
    "#     print(group.head(10).to_string(index=False))\n",
    "\n",
    "# noise = df[df[\"cluster\"] == -1]\n",
    "\n",
    "# print(f\"Noise points: {len(noise)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_points = df[df[\"cluster\"] == -1][[\"concept\", \"unit\"]].reset_index(drop=True)\n",
    "\n",
    "# noise_points.to_csv(\"noise_points.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f259b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = joblib.load(\"pca_model.joblib\")\n",
    "# compressed = np.load(\"concept_uom_embeddings_pca.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a051cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Save both embeddings and tuples\n",
    "np.savez_compressed(\n",
    "    \"data/stage1_latents.npz\",\n",
    "    keys=np.array([f\"{c}::{u}\" for c, u in concept_unit_pairs]),\n",
    "    embeddings=compressed,\n",
    "    concept_unit_value_tuples=np.array(concept_unit_value_tuples, dtype=object)\n",
    ")\n",
    "\n",
    "print(f\"âœ… Saved {len(concept_unit_value_tuples):,} tuples and {len(compressed):,} embeddings to 'stage1_latents.npz'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc170ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load saved latent data\n",
    "data = np.load(\"data/stage1_latents.npz\", allow_pickle=True)\n",
    "\n",
    "# Build embedding map\n",
    "embedding_map = {\n",
    "    tuple(key.split(\"::\", 1)): vec\n",
    "    for key, vec in zip(data[\"keys\"], data[\"embeddings\"])\n",
    "}\n",
    "\n",
    "# Load concept-unit-value tuples\n",
    "concept_unit_value_tuples = data[\"concept_unit_value_tuples\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# NOTE: Scaling is performed before train/val split to ensure that every\n",
    "# (concept, unit) pair receives a fitted StandardScaler. If we split first,\n",
    "# some (concept, unit) groups might not appear in the training set at all,\n",
    "# making it impossible to fit their scalers later â€” leading to missing\n",
    "# or unscalable entries downstream.\n",
    "#\n",
    "# IMPORTANT: The goal of this Stage 1 encoder is not to \"predict\" values,\n",
    "# but to learn meaningful latent representations of (concept, unit, value)\n",
    "# tuples. These embeddings are intended for use in downstream models and\n",
    "# alignment stages, not for direct forecasting or regression tasks.\n",
    "\n",
    "# Step 1: Group values per (concept, unit)\n",
    "grouped = defaultdict(list)\n",
    "for concept, unit, value in concept_unit_value_tuples:\n",
    "    grouped[(concept, unit)].append(value)\n",
    "\n",
    "# Step 2: Fit individual scalers and transform\n",
    "scalers = {}\n",
    "scaled_tuples = []\n",
    "\n",
    "for key, vals in tqdm(grouped.items(), desc=\"Scaling per concept/unit\"):\n",
    "    vals_np = np.array(vals).reshape(-1, 1)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_vals = scaler.fit_transform(vals_np).flatten()\n",
    "    scalers[key] = scaler\n",
    "\n",
    "    # Rebuild tuples\n",
    "    scaled_tuples.extend((key[0], key[1], v) for v in scaled_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65b584bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset\n",
    "from utils.pytorch import seed_everything\n",
    "import numpy as np\n",
    "from torch.nn.functional import cosine_similarity, l1_loss\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchmetrics.regression import R2Score\n",
    "\n",
    "\n",
    "# Stage 1 dataset: concept+uom embedding + value\n",
    "class ConceptValueDataset(Dataset):\n",
    "    def __init__(self, scaled_tuples, embedding_lookup, device: torch.tensor,\n",
    "                 value_noise_std=0.0, train=False, scalers=None, return_scaler=False):\n",
    "        \"\"\"\n",
    "        Dataset for (concept, unit, value) triplets with optional per-sample scaler.\n",
    "\n",
    "        :param scaled_tuples: List of (concept, unit, scaled_value) tuples\n",
    "        :param embedding_lookup: Dict[(concept, unit)] -> embedding np.array\n",
    "        :param device: torch device tensor to place tensors on\n",
    "        :param value_noise_std: Standard deviation of Gaussian noise to add (train only)\n",
    "        :param train: Whether this is training mode (controls noise)\n",
    "        :param scalers: Optional dict of (concept, unit) -> StandardScaler\n",
    "        :param return_scaler: If True, return the scaler used per sample\n",
    "        \"\"\"\n",
    "        self.rows = scaled_tuples\n",
    "        self.lookup = embedding_lookup\n",
    "        self.value_noise_std = value_noise_std\n",
    "        self.train = train\n",
    "        self.device = device\n",
    "        self.scalers = scalers or {}\n",
    "        self.return_scaler = return_scaler\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        concept, unit, value = self.rows[idx]\n",
    "\n",
    "        try:\n",
    "            embedding = self.lookup[(concept, unit)]\n",
    "        except KeyError:\n",
    "            raise ValueError(f\"Missing embedding for ({concept}, {unit})\")\n",
    "\n",
    "        if self.train and self.value_noise_std > 0:\n",
    "            value += np.random.normal(0, self.value_noise_std)\n",
    "\n",
    "        x = torch.tensor(np.concatenate([embedding, [value]]), dtype=torch.float32,\n",
    "                         device=self.device)\n",
    "        y = torch.tensor(np.concatenate([embedding, [value]]), dtype=torch.float32,\n",
    "                         device=self.device)\n",
    "\n",
    "        if self.return_scaler:\n",
    "            return x, y, self.scalers.get((concept, unit))\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def collate_with_scaler(batch):\n",
    "    \"\"\"Custom collate_fn that handles (x, y, scaler) samples.\"\"\"\n",
    "    xs, ys, scalers = zip(*batch)\n",
    "    return torch.stack(xs), torch.stack(ys), scalers\n",
    "\n",
    "\n",
    "# LightningModule\n",
    "class Stage1Autoencoder(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim=201,\n",
    "            latent_dim=128,\n",
    "            dropout_rate=0.0,\n",
    "            lr=0.00023072200683712404,\n",
    "            batch_size=64,\n",
    "            gradient_clip=0.7,\n",
    "            alpha_embed=0.5,\n",
    "            alpha_value=1.0,\n",
    "            weight_decay=5.220603379116996e-07\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # self.value_proj = nn.Sequential(\n",
    "        #     nn.Linear(1, 32),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(32, self.hparams.latent_dim),\n",
    "        #     nn.LayerNorm(self.hparams.latent_dim)\n",
    "        # )\n",
    "\n",
    "        self.value_proj = nn.Sequential(\n",
    "            nn.Linear(1, 32),               # Project from scalar (1) to 32\n",
    "            nn.GELU(),                      # Non-linearity for complex transformations\n",
    "            nn.Dropout(p=dropout_rate),     # Regularization\n",
    "            nn.Linear(32, 64),              # Expand to 64\n",
    "            nn.GELU(),                      # Non-linearity\n",
    "            nn.Dropout(p=dropout_rate),     # Regularization\n",
    "            nn.Linear(64, self.hparams.latent_dim),  # Final projection to latent_dim (128)\n",
    "            nn.LayerNorm(self.hparams.latent_dim)    # Normalize for stable training\n",
    "        )\n",
    "\n",
    "        # self.encoder = nn.Sequential(\n",
    "        #     nn.Linear(input_dim - 1 + self.hparams.latent_dim, 256),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Dropout(p=dropout_rate),\n",
    "        #     nn.Linear(256, latent_dim)\n",
    "        # )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim - 1 + latent_dim, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(256, latent_dim)\n",
    "        )\n",
    "\n",
    "        \n",
    "        # self.embedding_decoder = nn.Sequential(\n",
    "        #     nn.Linear(latent_dim, 256),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Dropout(p=dropout_rate),\n",
    "        #     nn.Linear(256, input_dim - 1)\n",
    "        # )\n",
    "\n",
    "        self.embedding_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(512, input_dim - 1)\n",
    "        )\n",
    "\n",
    "        # self.value_decoder = nn.Sequential(\n",
    "        #     nn.Linear(latent_dim, 128),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Dropout(p=dropout_rate),\n",
    "        #     nn.Linear(128, 1)\n",
    "        # )\n",
    "\n",
    "        self.value_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),       # Project from latent_dim (128) to 64\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(64, 32),               # Reduce to 32\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(32, 1)                 # Final output (single value)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.train_r2 = R2Score().to(self.device)\n",
    "        self.val_r2 = R2Score().to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, input_dim]\n",
    "        x_emb = x[:, :-1]\n",
    "        x_val = x[:, -1].unsqueeze(1)\n",
    "    \n",
    "        # project the value separately and then fuse\n",
    "        val_proj = self.value_proj(x_val)\n",
    "        fused_input = torch.cat([x_emb, val_proj], dim=1)\n",
    "    \n",
    "        z = self.encoder(fused_input)\n",
    "        \n",
    "        recon_emb = self.embedding_decoder(z)\n",
    "        recon_val = self.value_decoder(z)\n",
    "        \n",
    "        return recon_emb, recon_val\n",
    "    \n",
    "    # def compute_losses(self, x, target, train):\n",
    "    #     recon_emb, recon_val = self(x)\n",
    "\n",
    "    #     target_emb = target[:, :-1]\n",
    "    #     target_val = target[:, -1].unsqueeze(1)\n",
    "\n",
    "    #     embedding_loss = self.loss_fn(recon_emb, target_emb)\n",
    "    #     value_loss = self.loss_fn(recon_val, target_val)\n",
    "    #     loss = self.hparams.alpha_embed * embedding_loss + self.hparams.alpha_value * value_loss\n",
    "\n",
    "    #     cos_sim = cosine_similarity(recon_emb, target_emb, dim=1).mean()\n",
    "    #     mae_value = l1_loss(recon_val, target_val)\n",
    "    #     euclidean_dist = torch.norm(recon_emb - target_emb, dim=1).mean()\n",
    "\n",
    "    #     if train:\n",
    "    #         self.train_r2.update(recon_val.detach(), target_val.detach())\n",
    "    #         r2_value = self.train_r2.compute()\n",
    "    #     else:\n",
    "    #         self.val_r2.update(recon_val.detach(), target_val.detach())\n",
    "    #         r2_value = self.val_r2.compute()\n",
    "\n",
    "    #     return loss, embedding_loss, value_loss, cos_sim, mae_value, euclidean_dist, r2_value\n",
    "    def compute_losses(self, x, target, scaler=None, train=False):\n",
    "        recon_emb, recon_val = self(x)\n",
    "\n",
    "        target_emb = target[:, :-1]\n",
    "        target_val = target[:, -1].unsqueeze(1)\n",
    "\n",
    "        if scaler is not None and isinstance(scaler, (list, tuple)):\n",
    "            recon_val_np = recon_val.detach().cpu().numpy()\n",
    "            target_val_np = target_val.detach().cpu().numpy()\n",
    "\n",
    "            # Inverse transform per sample\n",
    "            recon_val_orig = np.stack([\n",
    "                s.inverse_transform(r.reshape(-1, 1)).flatten()\n",
    "                for s, r in zip(scaler, recon_val_np)\n",
    "            ])\n",
    "            target_val_orig = np.stack([\n",
    "                s.inverse_transform(t.reshape(-1, 1)).flatten()\n",
    "                for s, t in zip(scaler, target_val_np)\n",
    "            ])\n",
    "\n",
    "            recon_val_orig = torch.tensor(recon_val_orig, dtype=torch.float32,\n",
    "                                        device=recon_val.device)\n",
    "            target_val_orig = torch.tensor(target_val_orig, dtype=torch.float32,\n",
    "                                        device=target_val.device)\n",
    "        else:\n",
    "            raise Exception(\"Scaler not implemented\")\n",
    "            # If scaler is not implemented, use scaled values directly\n",
    "            # recon_val_orig = recon_val\n",
    "            # target_val_orig = target_val\n",
    "\n",
    "        # non-scaled\n",
    "        embedding_loss = self.loss_fn(recon_emb, target_emb)\n",
    "\n",
    "        # scaled\n",
    "        value_loss = self.loss_fn(recon_val, target_val)\n",
    "        loss = self.hparams.alpha_embed * embedding_loss + \\\n",
    "            self.hparams.alpha_value * value_loss\n",
    "\n",
    "        # non-scaled\n",
    "        cos_sim = cosine_similarity(recon_emb, target_emb, dim=1).mean()\n",
    "        euclidean_dist = torch.norm(recon_emb - target_emb, dim=1).mean()\n",
    "\n",
    "        # non-scaled\n",
    "        mae = l1_loss(recon_val_orig, target_val_orig)\n",
    "        relative_mae = (mae / (target_val_orig.abs().mean() + 1e-8)).clamp(0, 10)\n",
    "\n",
    "        if train:\n",
    "            self.train_r2.update(recon_val_orig, target_val_orig)\n",
    "            r2_value = self.train_r2.compute()\n",
    "        else:\n",
    "            self.val_r2.update(recon_val_orig, target_val_orig)\n",
    "            r2_value = self.val_r2.compute()\n",
    "\n",
    "        return loss, embedding_loss, value_loss, cos_sim, mae, relative_mae, \\\n",
    "            euclidean_dist, r2_value\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        if len(batch) == 3:\n",
    "            x, target, scaler = batch\n",
    "        else:\n",
    "            x, target = batch\n",
    "            scaler = None\n",
    "\n",
    "        loss, embedding_loss, value_loss, cos_sim, mae, relative_mae, euclidean_dist, r2_value = (\n",
    "            self.compute_losses(x, target, scaler, train=True)\n",
    "        )\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_embedding_loss\", embedding_loss)\n",
    "        self.log(\"train_value_loss\", value_loss)\n",
    "        self.log(\"train_embedding_cos_sim\", cos_sim)\n",
    "        self.log(\"train_value_mae\", mae)\n",
    "        self.log(\"train_value_relative_mae\", relative_mae)\n",
    "        self.log(\"train_embedding_euclidean\", euclidean_dist)\n",
    "        self.log(\"train_value_r2\", r2_value)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        if len(batch) == 3:\n",
    "            x, target, scaler = batch\n",
    "        else:\n",
    "            x, target = batch\n",
    "            scaler = None\n",
    "\n",
    "        loss, embedding_loss, value_loss, cos_sim, mae, relative_mae, euclidean_dist, r2_value = (\n",
    "            self.compute_losses(x, target, scaler, train=False)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_embedding_loss\", embedding_loss)\n",
    "        self.log(\"val_value_loss\", value_loss)\n",
    "        self.log(\"val_embedding_cos_sim\", cos_sim)\n",
    "        self.log(\"val_value_mae\", mae)\n",
    "        self.log(\"val_value_relative_mae\", relative_mae)\n",
    "        self.log(\"val_embedding_euclidean\", euclidean_dist)\n",
    "        self.log(\"val_value_r2\", r2_value)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.train_r2.reset()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.val_r2.reset()\n",
    "\n",
    "\n",
    "    # def configure_optimizers(self):\n",
    "    #     return torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        \n",
    "        # Use CosineAnnealingLR with T_max=15 and eta_min=1e-6 (matches your 15 epochs)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=15, eta_min=1e-6)\n",
    "        \n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccdb991-b887-4847-968b-8d9fb4b9517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning\n",
    "\n",
    "# import os\n",
    "# import optuna\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "# from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from torch.utils.data import DataLoader\n",
    "# from utils.pytorch import get_device\n",
    "\n",
    "# device = get_device()\n",
    "\n",
    "# # === CONFIG ===\n",
    "# OUTPUT_PATH = \"data/stage1\"\n",
    "# os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "# OPTUNA_DB_PATH = os.path.join(OUTPUT_PATH, \"optuna_study.db\")\n",
    "# EPOCHS = 3\n",
    "# PATIENCE = 5\n",
    "# VAL_SPLIT = 0.2\n",
    "\n",
    "# def objective(trial):\n",
    "#     batch_size = trial.suggest_int(\"batch_size\", 8, 64, step=8)\n",
    "#     lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "#     latent_dim = trial.suggest_int(\"latent_dim\", 32, 128, step=32)\n",
    "#     dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.2, step=0.1)\n",
    "#     weight_decay = trial.suggest_float(\"weight_decay\", 1e-8, 1e-4, log=True)\n",
    "#     gradient_clip = trial.suggest_float(\"gradient_clip\", 0.0, 1.0, step=0.1)\n",
    "\n",
    "#     # # 80/20 Train/Val Split\n",
    "#     # split = int(len(scaled_tuples) * (1 - VAL_SPLIT))\n",
    "#     # train_data = scaled_tuples[:split]\n",
    "#     # val_data = scaled_tuples[split:]\n",
    "\n",
    "#      # === Sample Subset for Faster Debugging ===\n",
    "#     SAMPLE_SIZE = 500_000\n",
    "#     subset = scaled_tuples[:SAMPLE_SIZE]\n",
    "    \n",
    "#     # 80/20 Train/Val Split\n",
    "#     split = int(len(subset) * (1 - VAL_SPLIT))\n",
    "#     train_data = subset[:split]\n",
    "#     val_data = subset[split:]\n",
    "\n",
    "#     train_loader = DataLoader(\n",
    "#         ConceptValueDataset(train_data, embedding_map, device=device, value_noise_std=0.005, train=True),\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=True\n",
    "#     )\n",
    "    \n",
    "#     val_loader = DataLoader(\n",
    "#         ConceptValueDataset(val_data, embedding_map, device=device, value_noise_std=0.00, train=False),\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=False\n",
    "#     )\n",
    "\n",
    "#     input_dim = len(next(iter(embedding_map.values()))) + 1\n",
    "\n",
    "#     model = Stage1Autoencoder(\n",
    "#         input_dim=input_dim,\n",
    "#         latent_dim=latent_dim,\n",
    "#         dropout_rate=dropout_rate,\n",
    "#         lr=lr,\n",
    "#         batch_size=batch_size,\n",
    "#         weight_decay=weight_decay,\n",
    "#         gradient_clip=gradient_clip\n",
    "#     )\n",
    "\n",
    "#     early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, verbose=True, mode=\"min\")\n",
    "\n",
    "#     model_checkpoint = ModelCheckpoint(\n",
    "#         dirpath=OUTPUT_PATH,\n",
    "#         filename=\"best_model_trial_{trial.number}\",\n",
    "#         monitor=\"val_loss\",\n",
    "#         mode=\"min\",\n",
    "#         save_top_k=1,\n",
    "#         verbose=True\n",
    "#     )\n",
    "\n",
    "#     trainer = pl.Trainer(\n",
    "#         max_epochs=EPOCHS,\n",
    "#         logger=TensorBoardLogger(OUTPUT_PATH, name=\"stage1_autoencoder\"),\n",
    "#         callbacks=[early_stop_callback, model_checkpoint],\n",
    "#         accelerator=\"auto\",\n",
    "#         devices=1,\n",
    "#         gradient_clip_val=gradient_clip\n",
    "#     )\n",
    "\n",
    "#     trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "#     return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "# # === Optuna Study ===\n",
    "# study = optuna.create_study(direction=\"minimize\",\n",
    "#                             storage=f\"sqlite:///{OPTUNA_DB_PATH}\",\n",
    "#                             load_if_exists=True)\n",
    "# study.optimize(objective, n_trials=25)\n",
    "\n",
    "# print(\"Best params:\", study.best_params)\n",
    "# print(\"Best trial value:\", study.best_trial.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c278a0d0-19d7-47ca-95ba-5d972553b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Instantiate dataset\n",
    "# dataset = ConceptValueDataset(scaled_tuples, embedding_map)\n",
    "\n",
    "# # Sample inspection\n",
    "# sample_x, sample_y = dataset[0]\n",
    "# print(\"Sample input:\", sample_x)\n",
    "# print(\"Min:\", sample_x.min().item(), \"Max:\", sample_x.max().item())\n",
    "# print(\"Mean:\", sample_x.mean().item(), \"Std:\", sample_x.std().item())\n",
    "# print(\"Input dim:\", sample_x.shape[0], \"Target dim:\", sample_y.shape[0])\n",
    "\n",
    "# # Optional: test batch loading\n",
    "# loader = DataLoader(dataset, batch_size=4)\n",
    "# for xb, yb in loader:\n",
    "#     print(\"Batch shape:\", xb.shape)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47484ee4-1c73-48be-8024-e368d977c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "import os\n",
    "import optuna\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.pytorch import get_device\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "# === CONFIG ===\n",
    "OUTPUT_PATH = \"data/stage1_23a\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "OPTUNA_DB_PATH = os.path.join(OUTPUT_PATH, \"optuna_study.db\")\n",
    "EPOCHS = 1000\n",
    "PATIENCE = 5\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "# 80/20 Train/Val Split\n",
    "split = int(len(scaled_tuples) * (1 - VAL_SPLIT))\n",
    "train_data = scaled_tuples[:split]\n",
    "val_data = scaled_tuples[split:]\n",
    "\n",
    "# model = Stage1Autoencoder.load_from_checkpoint(\"data/stage1.tuning/best_model_trial_trial.number=0-v23.ckpt\")\n",
    "model = Stage1Autoencoder()\n",
    "\n",
    "batch_size = model.hparams.batch_size\n",
    "gradient_clip = model.hparams.gradient_clip\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     ConceptValueDataset(train_data, embedding_map, device=device, value_noise_std=0.005, train=True),\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# val_loader = DataLoader(\n",
    "#     ConceptValueDataset(val_data, embedding_map, device=device, value_noise_std=0.00, train=False),\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False\n",
    "# )\n",
    "train_loader = DataLoader(\n",
    "    ConceptValueDataset(\n",
    "        train_data,\n",
    "        embedding_map,\n",
    "        device=device,\n",
    "        value_noise_std=0.005,\n",
    "        train=True,\n",
    "        scalers=scalers,\n",
    "        return_scaler=True\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_with_scaler\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    ConceptValueDataset(\n",
    "        val_data,\n",
    "        embedding_map,\n",
    "        device=device,\n",
    "        value_noise_std=0.00,\n",
    "        train=False,\n",
    "        scalers=scalers,\n",
    "        return_scaler=True\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_with_scaler\n",
    ")\n",
    "\n",
    "\n",
    "input_dim = len(next(iter(embedding_map.values()))) + 1\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, verbose=True, mode=\"min\")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    dirpath=OUTPUT_PATH,\n",
    "    filename=\"stage1_resume\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    logger=TensorBoardLogger(OUTPUT_PATH, name=\"stage1_autoencoder\"),\n",
    "    callbacks=[early_stop_callback, model_checkpoint],\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    gradient_clip_val=gradient_clip\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca87a09-65c9-42ee-9310-42558e6df0dc",
   "metadata": {},
   "source": [
    "# Conceptual Draft\n",
    "\n",
    "Stage 1 learns semantic+quantitative embeddings for individual concept/unit/value triplets.\n",
    "\n",
    "Stage 2 learns how to aggregate and contextualize those embeddings into higher-order units (i.e., financial statements).\n",
    "\n",
    "Stage 3 learns how to model temporal dynamics and structural evolution across filings â€” a full hierarchy of understanding.\n",
    "\n",
    "This pipeline could encode an entire company's financial narrative into vector space.\n",
    "\n",
    "Itâ€™s structured like language modeling, but for accounting â€” and thatâ€™s what makes it potentially groundbreaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d8512-2ade-484a-94d4-31d84a497782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
