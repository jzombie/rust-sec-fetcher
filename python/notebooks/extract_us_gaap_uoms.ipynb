{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceaa3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# This snippet ensures consistent import paths across environments.\n",
    "# When running notebooks via JupyterLab's web UI, the current working\n",
    "# directory is often different (e.g., /notebooks) compared to VS Code,\n",
    "# which typically starts at the project root. This handles that by \n",
    "# retrying the import after changing to the parent directory.\n",
    "# \n",
    "# Include this at the top of every notebook to standardize imports\n",
    "# across development environments.\n",
    "\n",
    "try:\n",
    "    from utils.os import chdir_to_git_root\n",
    "except ModuleNotFoundError:\n",
    "    os.chdir(Path.cwd().parent)\n",
    "    print(f\"Retrying import from: {os.getcwd()}\")\n",
    "    from utils.os import chdir_to_git_root\n",
    "\n",
    "chdir_to_git_root(\"python\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97813a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from db import DB\n",
    "\n",
    "# === Extraction Logic ===\n",
    "def extract_concept_unit_value_tuples(data_dir, valid_concepts):\n",
    "    rows = []\n",
    "    unit_values = defaultdict(list)\n",
    "    unit_concepts = defaultdict(set)\n",
    "    non_numeric_units = set()\n",
    "\n",
    "    csv_files = []\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "\n",
    "    for path in tqdm(csv_files, desc=\"Scanning CSV files\"):\n",
    "        try:\n",
    "            df = pd.read_csv(path, low_memory=False)\n",
    "            tag_columns = [col for col in df.columns if col in valid_concepts]\n",
    "            if not tag_columns:\n",
    "                continue\n",
    "\n",
    "            for col in tag_columns:\n",
    "                for val in df[col].dropna().astype(str):\n",
    "                    if \"::\" not in val:\n",
    "                        continue\n",
    "                    val_part, unit_part = val.split(\"::\", 1)\n",
    "                    unit_part = unit_part.strip().upper()\n",
    "                    try:\n",
    "                        num_val = float(val_part.strip())\n",
    "                        rows.append((col, unit_part, num_val))\n",
    "                        unit_values[unit_part].append(num_val)\n",
    "                        unit_concepts[unit_part].add(col)\n",
    "                    except ValueError:\n",
    "                        non_numeric_units.add(unit_part)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Skipped {path}: {e}\")\n",
    "\n",
    "    return rows, unit_values, unit_concepts, non_numeric_units, csv_files\n",
    "\n",
    "# === Main Execution ===\n",
    "data_dir = \"../data/us-gaap\"\n",
    "db = DB()\n",
    "concept_df = db.get(\"SELECT name FROM us_gaap_concept\", [\"name\"])\n",
    "valid_concepts = set(concept_df[\"name\"].values)\n",
    "\n",
    "concept_unit_value_tuples, unit_values, unit_concepts, non_numeric_units, csv_files = extract_concept_unit_value_tuples(data_dir, valid_concepts)\n",
    "\n",
    "print(f\"\\nâœ… Scanned {len(csv_files)} files.\")\n",
    "print(f\"ðŸ“¦ Found {len(unit_values)} numeric units and {len(non_numeric_units)} non-numeric units.\")\n",
    "\n",
    "for unit, values in sorted(unit_values.items()):\n",
    "    arr = np.array(values)\n",
    "    print(f\"ðŸ”¹ {unit}\")\n",
    "    print(f\"   Count: {len(arr)}\")\n",
    "    print(f\"   Min:   {arr.min():,.4f}\")\n",
    "    print(f\"   Max:   {arr.max():,.4f}\")\n",
    "    print(f\"   Mean:  {arr.mean():,.4f}\")\n",
    "    print(f\"   Std:   {arr.std():,.4f}\")\n",
    "    print(f\"   Concepts: {', '.join(sorted(unit_concepts[unit]))}\")\n",
    "\n",
    "if non_numeric_units:\n",
    "    print(\"\\nâš ï¸ Non-numeric units encountered:\")\n",
    "    for unit in sorted(non_numeric_units):\n",
    "        print(f\"  - {unit}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d24251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nðŸ§® Total values extracted: {len(concept_unit_value_tuples):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b55b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Build concept/unit dataset\n",
    "concept_unit_pairs = []\n",
    "for unit, concepts in unit_concepts.items():\n",
    "    for concept in concepts:\n",
    "        concept_unit_pairs.append((concept, unit))\n",
    "\n",
    "# Convert to DataFrame\n",
    "# concept_unit_df = pd.DataFrame(concept_unit_pairs,\n",
    "#                                columns=[\"concept\", \"unit\"])\n",
    "# concept_unit_df.to_csv(\"data/concept_unit_pairs.csv\", index=False)\n",
    "# print(\"âœ… data/concept_unit_pairs.csv saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a50b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "from utils import generate_us_gaap_description\n",
    "\n",
    "input_texts = [f\"{generate_us_gaap_description(concept)} measured in {unit}\" for concept, unit in concept_unit_pairs]\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
    "model.to(device)\n",
    "\n",
    "def encode_on_device(texts, model, batch_size=64):\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        tokens = model.tokenize(batch)\n",
    "        tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "        with torch.no_grad():\n",
    "            output = model.forward(tokens)\n",
    "            embeddings = output[\"sentence_embedding\"]\n",
    "        all_embeddings.append(embeddings.cpu())\n",
    "    return torch.cat(all_embeddings).numpy()\n",
    "\n",
    "embeddings = encode_on_device(input_texts, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c5b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # embeddings: np.ndarray of shape (N, 1024)\n",
    "# pca = PCA()\n",
    "# pca.fit(embeddings)\n",
    "\n",
    "# explained = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# plt.plot(np.arange(1, len(explained)+1), explained)\n",
    "# plt.xlabel(\"Number of PCA components\")\n",
    "# plt.ylabel(\"Cumulative explained variance\")\n",
    "# plt.grid(True)\n",
    "# plt.axhline(0.95, color='red', linestyle='--')  # e.g. 95% threshold\n",
    "# plt.title(\"Explained Variance vs PCA Components\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a1a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "\n",
    "# Assuming `embeddings` is your (N, 1024) array\n",
    "n_components = 200  # or 128 if you're more memory-conscious\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Fit PCA and transform the embeddings\n",
    "compressed = pca.fit_transform(embeddings)\n",
    "\n",
    "# # Save PCA model and compressed embeddings\n",
    "# joblib.dump(pca, \"pca_model.joblib\")\n",
    "# np.save(\"concept_uom_embeddings_pca.npy\", compressed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab25306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "import numpy as np\n",
    "\n",
    "def plot_embeddings(embeddings, labels=None, title=\"Embedding Scatterplot\"):\n",
    "    \"\"\"\n",
    "    Display a 2D or 3D scatterplot of the compressed embeddings.\n",
    "\n",
    "    Parameters:\n",
    "        embeddings (np.ndarray): Array of shape (N, 2) or (N, 3)\n",
    "        labels (List[str], optional): Labels to annotate points (optional)\n",
    "        title (str): Plot title\n",
    "    \"\"\"\n",
    "    dim = embeddings.shape[1]\n",
    "    assert dim in (2, 3), \"Embeddings must be 2D or 3D for scatterplot\"\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "    if dim == 3:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(*embeddings.T, s=10, alpha=0.7)\n",
    "    else:\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(embeddings[:, 0], embeddings[:, 1], s=10, alpha=0.7)\n",
    "\n",
    "    if labels is not None:\n",
    "        for i, label in enumerate(labels):\n",
    "            if dim == 3:\n",
    "                ax.text(*embeddings[i], label, fontsize=6)\n",
    "            else:\n",
    "                ax.text(embeddings[i, 0], embeddings[i, 1], label, fontsize=6)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_embeddings(compressed[:, :2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5c762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hdbscan\n",
    "# import umap\n",
    "# import matplotlib.pyplot as plt\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # Cluster\n",
    "# # clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=5, cluster_selection_method=\"leaf\")\n",
    "# clusterer = hdbscan.HDBSCAN(min_cluster_size=20, min_samples=10)\n",
    "# labels = clusterer.fit_predict(compressed)  # PCA-reduced embeddings\n",
    "\n",
    "# # Group input_texts by cluster\n",
    "# clusters = defaultdict(list)\n",
    "# for idx, label in enumerate(labels):\n",
    "#     clusters[label].append(input_texts[idx])\n",
    "\n",
    "# # Print samples from each cluster\n",
    "# for cluster_id, examples in clusters.items():\n",
    "#     if cluster_id == -1:\n",
    "#         continue  # Skip noise\n",
    "#     print(f\"\\nðŸ“¦ Cluster {cluster_id} ({len(examples)} samples):\")\n",
    "#     for e in examples[:10]:\n",
    "#         print(f\"  - {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98055128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UMAP visualization\n",
    "# umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, metric=\"cosine\")\n",
    "# umap_2d = umap_model.fit_transform(compressed)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(umap_2d[:, 0], umap_2d[:, 1], c=labels, cmap=\"tab10\", s=5)\n",
    "# plt.title(\"Concept/UOM Embeddings Clustered\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e52bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({\n",
    "#     \"concept\": [c for c, _ in concept_unit_pairs],\n",
    "#     \"unit\": [u for _, u in concept_unit_pairs],\n",
    "#     \"cluster\": labels\n",
    "# })\n",
    "# grouped = df.groupby(\"cluster\")\n",
    "\n",
    "# for cluster_id, group in grouped:\n",
    "#     print(f\"\\nCluster {cluster_id} ({len(group)} items):\")\n",
    "#     print(group.head(10).to_string(index=False))\n",
    "\n",
    "# noise = df[df[\"cluster\"] == -1]\n",
    "\n",
    "# print(f\"Noise points: {len(noise)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_points = df[df[\"cluster\"] == -1][[\"concept\", \"unit\"]].reset_index(drop=True)\n",
    "\n",
    "# noise_points.to_csv(\"noise_points.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f259b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = joblib.load(\"pca_model.joblib\")\n",
    "# compressed = np.load(\"concept_uom_embeddings_pca.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a051cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Save both embeddings and tuples\n",
    "np.savez_compressed(\n",
    "    \"data/stage1_latents.npz\",\n",
    "    keys=np.array([f\"{c}::{u}\" for c, u in concept_unit_pairs]),\n",
    "    embeddings=compressed,\n",
    "    concept_unit_value_tuples=np.array(concept_unit_value_tuples, dtype=object)\n",
    ")\n",
    "\n",
    "print(f\"âœ… Saved {len(concept_unit_value_tuples):,} tuples and {len(compressed):,} embeddings to 'stage1_latents.npz'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc170ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load saved latent data\n",
    "data = np.load(\"data/stage1_latents.npz\", allow_pickle=True)\n",
    "\n",
    "# Build embedding map\n",
    "embedding_map = {\n",
    "    tuple(key.split(\"::\", 1)): vec\n",
    "    for key, vec in zip(data[\"keys\"], data[\"embeddings\"])\n",
    "}\n",
    "\n",
    "# Load concept-unit-value tuples\n",
    "concept_unit_value_tuples = data[\"concept_unit_value_tuples\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# TODO: Document why this happens before data splitting in this particular dataset\n",
    "\n",
    "# Step 1: Group values per (concept, unit)\n",
    "grouped = defaultdict(list)\n",
    "for concept, unit, value in concept_unit_value_tuples:\n",
    "    grouped[(concept, unit)].append(value)\n",
    "\n",
    "# Step 2: Fit individual scalers and transform\n",
    "scalers = {}\n",
    "scaled_tuples = []\n",
    "\n",
    "for key, vals in tqdm(grouped.items(), desc=\"Scaling per concept/unit\"):\n",
    "    vals_np = np.array(vals).reshape(-1, 1)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_vals = scaler.fit_transform(vals_np).flatten()\n",
    "    scalers[key] = scaler\n",
    "\n",
    "    # Rebuild tuples\n",
    "    scaled_tuples.extend((key[0], key[1], v) for v in scaled_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b584bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.pytorch import seed_everything\n",
    "import numpy as np\n",
    "from torch.nn.functional import cosine_similarity, l1_loss\n",
    "\n",
    "# Stage 1 dataset: concept+uom embedding + value\n",
    "class ConceptValueDataset(Dataset):\n",
    "    def __init__(self, scaled_tuples, embedding_lookup, device: torch.tensor, value_noise_std=0.0, train=False):\n",
    "        self.rows = scaled_tuples\n",
    "        self.lookup = embedding_lookup\n",
    "        self.value_noise_std = value_noise_std\n",
    "        self.train = train\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        concept, unit, value = self.rows[idx]\n",
    "\n",
    "        try:\n",
    "            embedding = self.lookup[(concept, unit)]\n",
    "        except KeyError:\n",
    "            raise ValueError(f\"Missing embedding for ({concept}, {unit})\")\n",
    "\n",
    "        if self.train and self.value_noise_std > 0:\n",
    "            value += np.random.normal(0, self.value_noise_std)\n",
    "\n",
    "        x = torch.tensor(np.concatenate([embedding, [value]]), dtype=torch.float32, device=self.device)\n",
    "        y = torch.tensor(np.concatenate([embedding, [value]]), dtype=torch.float32, device=self.device)\n",
    "        return x, y\n",
    "\n",
    "# LightningModule\n",
    "class Stage1Autoencoder(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim=201,\n",
    "            latent_dim=64,\n",
    "            dropout_rate=0.1,\n",
    "            lr=1e-3,\n",
    "            batch_size=24,\n",
    "            gradient_clip=1.0,\n",
    "            alpha_embed=0.5,\n",
    "            alpha_value=1.0,\n",
    "            weight_decay=0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.value_proj = nn.Sequential(\n",
    "            nn.Linear(1, 32),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, self.hparams.latent_dim),\n",
    "            nn.LayerNorm(self.hparams.latent_dim)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim - 1 + self.hparams.latent_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(256, latent_dim)\n",
    "        )\n",
    "        \n",
    "        self.embedding_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(256, input_dim - 1)\n",
    "        )\n",
    "\n",
    "        self.value_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, input_dim]\n",
    "        x_emb = x[:, :-1]\n",
    "        x_val = x[:, -1].unsqueeze(1)\n",
    "    \n",
    "        # project the value separately and then fuse\n",
    "        val_proj = self.value_proj(x_val)\n",
    "        fused_input = torch.cat([x_emb, val_proj], dim=1)\n",
    "    \n",
    "        z = self.encoder(fused_input)\n",
    "        \n",
    "        recon_emb = self.embedding_decoder(z)\n",
    "        recon_val = self.value_decoder(z)\n",
    "        \n",
    "        return recon_emb, recon_val\n",
    "    \n",
    "    def compute_losses(self, x, target):\n",
    "        recon_emb, recon_val = self(x)\n",
    "\n",
    "        target_emb = target[:, :-1]\n",
    "        target_val = target[:, -1].unsqueeze(1)\n",
    "\n",
    "        embedding_loss = self.loss_fn(recon_emb, target_emb)\n",
    "        value_loss = self.loss_fn(recon_val, target_val)\n",
    "        loss = self.hparams.alpha_embed * embedding_loss + self.hparams.alpha_value * value_loss\n",
    "\n",
    "        cos_sim = cosine_similarity(recon_emb, target_emb, dim=1).mean()\n",
    "        mae_value = l1_loss(recon_val, target_val)\n",
    "        euclidean_dist = torch.norm(recon_emb - target_emb, dim=1).mean()\n",
    "\n",
    "        ss_res = ((recon_val - target_val) ** 2).sum()\n",
    "        ss_tot = ((target_val - target_val.mean()) ** 2).sum()\n",
    "        r2_raw = 1 - ss_res / ss_tot\n",
    "        r2_value = torch.clamp(r2_raw, min=-10.0, max=1.0)\n",
    "\n",
    "        return loss, embedding_loss, value_loss, cos_sim, mae_value, euclidean_dist, r2_value\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, target = batch\n",
    "        loss, embedding_loss, value_loss, cos_sim, mae_value, euclidean_dist, r2_value = self.compute_losses(x, target)\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_embedding_loss\", embedding_loss)\n",
    "        self.log(\"train_value_loss\", value_loss)\n",
    "        self.log(\"train_embedding_cos_sim\", cos_sim)\n",
    "        self.log(\"train_value_mae\", mae_value)\n",
    "        self.log(\"train_embedding_euclidean\", euclidean_dist)\n",
    "        self.log(\"train_value_r2\", r2_value)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, target = batch\n",
    "        loss, embedding_loss, value_loss, cos_sim, mae_value, euclidean_dist, r2_value = self.compute_losses(x, target)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_embedding_loss\", embedding_loss)\n",
    "        self.log(\"val_value_loss\", value_loss)\n",
    "        self.log(\"val_embedding_cos_sim\", cos_sim)\n",
    "        self.log(\"val_value_mae\", mae_value)\n",
    "        self.log(\"val_embedding_euclidean\", euclidean_dist)\n",
    "        self.log(\"val_value_r2\", r2_value)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccdb991-b887-4847-968b-8d9fb4b9517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.pytorch import get_device\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "# === CONFIG ===\n",
    "OUTPUT_PATH = \"data/stage1\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "OPTUNA_DB_PATH = os.path.join(OUTPUT_PATH, \"optuna_study.db\")\n",
    "EPOCHS = 3\n",
    "PATIENCE = 5\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 8, 64, step=8)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    latent_dim = trial.suggest_int(\"latent_dim\", 32, 128, step=32)\n",
    "    # dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.2, step=0.1)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-8, 1e-4, log=True)\n",
    "    gradient_clip = trial.suggest_float(\"gradient_clip\", 0.0, 1.0, step=0.1)\n",
    "\n",
    "    # === Sample Subset for Faster Debugging ===\n",
    "    SAMPLE_SIZE = 500_000\n",
    "    subset = scaled_tuples[:SAMPLE_SIZE]\n",
    "    \n",
    "    # 80/20 Train/Val Split\n",
    "    split = int(len(subset) * (1 - VAL_SPLIT))\n",
    "    train_data = scaled_tuples[:split]\n",
    "    val_data = scaled_tuples[split:]\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        ConceptValueDataset(train_data, embedding_map, device=device, value_noise_std=0.005, train=True),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        ConceptValueDataset(val_data, embedding_map, device=device, value_noise_std=0.00, train=False),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    input_dim = len(next(iter(embedding_map.values()))) + 1\n",
    "\n",
    "    model = Stage1Autoencoder(\n",
    "        input_dim=input_dim,\n",
    "        latent_dim=latent_dim,\n",
    "        # dropout_rate=dropout_rate,\n",
    "        lr=lr,\n",
    "        batch_size=batch_size,\n",
    "        weight_decay=weight_decay,\n",
    "        gradient_clip=gradient_clip\n",
    "    )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, verbose=True, mode=\"min\")\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        dirpath=OUTPUT_PATH,\n",
    "        filename=\"best_model_trial_{trial.number}\",\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=1,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=EPOCHS,\n",
    "        logger=TensorBoardLogger(OUTPUT_PATH, name=\"stage1_autoencoder\"),\n",
    "        callbacks=[early_stop_callback, model_checkpoint],\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        gradient_clip_val=gradient_clip\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "# === Optuna Study ===\n",
    "study = optuna.create_study(direction=\"minimize\",\n",
    "                            storage=f\"sqlite:///{OPTUNA_DB_PATH}\",\n",
    "                            load_if_exists=True)\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best params:\", study.best_params)\n",
    "print(\"Best trial value:\", study.best_trial.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c278a0d0-19d7-47ca-95ba-5d972553b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Instantiate dataset\n",
    "# dataset = ConceptValueDataset(scaled_tuples, embedding_map)\n",
    "\n",
    "# # Sample inspection\n",
    "# sample_x, sample_y = dataset[0]\n",
    "# print(\"Sample input:\", sample_x)\n",
    "# print(\"Min:\", sample_x.min().item(), \"Max:\", sample_x.max().item())\n",
    "# print(\"Mean:\", sample_x.mean().item(), \"Std:\", sample_x.std().item())\n",
    "# print(\"Input dim:\", sample_x.shape[0], \"Target dim:\", sample_y.shape[0])\n",
    "\n",
    "# # Optional: test batch loading\n",
    "# loader = DataLoader(dataset, batch_size=4)\n",
    "# for xb, yb in loader:\n",
    "#     print(\"Batch shape:\", xb.shape)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47484ee4-1c73-48be-8024-e368d977c014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
