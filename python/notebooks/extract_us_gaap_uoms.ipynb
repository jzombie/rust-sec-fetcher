{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceaa3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# This snippet ensures consistent import paths across environments.\n",
    "# When running notebooks via JupyterLab's web UI, the current working\n",
    "# directory is often different (e.g., /notebooks) compared to VS Code,\n",
    "# which typically starts at the project root. This handles that by \n",
    "# retrying the import after changing to the parent directory.\n",
    "# \n",
    "# Include this at the top of every notebook to standardize imports\n",
    "# across development environments.\n",
    "\n",
    "try:\n",
    "    from utils.os import chdir_to_git_root\n",
    "except ModuleNotFoundError:\n",
    "    os.chdir(Path.cwd().parent)\n",
    "    print(f\"Retrying import from: {os.getcwd()}\")\n",
    "    from utils.os import chdir_to_git_root\n",
    "\n",
    "chdir_to_git_root(\"python\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97813a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from db import DB\n",
    "\n",
    "data_dir = \"../data/us-gaap\"\n",
    "db = DB()\n",
    "\n",
    "# Step 1: Get all US GAAP concept names from the DB\n",
    "concept_df = db.get(\"SELECT name FROM us_gaap_concept\", [\"name\"])\n",
    "valid_concepts = set(concept_df[\"name\"].values)\n",
    "\n",
    "# Step 2: Prepare structures to collect values per unit and concept\n",
    "unit_values = defaultdict(list)\n",
    "unit_concepts = defaultdict(set)\n",
    "non_numeric_units = set()\n",
    "\n",
    "# Step 3: Traverse CSV files\n",
    "csv_files = []\n",
    "for root, _, files in os.walk(data_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            csv_files.append(os.path.join(root, file))\n",
    "\n",
    "for path in tqdm(csv_files, desc=\"Scanning CSVs\"):\n",
    "    try:\n",
    "        df = pd.read_csv(path, low_memory=False)\n",
    "\n",
    "        # Filter to only valid GAAP concept columns\n",
    "        tag_columns = [col for col in df.columns if col in valid_concepts]\n",
    "        if not tag_columns:\n",
    "            continue\n",
    "\n",
    "        for col in tag_columns:\n",
    "            for val in df[col].dropna().astype(str):\n",
    "                if \"::\" not in val:\n",
    "                    continue\n",
    "                val_part, unit_part = val.split(\"::\", 1)\n",
    "\n",
    "                # Obtain unit part and normalize to uppercase\n",
    "                unit_part = unit_part.strip().upper()\n",
    "\n",
    "                try:\n",
    "                    num_val = float(val_part.strip())\n",
    "                    unit_values[unit_part].append(num_val)\n",
    "                    unit_concepts[unit_part].add(col)\n",
    "                except ValueError:\n",
    "                    non_numeric_units.add(unit_part)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipped {path} due to error: {e}\")\n",
    "\n",
    "# Step 4: Report\n",
    "print(f\"\\n‚úÖ Scanned {len(csv_files)} files.\")\n",
    "print(f\"üì¶ Found {len(unit_values)} numeric units and {len(non_numeric_units)} non-numeric units.\")\n",
    "\n",
    "# Step 5: Show stats per numeric unit\n",
    "for unit, values in sorted(unit_values.items()):\n",
    "    arr = np.array(values)\n",
    "    print(f\"üîπ {unit}\")\n",
    "    print(f\"   Count: {len(arr)}\")\n",
    "    print(f\"   Min:   {arr.min():,.4f}\")\n",
    "    print(f\"   Max:   {arr.max():,.4f}\")\n",
    "    print(f\"   Mean:  {arr.mean():,.4f}\")\n",
    "    print(f\"   Std:   {arr.std():,.4f}\")\n",
    "    print(f\"   Concepts: {', '.join(sorted(unit_concepts[unit]))}\")\n",
    "\n",
    "# Step 6: Optionally show non-numeric units\n",
    "if non_numeric_units:\n",
    "    print(\"\\n‚ö†Ô∏è Non-numeric units encountered:\")\n",
    "    for unit in sorted(non_numeric_units):\n",
    "        print(f\"  - {unit}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b55b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Build concept/unit dataset\n",
    "concept_unit_pairs = []\n",
    "for unit, concepts in unit_concepts.items():\n",
    "    for concept in concepts:\n",
    "        concept_unit_pairs.append((concept, unit))\n",
    "\n",
    "# Convert to DataFrame\n",
    "# concept_unit_df = pd.DataFrame(concept_unit_pairs,\n",
    "#                                columns=[\"concept\", \"unit\"])\n",
    "# concept_unit_df.to_csv(\"data/concept_unit_pairs.csv\", index=False)\n",
    "# print(\"‚úÖ data/concept_unit_pairs.csv saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a50b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "from utils import generate_us_gaap_description\n",
    "\n",
    "input_texts = [f\"{generate_us_gaap_description(concept)} measured in {unit}\" for concept, unit in concept_unit_pairs]\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
    "model.to(device)\n",
    "\n",
    "def encode_on_device(texts, model, batch_size=64):\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        tokens = model.tokenize(batch)\n",
    "        tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "        with torch.no_grad():\n",
    "            output = model.forward(tokens)\n",
    "            embeddings = output[\"sentence_embedding\"]\n",
    "        all_embeddings.append(embeddings.cpu())\n",
    "    return torch.cat(all_embeddings).numpy()\n",
    "\n",
    "embeddings = encode_on_device(input_texts, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c5b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# embeddings: np.ndarray of shape (N, 1024)\n",
    "pca = PCA()\n",
    "pca.fit(embeddings)\n",
    "\n",
    "explained = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.plot(np.arange(1, len(explained)+1), explained)\n",
    "plt.xlabel(\"Number of PCA components\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.grid(True)\n",
    "plt.axhline(0.95, color='red', linestyle='--')  # e.g. 95% threshold\n",
    "plt.title(\"Explained Variance vs PCA Components\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a1a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "\n",
    "# Assuming `embeddings` is your (N, 1024) array\n",
    "n_components = 150  # or 128 if you're more memory-conscious\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Fit PCA and transform the embeddings\n",
    "compressed = pca.fit_transform(embeddings)\n",
    "\n",
    "# # Save PCA model and compressed embeddings\n",
    "# joblib.dump(pca, \"pca_model.joblib\")\n",
    "# np.save(\"concept_uom_embeddings_pca.npy\", compressed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab25306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "import numpy as np\n",
    "\n",
    "def plot_embeddings(embeddings, labels=None, title=\"Embedding Scatterplot\"):\n",
    "    \"\"\"\n",
    "    Display a 2D or 3D scatterplot of the compressed embeddings.\n",
    "\n",
    "    Parameters:\n",
    "        embeddings (np.ndarray): Array of shape (N, 2) or (N, 3)\n",
    "        labels (List[str], optional): Labels to annotate points (optional)\n",
    "        title (str): Plot title\n",
    "    \"\"\"\n",
    "    dim = embeddings.shape[1]\n",
    "    assert dim in (2, 3), \"Embeddings must be 2D or 3D for scatterplot\"\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "    if dim == 3:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(*embeddings.T, s=10, alpha=0.7)\n",
    "    else:\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(embeddings[:, 0], embeddings[:, 1], s=10, alpha=0.7)\n",
    "\n",
    "    if labels is not None:\n",
    "        for i, label in enumerate(labels):\n",
    "            if dim == 3:\n",
    "                ax.text(*embeddings[i], label, fontsize=6)\n",
    "            else:\n",
    "                ax.text(embeddings[i, 0], embeddings[i, 1], label, fontsize=6)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_embeddings(compressed[:, :2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5c762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Cluster\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=5, cluster_selection_method=\"leaf\")\n",
    "labels = clusterer.fit_predict(compressed)  # PCA-reduced embeddings\n",
    "\n",
    "# Group input_texts by cluster\n",
    "clusters = defaultdict(list)\n",
    "for idx, label in enumerate(labels):\n",
    "    clusters[label].append(input_texts[idx])\n",
    "\n",
    "# Print samples from each cluster\n",
    "for cluster_id, examples in clusters.items():\n",
    "    if cluster_id == -1:\n",
    "        continue  # Skip noise\n",
    "    print(f\"\\nüì¶ Cluster {cluster_id} ({len(examples)} samples):\")\n",
    "    for e in examples[:10]:\n",
    "        print(f\"  - {e}\")\n",
    "\n",
    "# UMAP visualization\n",
    "umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, metric=\"cosine\")\n",
    "umap_2d = umap_model.fit_transform(compressed)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(umap_2d[:, 0], umap_2d[:, 1], c=labels, cmap=\"tab10\", s=5)\n",
    "plt.title(\"Concept/UOM Embeddings Clustered\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e52bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"concept\": [c for c, _ in concept_unit_pairs],\n",
    "    \"unit\": [u for _, u in concept_unit_pairs],\n",
    "    \"cluster\": labels\n",
    "})\n",
    "grouped = df.groupby(\"cluster\")\n",
    "\n",
    "for cluster_id, group in grouped:\n",
    "    print(f\"\\nCluster {cluster_id} ({len(group)} items):\")\n",
    "    print(group.head(10).to_string(index=False))\n",
    "\n",
    "noise = df[df[\"cluster\"] == -1]\n",
    "\n",
    "print(f\"Noise points: {len(noise)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_points = df[df[\"cluster\"] == -1][[\"concept\", \"unit\"]].reset_index(drop=True)\n",
    "\n",
    "noise_points.to_csv(\"noise_points.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f259b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = joblib.load(\"pca_model.joblib\")\n",
    "# compressed = np.load(\"concept_uom_embeddings_pca.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a051cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming these are already defined in your notebook\n",
    "# - `concept_unit_pairs` is a list of (concept, unit) tuples\n",
    "# - `compressed` is the corresponding array of embeddings\n",
    "\n",
    "# Convert keys to a structured array of strings\n",
    "keys = np.array([f\"{c}::{u}\" for c, u in concept_unit_pairs])\n",
    "\n",
    "# Save compressed embeddings\n",
    "np.savez_compressed(\"data/stage1_latents.npz\", keys=keys, embeddings=compressed)\n",
    "print(f\"‚úÖ Saved {len(keys)} embeddings to 'stage1_latents.npz'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc170ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# keys: [\"Revenue::USD\", \"Assets::EUR\", ...]\n",
    "# embeddings: numpy array of shape (N, 128)\n",
    "data = np.load(\"data/stage1_latents.npz\")\n",
    "embedding_map = {\n",
    "    tuple(key.split(\"::\", 1)): vec\n",
    "    for key, vec in zip(data[\"keys\"], data[\"embeddings\"])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54a9310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from db import DB\n",
    "from tqdm import tqdm\n",
    "\n",
    "db = DB()\n",
    "\n",
    "def extract_concept_unit_value_tuples(data_dir, valid_concepts):\n",
    "    rows = []\n",
    "    all_files = []\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                all_files.append(os.path.join(root, file))\n",
    "\n",
    "    for path in tqdm(all_files, desc=\"Scanning CSV files\"):\n",
    "        try:\n",
    "            df = pd.read_csv(path, low_memory=False)\n",
    "            tag_columns = [col for col in df.columns if col in valid_concepts]\n",
    "            for col in tag_columns:\n",
    "                for val in df[col].dropna().astype(str):\n",
    "                    if \"::\" not in val:\n",
    "                        continue\n",
    "                    val_part, unit_part = val.split(\"::\", 1)\n",
    "                    unit_part = unit_part.strip().upper()\n",
    "                    try:\n",
    "                        num_val = float(val_part.strip())\n",
    "                        rows.append((col, unit_part, num_val))\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipped {path}: {e}\")\n",
    "    return rows\n",
    "\n",
    "\n",
    "concept_df = db.get(\"SELECT name FROM us_gaap_concept\", [\"name\"])\n",
    "valid_concepts = set(concept_df[\"name\"].values)\n",
    "\n",
    "concept_unit_value_tuples = extract_concept_unit_value_tuples(\"../data/us-gaap\", valid_concepts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concept_unit_value_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Group values per (concept, unit)\n",
    "grouped = defaultdict(list)\n",
    "for concept, unit, value in concept_unit_value_tuples:\n",
    "    grouped[(concept, unit)].append(value)\n",
    "\n",
    "# Step 2: Fit individual scalers and transform\n",
    "scalers = {}\n",
    "scaled_tuples = []\n",
    "\n",
    "for key, vals in tqdm(grouped.items(), desc=\"Scaling per concept/unit\"):\n",
    "    vals_np = np.array(vals).reshape(-1, 1)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_vals = scaler.fit_transform(vals_np).flatten()\n",
    "    scalers[key] = scaler\n",
    "\n",
    "    # Rebuild tuples\n",
    "    scaled_tuples.extend((key[0], key[1], v) for v in scaled_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b584bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Stage 1 dataset: concept+uom embedding + value\n",
    "class ConceptValueDataset(Dataset):\n",
    "    def __init__(self, concept_unit_value_tuples, embedding_lookup):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            concept_unit_value_tuples: List of (concept, unit, value)\n",
    "            embedding_lookup: Dict[(concept, unit)] -> np.ndarray\n",
    "        \"\"\"\n",
    "        self.rows = concept_unit_value_tuples\n",
    "        self.lookup = embedding_lookup\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        concept, unit, value = self.rows[idx]\n",
    "\n",
    "        try:\n",
    "            embedding = self.lookup[(concept, unit)]\n",
    "        except KeyError:\n",
    "            raise ValueError(f\"Missing embedding for ({concept}, {unit})\")\n",
    "        x = torch.tensor(np.concatenate([embedding, [value]]), dtype=torch.float32)\n",
    "        y = torch.tensor(np.concatenate([embedding, [value]]), dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "# LightningModule\n",
    "class Stage1Autoencoder(pl.LightningModule):\n",
    "    def __init__(self, input_dim=129, latent_dim=64, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim)\n",
    "        )\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, target = batch\n",
    "        recon = self(x)\n",
    "\n",
    "        target_embedding = target[:, :-1]\n",
    "        target_value = target[:, -1].unsqueeze(1)\n",
    "        recon_embedding = recon[:, :-1]\n",
    "        recon_value = recon[:, -1].unsqueeze(1)\n",
    "\n",
    "        embedding_loss = self.loss_fn(recon_embedding, target_embedding)\n",
    "        value_loss = self.loss_fn(recon_value, target_value)\n",
    "        loss = 0.2 * embedding_loss + 1.0 * value_loss\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_embedding_loss\", embedding_loss, prog_bar=False)\n",
    "        self.log(\"train_value_loss\", value_loss, prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, target = batch\n",
    "        recon = self(x)\n",
    "\n",
    "        target_embedding = target[:, :-1]\n",
    "        target_value = target[:, -1].unsqueeze(1)\n",
    "        recon_embedding = recon[:, :-1]\n",
    "        recon_value = recon[:, -1].unsqueeze(1)\n",
    "\n",
    "        embedding_loss = self.loss_fn(recon_embedding, target_embedding)\n",
    "        value_loss = self.loss_fn(recon_value, target_value)\n",
    "        loss = 0.2 * embedding_loss + 1.0 * value_loss\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_embedding_loss\", embedding_loss, prog_bar=False)\n",
    "        self.log(\"val_value_loss\", value_loss, prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "# Example usage:\n",
    "dataset = ConceptValueDataset(concept_unit_value_tuples, embedding_map)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Determine true input dimension\n",
    "sample_x, _ = dataset[0]\n",
    "model = Stage1Autoencoder(input_dim=sample_x.shape[0], latent_dim=64)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=20, accelerator=\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "trainer.fit(model, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccdb991-b887-4847-968b-8d9fb4b9517d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
