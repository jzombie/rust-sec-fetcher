{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir('../')\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "[I 2025-04-07 20:23:30,571] A new study created in RDB with name: no-name-d877c87f-3d29-4b04-aaf1-dbe3cccc36bb\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /Volumes/2TB Storage Vault/rust-sec-fetcher/python/data/fine_tuned_gaap_classifier exists and is not empty.\n",
      "\n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | fc      | Sequential | 1.0 M  | train\n",
      "1 | dropout | Dropout    | 0      | train\n",
      "2 | fc_out  | Linear     | 1.0 M  | train\n",
      "-----------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.397     Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fbae137733746aeadd17d1c2a2f6994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e045f69e444e480288bd5a39b52171c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fe9f8627b54d158921687161a04b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.003\n",
      "Epoch 0, global step 484: 'val_loss' reached 0.00348 (best 0.00348), saving model to '/Volumes/2TB Storage Vault/rust-sec-fetcher/python/data/fine_tuned_gaap_classifier/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6eedf533e449d1b5c727de95a59b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.001\n",
      "Epoch 1, global step 968: 'val_loss' reached 0.00145 (best 0.00145), saving model to '/Volumes/2TB Storage Vault/rust-sec-fetcher/python/data/fine_tuned_gaap_classifier/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dbef6627b1b4798a6a9f0d79c820610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.001\n",
      "Epoch 2, global step 1452: 'val_loss' reached 0.00084 (best 0.00084), saving model to '/Volumes/2TB Storage Vault/rust-sec-fetcher/python/data/fine_tuned_gaap_classifier/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c5081ac3ea44889647418a33306f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.001\n",
      "Epoch 3, global step 1936: 'val_loss' reached 0.00055 (best 0.00055), saving model to '/Volumes/2TB Storage Vault/rust-sec-fetcher/python/data/fine_tuned_gaap_classifier/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39ec18f67344a68b906c924ed4635b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Epoch 4, global step 2420: 'val_loss' reached 0.00039 (best 0.00039), saving model to '/Volumes/2TB Storage Vault/rust-sec-fetcher/python/data/fine_tuned_gaap_classifier/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f139eab8df0483690f87a964b320d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Epoch 5, global step 2904: 'val_loss' reached 0.00028 (best 0.00028), saving model to '/Volumes/2TB Storage Vault/rust-sec-fetcher/python/data/fine_tuned_gaap_classifier/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb92d567aa848ac8075cb30af02607e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Epoch 6, global step 3388: 'val_loss' reached 0.00021 (best 0.00021), saving model to '/Volumes/2TB Storage Vault/rust-sec-fetcher/python/data/fine_tuned_gaap_classifier/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792dfc4512eb453e9b64ccebe1d89eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Epoch 7, global step 3872: 'val_loss' reached 0.00016 (best 0.00016), saving model to '/Volumes/2TB Storage Vault/rust-sec-fetcher/python/data/fine_tuned_gaap_classifier/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898571e753004efa94b5b6a91131f853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.000\n",
      "Epoch 8, global step 4356: 'val_loss' reached 0.00012 (best 0.00012), saving model to '/Volumes/2TB Storage Vault/rust-sec-fetcher/python/data/fine_tuned_gaap_classifier/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n",
      "[W 2025-04-07 20:24:15,498] Trial 0 failed with parameters: {'batch_size': 48, 'dropout_rate': 0.4, 'hidden_size': 256, 'num_heads': 8} because of the following error: NameError(\"name 'exit' is not defined\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1056, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 150, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 320, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 192, in run\n",
      "    self._optimizer_step(batch_idx, closure)\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 270, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 1302, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py\", line 154, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 239, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py\", line 123, in optimizer_step\n",
      "    return optimizer.step(closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 493, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/torch/optim/adamw.py\", line 220, in step\n",
      "    loss = closure()\n",
      "           ^^^^^^^^^\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py\", line 109, in _wrap_closure\n",
      "    closure_result = closure()\n",
      "                     ^^^^^^^^^\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 146, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 131, in closure\n",
      "    step_output = self._step_fn()\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 319, in _training_step\n",
      "    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 391, in training_step\n",
      "    return self.lightning_module.training_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/hn/cb9fbnv50kqdqz8zh4yth8p40000gn/T/ipykernel_83599/1836671996.py\", line 135, in training_step\n",
      "    loss = self.cosine_similarity_loss(transformed_input_embeddings, transformed_description_embeddings)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/hn/cb9fbnv50kqdqz8zh4yth8p40000gn/T/ipykernel_83599/1836671996.py\", line 161, in cosine_similarity_loss\n",
      "    cosine_sim = F.cosine_similarity(embeddings1, embeddings2, dim=-1)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/hn/cb9fbnv50kqdqz8zh4yth8p40000gn/T/ipykernel_83599/1836671996.py\", line 219, in objective\n",
      "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 65, in _call_and_handle_interrupt\n",
      "    exit(1)\n",
      "    ^^^^\n",
      "NameError: name 'exit' is not defined\n",
      "[W 2025-04-07 20:24:15,501] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:150\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:320\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    319\u001b[39m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m     batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:192\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m result = closure.consume_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:270\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/core/module.py:1302\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1278\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1279\u001b[39m \u001b[33;03mthe optimizer.\u001b[39;00m\n\u001b[32m   1280\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1300\u001b[39m \n\u001b[32m   1301\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1302\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py:154\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:239\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py:123\u001b[39m, in \u001b[36mPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m closure = partial(\u001b[38;5;28mself\u001b[39m._wrap_closure, model, optimizer, closure)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/torch/optim/optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/torch/optim/adamw.py:220\u001b[39m, in \u001b[36mAdamW.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py:109\u001b[39m, in \u001b[36mPrecision._wrap_closure\u001b[39m\u001b[34m(self, model, optimizer, closure)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03mhook is called.\u001b[39;00m\n\u001b[32m    104\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \n\u001b[32m    108\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m closure_result = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28mself\u001b[39m._after_closure(model, optimizer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:146\u001b[39m, in \u001b[36mClosure.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Optional[Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result.loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:131\u001b[39m, in \u001b[36mClosure.closure\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;129m@torch\u001b[39m.enable_grad()\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> ClosureResult:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m step_output.closure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:319\u001b[39m, in \u001b[36m_AutomaticOptimization._training_step\u001b[39m\u001b[34m(self, kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m trainer = \u001b[38;5;28mself\u001b[39m.trainer\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m training_step_output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.strategy.post_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:328\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:391\u001b[39m, in \u001b[36mStrategy.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mtraining_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 135\u001b[39m, in \u001b[36mAlignmentModel.training_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m# Compute training loss\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcosine_similarity_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_input_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_description_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28mself\u001b[39m.log(\u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m, loss, prog_bar=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 161\u001b[39m, in \u001b[36mAlignmentModel.cosine_similarity_loss\u001b[39m\u001b[34m(self, embeddings1, embeddings2)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcosine_similarity_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, embeddings1, embeddings2):\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Compute cosine similarity using PyTorch's functional API\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     cosine_sim = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# Convert cosine similarity to a loss value (1 - similarity)\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 231\u001b[39m\n\u001b[32m    228\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m\"\u001b[39m, storage=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msqlite:///\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOPTUNA_DB_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, load_if_exists=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    230\u001b[39m \u001b[38;5;66;03m# Start the Optuna study to optimize hyperparameters\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# Print the best hyperparameters found during the study\u001b[39;00m\n\u001b[32m    234\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest Hyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy.best_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/optuna/study/study.py:475\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/optuna/study/_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/optuna/study/_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/optuna/study/_optimize.py:248\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    244\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    247\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/optuna/study/_optimize.py:197\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    200\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 219\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;66;03m# Trainer setup for Optuna\u001b[39;00m\n\u001b[32m    210\u001b[39m trainer = pl.Trainer(\n\u001b[32m    211\u001b[39m     max_epochs=\u001b[32m200\u001b[39m,\n\u001b[32m    212\u001b[39m     callbacks=[early_stop_callback, model_checkpoint],\n\u001b[32m   (...)\u001b[39m\u001b[32m    216\u001b[39m     gradient_clip_val=\u001b[32m1.0\u001b[39m  \u001b[38;5;66;03m# Optional, set as needed\u001b[39;00m\n\u001b[32m    217\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trainer.callback_metrics[\u001b[33m\"\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m\"\u001b[39m].item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/2TB Storage Vault/rust-sec-fetcher/python/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:65\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[32m     64\u001b[39m         launcher.kill(_get_sigkill_signal())\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mexit\u001b[49m(\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n",
      "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import optuna\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Set the correct output path for saving the logs\n",
    "OUTPUT_PATH = \"data/fine_tuned_gaap_classifier\"  # Directory for saving outputs\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "OPTUNA_DB_PATH = os.path.join(OUTPUT_PATH, \"optuna_study.db\")\n",
    "\n",
    "# === Seeder for reproducibility ===\n",
    "def seed_everything(seed: int):\n",
    "    \"\"\"\n",
    "    Sets the seed for reproducibility across various libraries\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    pl.seed_everything(seed, workers=True)\n",
    "\n",
    "\n",
    "# === Setup for BGE model ===\n",
    "MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "encoder = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Ensure the model is in evaluation mode (no gradients needed)\n",
    "encoder.eval()\n",
    "\n",
    "# === Check if MPS is available (for Apple Silicon users) ===\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move the model to the selected device\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "\n",
    "# === Dataset Class for Data Loading (Using Precomputed Embeddings) ===\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        \"\"\"\n",
    "        Loads the dataset and the precomputed embeddings directly from the JSONL file.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_json(data_file, lines=True)\n",
    "\n",
    "        # Extract embeddings from the data as NumPy arrays and cast them to float32\n",
    "        self.input_embeddings = np.array(self.data[\"variation_embedding\"].tolist(), dtype=np.float32)\n",
    "        self.description_embeddings = np.array(self.data[\"description_embedding\"].tolist(), dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert the NumPy arrays to PyTorch tensors with requires_grad=True\n",
    "        input_embedding = torch.tensor(self.input_embeddings[idx], dtype=torch.float32, requires_grad=False)\n",
    "        description_embedding = torch.tensor(self.description_embeddings[idx], dtype=torch.float32, requires_grad=False)\n",
    "        \n",
    "        return input_embedding, description_embedding\n",
    "\n",
    "# === Model Definition with Attention and Fully Connected Layer ===\n",
    "class AlignmentModel(pl.LightningModule):\n",
    "    def __init__(self, dropout_rate=0.2, hidden_size=256, num_heads=8):\n",
    "        super(AlignmentModel, self).__init__()\n",
    "\n",
    "        # TODO: Don't hardcode\n",
    "        hidden_size = 1024\n",
    "        num_heads = 8\n",
    "\n",
    "        input_size = 1024\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),  # Ensure this matches your input size\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        # Attention layer\n",
    "        # self.attn = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads)\n",
    "        # self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc_out = nn.Linear(hidden_size, input_size)  # Adjust to match input size\n",
    "\n",
    "\n",
    "    def forward(self, input_embeddings, description_embeddings):\n",
    "        # Use the embeddings directly as input features (without detaching them)\n",
    "        \n",
    "        # Pass the embeddings through the fully connected layers\n",
    "        input_embeddings = self.fc(input_embeddings)\n",
    "        description_embeddings = self.fc(description_embeddings)\n",
    "\n",
    "        # Combine the embeddings for attention processing\n",
    "        combined_embeddings = torch.cat((input_embeddings, description_embeddings), dim=0)\n",
    "\n",
    "        # Apply attention mechanism\n",
    "        # attn_output, _ = self.attn(combined_embeddings.unsqueeze(0), combined_embeddings.unsqueeze(0), combined_embeddings.unsqueeze(0))\n",
    "        # attn_output = attn_output.squeeze(0) + combined_embeddings  # Add the input to the attention output\n",
    "\n",
    "        # Apply normalization and dropout\n",
    "        # attn_output = self.layer_norm(attn_output)\n",
    "        # attn_output = self.dropout(attn_output)\n",
    "        # or\n",
    "        attn_output = combined_embeddings\n",
    "\n",
    "        # Split back into the input and description embeddings\n",
    "        input_embeddings, description_embeddings = torch.split(attn_output, input_embeddings.size(0), dim=0)\n",
    "\n",
    "        # Final output layer\n",
    "        input_embeddings = self.fc_out(input_embeddings)\n",
    "        description_embeddings = self.fc_out(description_embeddings)\n",
    "\n",
    "        return input_embeddings, description_embeddings\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_embeddings, description_embeddings = batch\n",
    "\n",
    "        transformed_input_embeddings, transformed_description_embeddings = self(input_embeddings, description_embeddings)\n",
    "\n",
    "        # Compute training loss\n",
    "        loss = self.cosine_similarity_loss(transformed_input_embeddings, transformed_description_embeddings)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "\n",
    "        # Compute the cosine similarity loss on the original (non-transformed) embeddings\n",
    "        raw_loss = self.cosine_similarity_loss(input_embeddings, description_embeddings)\n",
    "        self.log(\"train_raw_loss\", raw_loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_embeddings, description_embeddings = batch\n",
    "        \n",
    "        transformed_input_embeddings, transformed_description_embeddings = self(input_embeddings, description_embeddings)\n",
    "\n",
    "        # Compute validation loss\n",
    "        loss = self.cosine_similarity_loss(transformed_input_embeddings, transformed_description_embeddings)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "\n",
    "        # Compute the cosine similarity loss on the original (non-transformed) embeddings\n",
    "        raw_loss = self.cosine_similarity_loss(input_embeddings, description_embeddings)\n",
    "        self.log(\"val_raw_loss\", raw_loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def cosine_similarity_loss(self, embeddings1, embeddings2):\n",
    "        # Compute cosine similarity using PyTorch's functional API\n",
    "        cosine_sim = F.cosine_similarity(embeddings1, embeddings2, dim=-1)\n",
    "        \n",
    "        # Convert cosine similarity to a loss value (1 - similarity)\n",
    "        loss = 1 - cosine_sim.mean()  # Return as a scalar loss\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "# === Objective Function for Optuna ===\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 8, 64, step=8)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5, step=0.1)\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 256, 1024, step=256)\n",
    "    num_heads = trial.suggest_int(\"num_heads\", 2, 8, step=2)\n",
    "\n",
    "    # Initialize model with hyperparameters\n",
    "    model = AlignmentModel(dropout_rate=dropout_rate, hidden_size=hidden_size, num_heads=num_heads)\n",
    "\n",
    "    # Load the original dataset (use TextDataset class to load it)\n",
    "    data_file = \"data/us_gaap_tags_with_variations_and_embeddings.jsonl\"\n",
    "\n",
    "    # Use the original method of loading the embeddings directly from the JSONL file\n",
    "    full_dataset = TextDataset(data_file)\n",
    "\n",
    "    # Split the dataset manually for training and validation\n",
    "    train_size = int(0.8 * len(full_dataset))  # 80% for training\n",
    "    val_size = len(full_dataset) - train_size  # 20% for validation\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Set up callbacks for early stopping and model checkpointing\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=3, verbose=True, mode=\"min\")\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        dirpath=OUTPUT_PATH,\n",
    "        filename=\"best_model\",\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=1,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Set up the logger\n",
    "    logger = TensorBoardLogger(OUTPUT_PATH, name=\"tb_logs\")\n",
    "\n",
    "    # Trainer setup for Optuna\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=200,\n",
    "        callbacks=[early_stop_callback, model_checkpoint],\n",
    "        logger=logger,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        gradient_clip_val=1.0  # Optional, set as needed\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "# === Run Optuna Optimization ===\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(42)  # Ensure reproducibility\n",
    "\n",
    "    # Create a study with SQLite monitoring\n",
    "    study = optuna.create_study(direction=\"minimize\", storage=f\"sqlite:///{OPTUNA_DB_PATH}\", load_if_exists=True)\n",
    "\n",
    "    # Start the Optuna study to optimize hyperparameters\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    # Print the best hyperparameters found during the study\n",
    "    print(f\"Best Hyperparameters: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Refactor as necessary\n",
    "\n",
    "# import torch\n",
    "# import pytorch_lightning as pl\n",
    "# from torch import nn\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import optuna\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import random\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from utils.pytorch import seed_everything, get_device\n",
    "\n",
    "# # === Setup for BGE model ===\n",
    "# MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# device = get_device()\n",
    "# encoder = AutoModel.from_pretrained(MODEL_NAME)\n",
    "# encoder = encoder.to(device)\n",
    "\n",
    "\n",
    "# # Cache embeddings for each text individually\n",
    "# embedding_cache = {}\n",
    "\n",
    "# def generate_embeddings(texts):\n",
    "#     embeddings = []\n",
    "#     for text in texts:\n",
    "#         if text in embedding_cache:  # Check if the embedding is cached\n",
    "#             embeddings.append(embedding_cache[text])\n",
    "#         else:\n",
    "#             inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = encoder(**inputs)\n",
    "#             text_embedding = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token embedding\n",
    "#             embedding_cache[text] = text_embedding  # Cache the embedding\n",
    "#             embeddings.append(text_embedding)\n",
    "#     return torch.stack(embeddings)\n",
    "\n",
    "# # Cosine Similarity Loss\n",
    "# def cosine_similarity_loss(embeddings1, embeddings2):\n",
    "#     cosine_sim = cosine_similarity(embeddings1.cpu().numpy(), embeddings2.cpu().numpy())\n",
    "#     return torch.tensor(1 - cosine_sim.mean(), device=device)\n",
    "\n",
    "# # === Model Definition with Attention and Fully Connected Layer ===\n",
    "# class AlignmentModel(pl.LightningModule):\n",
    "#     def __init__(self, dropout_rate=0.2, hidden_size=256, num_heads=2):\n",
    "#         super(AlignmentModel, self).__init__()\n",
    "\n",
    "#         # Fully connected layer to transform embeddings\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(encoder.config.hidden_size, hidden_size),\n",
    "#             nn.ReLU(),  # Non-linearity\n",
    "#             nn.Dropout(dropout_rate)\n",
    "#         )\n",
    "        \n",
    "#         # Attention layer that operates on combined embeddings\n",
    "#         self.attn = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads)\n",
    "        \n",
    "#         # Layer normalization after attention\n",
    "#         self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "#         # Output layer to restore embedding size (optional)\n",
    "#         self.fc_out = nn.Linear(hidden_size, encoder.config.hidden_size)\n",
    "        \n",
    "#         # Dropout after attention\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "#     def forward(self, input_texts, description_texts):\n",
    "#         # Generate embeddings for input and description texts\n",
    "#         input_embeddings = generate_embeddings(input_texts)\n",
    "#         description_embeddings = generate_embeddings(description_texts)\n",
    "        \n",
    "#         # Apply dropout to the embeddings\n",
    "#         input_embeddings = self.fc(input_embeddings)\n",
    "#         description_embeddings = self.fc(description_embeddings)\n",
    "        \n",
    "#         # Concatenate the embeddings to allow attention to focus on the relationships\n",
    "#         combined_embeddings = torch.cat((input_embeddings, description_embeddings), dim=0)\n",
    "        \n",
    "#         # Apply attention to focus on the relationships between the input and description embeddings\n",
    "#         attn_output, _ = self.attn(combined_embeddings, combined_embeddings, combined_embeddings)\n",
    "        \n",
    "#         # Residual connection after attention (adding the original embeddings back)\n",
    "#         attn_output = attn_output + combined_embeddings\n",
    "        \n",
    "#         # Apply layer normalization after attention\n",
    "#         attn_output = self.layer_norm(attn_output)\n",
    "        \n",
    "#         # Apply dropout to the attention output\n",
    "#         attn_output = self.dropout(attn_output)\n",
    "        \n",
    "#         # Split the output back into two sets: input and description embeddings\n",
    "#         input_embeddings, description_embeddings = torch.split(attn_output, input_embeddings.size(0), dim=0)\n",
    "        \n",
    "#         # Output layer to adjust embedding size\n",
    "#         input_embeddings = self.fc_out(input_embeddings)\n",
    "#         description_embeddings = self.fc_out(description_embeddings)\n",
    "\n",
    "#         return input_embeddings, description_embeddings\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         input_texts, description_texts = batch\n",
    "#         input_embeddings, description_embeddings = self(input_texts, description_texts)\n",
    "        \n",
    "#         # Calculate cosine similarity loss\n",
    "#         loss = cosine_similarity_loss(input_embeddings, description_embeddings)\n",
    "        \n",
    "#         self.log(\"train_loss\", loss, prog_bar=True)\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         input_texts, description_texts = batch\n",
    "#         input_embeddings, description_embeddings = self(input_texts, description_texts)\n",
    "        \n",
    "#         # Calculate cosine similarity loss for validation\n",
    "#         loss = cosine_similarity_loss(input_embeddings, description_embeddings)\n",
    "        \n",
    "#         self.log(\"val_loss\", loss, prog_bar=True)\n",
    "#         return loss\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         return torch.optim.AdamW(self.parameters(), lr=1e-5)\n",
    "\n",
    "# # === Data Loading ===\n",
    "# def load_data(file_path):\n",
    "#     df = pd.read_json(file_path, lines=True)\n",
    "#     return df\n",
    "\n",
    "# # === Objective Function for Optuna ===\n",
    "# def objective(trial):\n",
    "#     batch_size = trial.suggest_int(\"batch_size\", 8, 64, step=8)\n",
    "#     dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5, step=0.1)\n",
    "#     hidden_size = trial.suggest_int(\"hidden_size\", 256, 1024, step=256)\n",
    "#     num_heads = trial.suggest_int(\"num_heads\", 2, 8, step=2)\n",
    "\n",
    "#     # Initialize model with hyperparameters\n",
    "#     model = AlignmentModel(dropout_rate=dropout_rate, hidden_size=hidden_size, num_heads=num_heads)\n",
    "\n",
    "#     # Load datasets (Assume the datasets are already loaded into `train_data` and `val_data`)\n",
    "#     train_data = load_data(\"data/train.jsonl\")  # Replace with your train dataset path\n",
    "#     val_data = load_data(\"data/val.jsonl\")  # Replace with your val dataset path\n",
    "\n",
    "#     train_texts = list(train_data[\"input_text\"])\n",
    "#     train_descriptions = list(train_data[\"us_gaap_description\"])\n",
    "#     val_texts = list(val_data[\"input_text\"])\n",
    "#     val_descriptions = list(val_data[\"us_gaap_description\"])\n",
    "\n",
    "#     # Create DataLoader for training and validation\n",
    "#     train_dataset = TextDataset(train_texts, train_descriptions)\n",
    "#     val_dataset = TextDataset(val_texts, val_descriptions)\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     # Trainer setup for Optuna\n",
    "#     trainer = pl.Trainer(max_epochs=5, gpus=1, progress_bar_refresh_rate=20)\n",
    "#     trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "#     return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "# # === Run Optuna Optimization ===\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Start the Optuna study to optimize hyperparameters\n",
    "#     study = optuna.create_study(direction=\"minimize\")\n",
    "#     study.optimize(objective, n_trials=20)\n",
    "    \n",
    "#     print(f\"Best Hyperparameters: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# class FocalLoss(torch.nn.Module):\n",
    "#     def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "#         super(FocalLoss, self).__init__()\n",
    "#         self.alpha = alpha  # Balancing factor\n",
    "#         self.gamma = gamma  # Focusing parameter\n",
    "#         self.reduction = reduction\n",
    "\n",
    "#     def forward(self, inputs, targets):\n",
    "#         # Sigmoid activation for multi-label classification (logits to probabilities)\n",
    "#         inputs = torch.sigmoid(inputs)\n",
    "        \n",
    "#         # Ensure the targets are in the same shape as inputs\n",
    "#         targets = targets.float()\n",
    "\n",
    "#         # Cross entropy part\n",
    "#         cross_entropy = -targets * torch.log(inputs + 1e-8) - (1 - targets) * torch.log(1 - inputs + 1e-8)\n",
    "\n",
    "#         # Focal loss\n",
    "#         focal_loss = self.alpha * (1 - inputs) ** self.gamma * cross_entropy\n",
    "\n",
    "#         # Reduce (mean or sum)\n",
    "#         if self.reduction == 'mean':\n",
    "#             return torch.mean(focal_loss)\n",
    "#         elif self.reduction == 'sum':\n",
    "#             return torch.sum(focal_loss)\n",
    "#         else:\n",
    "#             return focal_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# class DiceLoss(torch.nn.Module):\n",
    "#     def __init__(self, smooth=1e-6):\n",
    "#         super(DiceLoss, self).__init__()\n",
    "#         self.smooth = smooth\n",
    "\n",
    "#     def forward(self, preds, target):\n",
    "#         # Flatten the input and target tensors\n",
    "#         preds = preds.view(-1)\n",
    "#         target = target.view(-1)\n",
    "\n",
    "#         intersection = (preds * target).sum()\n",
    "#         union = preds.sum() + target.sum()\n",
    "\n",
    "#         # Dice coefficient (with smoothing)\n",
    "#         dice_coeff = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "\n",
    "#         return 1 - dice_coeff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import os\n",
    "# import json\n",
    "# import torch\n",
    "# import optuna\n",
    "# import numpy as np\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import pytorch_lightning as pl\n",
    "# from pytorch_lightning.callbacks import EarlyStopping\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# # === SEED ===\n",
    "# SEED = 42\n",
    "# def seed_everything(seed: int):\n",
    "#     \"\"\"\n",
    "#     This function sets the seed for various libraries to ensure reproducibility.\n",
    "#     It seeds Python's built-in random module, NumPy, PyTorch (CPU and GPU), and PyTorch Lightning.\n",
    "#     \"\"\"\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.manual_seed_all(seed)\n",
    "#     pl.seed_everything(seed, workers=True)\n",
    "\n",
    "# # Ensure it's called!\n",
    "# seed_everything(SEED)\n",
    "\n",
    "\n",
    "# # === CONFIG ===\n",
    "# TRAIN_JSONL_PATH = \"data/train.jsonl\"  # Path to your training dataset\n",
    "# VAL_JSONL_PATH = \"data/val.jsonl\"  # Path to your validation dataset\n",
    "# MODEL_NAME = \"BAAI/bge-large-en-v1.5\"  # Base model\n",
    "# OUTPUT_PATH = \"data/fine_tuned_gaap_classifier\"  # Directory for saving outputs\n",
    "# os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "# OPTUNA_DB_PATH = os.path.join(OUTPUT_PATH, \"optuna_study.db\")\n",
    "# EPOCHS = 10\n",
    "# PATIENCE = 5\n",
    "\n",
    "# # Define the device (MPS or CPU)\n",
    "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# # === Load Data from JSONL files ===\n",
    "# def load_jsonl(filepath):\n",
    "#     with open(filepath, \"r\") as f:\n",
    "#         return [json.loads(line) for line in f]\n",
    "\n",
    "# train_data = load_jsonl(TRAIN_JSONL_PATH)\n",
    "# val_data = load_jsonl(VAL_JSONL_PATH)\n",
    "\n",
    "# # === Dynamically determine the number of possible categories ===\n",
    "# all_categories = set()\n",
    "# for entry in train_data + val_data:\n",
    "#     all_categories.update(entry[\"labels\"])\n",
    "\n",
    "# num_labels = max(all_categories)  # Dynamically find the highest category label number\n",
    "# print(f\"Number of categories: {num_labels}\")\n",
    "\n",
    "\n",
    "# # === Initialize Tokenizer and Encoder ===\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# encoder = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "\n",
    "# # === Prepare Label Embeddings ===\n",
    "# def cache_label_embeddings(label_descriptions, model_name=\"BAAI/bge-large-en-v1.5\", save_path=\"data/label_embeddings.pt\"):\n",
    "#     if os.path.exists(save_path):\n",
    "#         print(\"Loading cached label embeddings...\")\n",
    "#         return torch.load(save_path)\n",
    "    \n",
    "#     print(\"Generating new label embeddings...\")\n",
    "#     label_embeddings = {}\n",
    "#     for label_id, label_text in label_descriptions.items():\n",
    "#         label_embeddings[label_id] = generate_embeddings([label_text])[0]  # Generate embedding for each label\n",
    "    \n",
    "#     label_embeddings_tensor = torch.stack(list(label_embeddings.values()))\n",
    "#     torch.save(label_embeddings_tensor, save_path)\n",
    "#     print(f\"Label embeddings saved to {save_path}\")\n",
    "    \n",
    "#     return label_embeddings_tensor\n",
    "\n",
    "\n",
    "# def generate_embeddings(texts):\n",
    "#     \"\"\"\n",
    "#     This function generates embeddings for a given list of text descriptions.\n",
    "#     It uses the base model to generate the embeddings, specifically the [CLS] token representation.\n",
    "#     \"\"\"\n",
    "#     if isinstance(texts, str):\n",
    "#         texts = [texts]  # Convert single string to a list of strings\n",
    "#     elif not isinstance(texts, list):\n",
    "#         raise ValueError(\"Input must be a string or a list of strings.\")\n",
    "    \n",
    "#     texts = [str(text) if not isinstance(text, str) else text for text in texts]\n",
    "#     inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = encoder(**inputs)\n",
    "    \n",
    "#     embeddings = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token embedding\n",
    "#     return embeddings\n",
    "\n",
    "\n",
    "# # === Dataset Class ===\n",
    "# class MultiLabelDataset(Dataset):\n",
    "#     def __init__(self, data, tokenizer, encoder):\n",
    "#         self.samples = []\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.encoder = encoder\n",
    "#         for d in data:\n",
    "#             input_text = d[\"input_text\"]\n",
    "#             if not isinstance(input_text, str):\n",
    "#                 input_text = str(input_text)\n",
    "#             self.samples.append(input_text)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.samples)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         text = self.samples[idx]\n",
    "#         if not isinstance(text, str):\n",
    "#             text = str(text)\n",
    "        \n",
    "#         inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(self.encoder.device)\n",
    "#         with torch.no_grad():\n",
    "#             outputs = self.encoder(**inputs)\n",
    "#         embedding = outputs.last_hidden_state[:, 0, :]\n",
    "#         embedding.requires_grad_()  # Make sure gradients are tracked for embeddings\n",
    "#         return embedding\n",
    "\n",
    "\n",
    "# # === GAAP Classifier ===\n",
    "# class GAAPClassifier(pl.LightningModule):\n",
    "#     def __init__(self, model_name, dropout_rate, batch_size, lr, gradient_clip, weight_decay, label_embeddings, label_descriptions):\n",
    "#         super().__init__()\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         self.encoder = AutoModel.from_pretrained(model_name).to(device)\n",
    "#         self.label_embeddings = label_embeddings\n",
    "#         self.label_descriptions = label_descriptions\n",
    "#         self.batch_size = batch_size\n",
    "#         self.lr = lr\n",
    "#         self.gradient_clip = gradient_clip\n",
    "#         self.weight_decay = weight_decay\n",
    "#         self.save_hyperparameters()\n",
    "\n",
    "#         # Enable gradient tracking for model parameters\n",
    "#         for param in self.encoder.parameters():\n",
    "#             param.requires_grad = True  # Ensure all parameters in the encoder require gradients\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         outputs = self.encoder(**inputs)\n",
    "#         embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "#         embeddings.requires_grad_()  # Ensure gradients are tracked for embeddings\n",
    "#         return embeddings\n",
    "\n",
    "#     def compute_loss(self, outputs, labels):\n",
    "#         outputs = outputs.detach().cpu().numpy()\n",
    "#         labels = self.label_embeddings.detach().cpu().numpy()\n",
    "#         similarities = cosine_similarity(outputs, labels)\n",
    "#         loss = 1 - similarities.mean()\n",
    "#         return torch.tensor(loss, device=self.device)\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         texts = batch\n",
    "#         if isinstance(texts, torch.Tensor):\n",
    "#             texts = [str(t) for t in texts]\n",
    "#         inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(self.device)\n",
    "#         outputs = self(inputs)\n",
    "#         loss = self.compute_loss(outputs, self.label_embeddings)\n",
    "#         self.log(\"train/loss\", loss, prog_bar=True)\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         texts = batch\n",
    "#         if isinstance(texts, torch.Tensor):\n",
    "#             texts = [str(t) for t in texts]\n",
    "#         inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(self.device)\n",
    "#         outputs = self(inputs)\n",
    "#         loss = self.compute_loss(outputs, self.label_embeddings)\n",
    "#         self.log(\"val/loss\", loss, prog_bar=True)\n",
    "#         return loss\n",
    "    \n",
    "#     def configure_optimizers(self):\n",
    "#         return torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "\n",
    "# # === Cache Label Embeddings ===\n",
    "# label_descriptions = {label: f\"Description for label {label}\" for label in all_categories}\n",
    "# label_embeddings_tensor = cache_label_embeddings(label_descriptions)\n",
    "\n",
    "\n",
    "# # === Training Setup ===\n",
    "# def objective(trial):\n",
    "#     batch_size = trial.suggest_int(\"batch_size\", 8, 64, step=8)\n",
    "#     lr = trial.suggest_float(\"lr\", 1e-6, 1e-3, log=True)\n",
    "#     dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5, step=0.1)\n",
    "#     gradient_clip = trial.suggest_float(\"gradient_clip\", 0.0, 0.8, step=0.1)\n",
    "#     weight_decay = trial.suggest_float(\"weight_decay\", 1e-8, 1e-4, log=True)\n",
    "\n",
    "#     train_loader = DataLoader(MultiLabelDataset(train_data, tokenizer, encoder), batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(MultiLabelDataset(val_data, tokenizer, encoder), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     model = GAAPClassifier(MODEL_NAME, dropout_rate, batch_size, lr, gradient_clip, weight_decay, label_embeddings_tensor, label_descriptions)\n",
    "\n",
    "#     trainer = pl.Trainer(\n",
    "#         max_epochs=EPOCHS,\n",
    "#         callbacks=[EarlyStopping(monitor=\"val/loss\", patience=PATIENCE)],\n",
    "#         logger=TensorBoardLogger(OUTPUT_PATH),\n",
    "#         accelerator=\"auto\",\n",
    "#         devices=1,\n",
    "#         gradient_clip_val=gradient_clip\n",
    "#     )\n",
    "\n",
    "#     trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "#     return trainer.callback_metrics[\"val/loss\"].item()\n",
    "\n",
    "# # === Optuna Optimization ===\n",
    "# study = optuna.create_study(direction=\"minimize\", storage=f\"sqlite:///{OPTUNA_DB_PATH}\", load_if_exists=True)\n",
    "# study.optimize(objective, n_trials=50)\n",
    "\n",
    "# # Best Params\n",
    "# print(\"Best params:\", study.best_params)\n",
    "# best_trial = study.best_trial\n",
    "# print(f\"Best trial value: {best_trial.value}\")\n",
    "# for k, v in best_trial.params.items():\n",
    "#     print(f\"    {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
