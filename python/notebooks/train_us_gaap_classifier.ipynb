{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir('../')\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import optuna\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Set the correct output path for saving the logs\n",
    "OUTPUT_PATH = \"data/fine_tuned_gaap_classifier\"  # Directory for saving outputs\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "OPTUNA_DB_PATH = os.path.join(OUTPUT_PATH, \"optuna_study.db\")\n",
    "\n",
    "# === Seeder for reproducibility ===\n",
    "def seed_everything(seed: int):\n",
    "    \"\"\"\n",
    "    Sets the seed for reproducibility across various libraries\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    pl.seed_everything(seed, workers=True)\n",
    "\n",
    "\n",
    "# === Setup for BGE model ===\n",
    "MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "encoder = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Ensure the model is in evaluation mode (no gradients needed)\n",
    "encoder.eval()\n",
    "\n",
    "# === Check if MPS is available (for Apple Silicon users) ===\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move the model to the selected device\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "\n",
    "# === Dataset Class for Data Loading (Using Precomputed Embeddings) ===\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        \"\"\"\n",
    "        Loads the dataset and the precomputed embeddings directly from the JSONL file.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_json(data_file, lines=True)\n",
    "\n",
    "        # Extract embeddings from the data as NumPy arrays and cast them to float32\n",
    "        self.input_embeddings = np.array(self.data[\"variation_embedding\"].tolist(), dtype=np.float32)\n",
    "        self.description_embeddings = np.array(self.data[\"description_embedding\"].tolist(), dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert the NumPy arrays to PyTorch tensors with requires_grad=True\n",
    "        input_embedding = torch.tensor(self.input_embeddings[idx], dtype=torch.float32, requires_grad=False)\n",
    "        description_embedding = torch.tensor(self.description_embeddings[idx], dtype=torch.float32, requires_grad=False)\n",
    "        \n",
    "        return input_embedding, description_embedding\n",
    "\n",
    "# === Model Definition with Attention and Fully Connected Layer ===\n",
    "class AlignmentModel(pl.LightningModule):\n",
    "    def __init__(self, dropout_rate=0.2, hidden_size=256, num_heads=8):\n",
    "        super(AlignmentModel, self).__init__()\n",
    "\n",
    "        # Ensure that num_heads is a divisor of hidden_size\n",
    "        if hidden_size % num_heads != 0:\n",
    "            # Adjust num_heads to make sure it's a valid divisor of hidden_size\n",
    "            num_heads = hidden_size // (hidden_size // num_heads)\n",
    "\n",
    "        input_size = 1024\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),  # Ensure this matches your input size\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        # Attention layer\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_size, input_size)  # Adjust to match input size\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "\n",
    "    def forward(self, variation_embeddings):\n",
    "        # Use the embeddings directly as input features (without detaching them)\n",
    "        \n",
    "        # Pass the embeddings through the fully connected layers\n",
    "        variation_embeddings = self.fc(variation_embeddings)\n",
    "        # description_embeddings = self.fc(description_embeddings)\n",
    "\n",
    "        # Combine the embeddings for attention processing\n",
    "        # combined_embeddings = torch.cat((variation_embeddings, description_embeddings), dim=0)\n",
    "\n",
    "        # Apply attention mechanism\n",
    "        attn_output, _ = self.attn(variation_embeddings.unsqueeze(0), variation_embeddings.unsqueeze(0), variation_embeddings.unsqueeze(0))\n",
    "        attn_output = attn_output.squeeze(0) + variation_embeddings  # Add the input to the attention output\n",
    "\n",
    "        # Apply normalization and dropout\n",
    "        attn_output = self.layer_norm(attn_output)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "\n",
    "        # Split back into the input and description embeddings\n",
    "        # variation_embeddings, _ = torch.split(attn_output, variation_embeddings.size(0), dim=0)\n",
    "        variation_embeddings = attn_output\n",
    "\n",
    "        # Final output layer\n",
    "        transformed_variation_embeddings = self.fc_out(variation_embeddings)\n",
    "\n",
    "        return transformed_variation_embeddings\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Get the variation and description embeddings from the batch\n",
    "        variation_embeddings, description_embeddings = batch\n",
    "\n",
    "        # Pass the variation embedding through the model to transform it\n",
    "        transformed_variation_embeddings = self(variation_embeddings)\n",
    "\n",
    "        # Compute the loss using the cosine similarity between transformed variation and original description embeddings\n",
    "        loss = self.cosine_similarity_loss(transformed_variation_embeddings, description_embeddings)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Get the variation and description embeddings from the batch\n",
    "        variation_embeddings, description_embeddings = batch\n",
    "        \n",
    "        # Pass the variation embedding through the model to transform it\n",
    "        transformed_variation_embedding = self(variation_embeddings)\n",
    "\n",
    "        # Compute the validation loss using the cosine similarity between transformed variation and original description embeddings\n",
    "        loss = self.cosine_similarity_loss(transformed_variation_embedding, description_embeddings)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def cosine_similarity_loss(self, transformed_variation_embedding, description_embedding):\n",
    "        # Compute cosine similarity\n",
    "        cosine_sim = F.cosine_similarity(transformed_variation_embedding, description_embedding, dim=-1)\n",
    "        \n",
    "        # Convert cosine similarity to a loss value (1 - similarity)\n",
    "        loss = 1 - cosine_sim.mean()  # Lower is better, as we want the transformed variation to be closer to the description\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "# === Objective Function for Optuna ===\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 8, 64, step=8)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5, step=0.1)\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 256, 1024, step=256)\n",
    "    num_heads = trial.suggest_int(\"num_heads\", 2, 8, step=2)\n",
    "\n",
    "    # Initialize model with hyperparameters\n",
    "    model = AlignmentModel(dropout_rate=dropout_rate, hidden_size=hidden_size, num_heads=num_heads)\n",
    "\n",
    "    # Load the original dataset (use TextDataset class to load it)\n",
    "    data_file = \"data/us_gaap_tags_with_variations_and_embeddings.jsonl\"\n",
    "\n",
    "    # Use the original method of loading the embeddings directly from the JSONL file\n",
    "    full_dataset = TextDataset(data_file)\n",
    "\n",
    "    # Split the dataset manually for training and validation\n",
    "    train_size = int(0.8 * len(full_dataset))  # 80% for training\n",
    "    val_size = len(full_dataset) - train_size  # 20% for validation\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Set up callbacks for early stopping and model checkpointing\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=3, verbose=True, mode=\"min\")\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        dirpath=OUTPUT_PATH,\n",
    "        filename=\"best_model\",\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=1,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Set up the logger\n",
    "    logger = TensorBoardLogger(OUTPUT_PATH, name=\"tb_logs\")\n",
    "\n",
    "    # Trainer setup for Optuna\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=200,\n",
    "        callbacks=[early_stop_callback, model_checkpoint],\n",
    "        logger=logger,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        gradient_clip_val=1.0  # Optional, set as needed\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "# === Run Optuna Optimization ===\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(42)  # Ensure reproducibility\n",
    "\n",
    "    # Create a study with SQLite monitoring\n",
    "    study = optuna.create_study(direction=\"minimize\", storage=f\"sqlite:///{OPTUNA_DB_PATH}\", load_if_exists=True)\n",
    "\n",
    "    # Start the Optuna study to optimize hyperparameters\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    # Print the best hyperparameters found during the study\n",
    "    print(f\"Best Hyperparameters: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Refactor as necessary\n",
    "\n",
    "# import torch\n",
    "# import pytorch_lightning as pl\n",
    "# from torch import nn\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import optuna\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import random\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from utils.pytorch import seed_everything, get_device\n",
    "\n",
    "# # === Setup for BGE model ===\n",
    "# MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# device = get_device()\n",
    "# encoder = AutoModel.from_pretrained(MODEL_NAME)\n",
    "# encoder = encoder.to(device)\n",
    "\n",
    "\n",
    "# # Cache embeddings for each text individually\n",
    "# embedding_cache = {}\n",
    "\n",
    "# def generate_embeddings(texts):\n",
    "#     embeddings = []\n",
    "#     for text in texts:\n",
    "#         if text in embedding_cache:  # Check if the embedding is cached\n",
    "#             embeddings.append(embedding_cache[text])\n",
    "#         else:\n",
    "#             inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = encoder(**inputs)\n",
    "#             text_embedding = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token embedding\n",
    "#             embedding_cache[text] = text_embedding  # Cache the embedding\n",
    "#             embeddings.append(text_embedding)\n",
    "#     return torch.stack(embeddings)\n",
    "\n",
    "# # Cosine Similarity Loss\n",
    "# def cosine_similarity_loss(embeddings1, embeddings2):\n",
    "#     cosine_sim = cosine_similarity(embeddings1.cpu().numpy(), embeddings2.cpu().numpy())\n",
    "#     return torch.tensor(1 - cosine_sim.mean(), device=device)\n",
    "\n",
    "# # === Model Definition with Attention and Fully Connected Layer ===\n",
    "# class AlignmentModel(pl.LightningModule):\n",
    "#     def __init__(self, dropout_rate=0.2, hidden_size=256, num_heads=2):\n",
    "#         super(AlignmentModel, self).__init__()\n",
    "\n",
    "#         # Fully connected layer to transform embeddings\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(encoder.config.hidden_size, hidden_size),\n",
    "#             nn.ReLU(),  # Non-linearity\n",
    "#             nn.Dropout(dropout_rate)\n",
    "#         )\n",
    "        \n",
    "#         # Attention layer that operates on combined embeddings\n",
    "#         self.attn = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads)\n",
    "        \n",
    "#         # Layer normalization after attention\n",
    "#         self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "#         # Output layer to restore embedding size (optional)\n",
    "#         self.fc_out = nn.Linear(hidden_size, encoder.config.hidden_size)\n",
    "        \n",
    "#         # Dropout after attention\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "#     def forward(self, input_texts, description_texts):\n",
    "#         # Generate embeddings for input and description texts\n",
    "#         input_embeddings = generate_embeddings(input_texts)\n",
    "#         description_embeddings = generate_embeddings(description_texts)\n",
    "        \n",
    "#         # Apply dropout to the embeddings\n",
    "#         input_embeddings = self.fc(input_embeddings)\n",
    "#         description_embeddings = self.fc(description_embeddings)\n",
    "        \n",
    "#         # Concatenate the embeddings to allow attention to focus on the relationships\n",
    "#         combined_embeddings = torch.cat((input_embeddings, description_embeddings), dim=0)\n",
    "        \n",
    "#         # Apply attention to focus on the relationships between the input and description embeddings\n",
    "#         attn_output, _ = self.attn(combined_embeddings, combined_embeddings, combined_embeddings)\n",
    "        \n",
    "#         # Residual connection after attention (adding the original embeddings back)\n",
    "#         attn_output = attn_output + combined_embeddings\n",
    "        \n",
    "#         # Apply layer normalization after attention\n",
    "#         attn_output = self.layer_norm(attn_output)\n",
    "        \n",
    "#         # Apply dropout to the attention output\n",
    "#         attn_output = self.dropout(attn_output)\n",
    "        \n",
    "#         # Split the output back into two sets: input and description embeddings\n",
    "#         input_embeddings, description_embeddings = torch.split(attn_output, input_embeddings.size(0), dim=0)\n",
    "        \n",
    "#         # Output layer to adjust embedding size\n",
    "#         input_embeddings = self.fc_out(input_embeddings)\n",
    "#         description_embeddings = self.fc_out(description_embeddings)\n",
    "\n",
    "#         return input_embeddings, description_embeddings\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         input_texts, description_texts = batch\n",
    "#         input_embeddings, description_embeddings = self(input_texts, description_texts)\n",
    "        \n",
    "#         # Calculate cosine similarity loss\n",
    "#         loss = cosine_similarity_loss(input_embeddings, description_embeddings)\n",
    "        \n",
    "#         self.log(\"train_loss\", loss, prog_bar=True)\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         input_texts, description_texts = batch\n",
    "#         input_embeddings, description_embeddings = self(input_texts, description_texts)\n",
    "        \n",
    "#         # Calculate cosine similarity loss for validation\n",
    "#         loss = cosine_similarity_loss(input_embeddings, description_embeddings)\n",
    "        \n",
    "#         self.log(\"val_loss\", loss, prog_bar=True)\n",
    "#         return loss\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         return torch.optim.AdamW(self.parameters(), lr=1e-5)\n",
    "\n",
    "# # === Data Loading ===\n",
    "# def load_data(file_path):\n",
    "#     df = pd.read_json(file_path, lines=True)\n",
    "#     return df\n",
    "\n",
    "# # === Objective Function for Optuna ===\n",
    "# def objective(trial):\n",
    "#     batch_size = trial.suggest_int(\"batch_size\", 8, 64, step=8)\n",
    "#     dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5, step=0.1)\n",
    "#     hidden_size = trial.suggest_int(\"hidden_size\", 256, 1024, step=256)\n",
    "#     num_heads = trial.suggest_int(\"num_heads\", 2, 8, step=2)\n",
    "\n",
    "#     # Initialize model with hyperparameters\n",
    "#     model = AlignmentModel(dropout_rate=dropout_rate, hidden_size=hidden_size, num_heads=num_heads)\n",
    "\n",
    "#     # Load datasets (Assume the datasets are already loaded into `train_data` and `val_data`)\n",
    "#     train_data = load_data(\"data/train.jsonl\")  # Replace with your train dataset path\n",
    "#     val_data = load_data(\"data/val.jsonl\")  # Replace with your val dataset path\n",
    "\n",
    "#     train_texts = list(train_data[\"input_text\"])\n",
    "#     train_descriptions = list(train_data[\"us_gaap_description\"])\n",
    "#     val_texts = list(val_data[\"input_text\"])\n",
    "#     val_descriptions = list(val_data[\"us_gaap_description\"])\n",
    "\n",
    "#     # Create DataLoader for training and validation\n",
    "#     train_dataset = TextDataset(train_texts, train_descriptions)\n",
    "#     val_dataset = TextDataset(val_texts, val_descriptions)\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     # Trainer setup for Optuna\n",
    "#     trainer = pl.Trainer(max_epochs=5, gpus=1, progress_bar_refresh_rate=20)\n",
    "#     trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "#     return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "# # === Run Optuna Optimization ===\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Start the Optuna study to optimize hyperparameters\n",
    "#     study = optuna.create_study(direction=\"minimize\")\n",
    "#     study.optimize(objective, n_trials=20)\n",
    "    \n",
    "#     print(f\"Best Hyperparameters: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# class FocalLoss(torch.nn.Module):\n",
    "#     def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "#         super(FocalLoss, self).__init__()\n",
    "#         self.alpha = alpha  # Balancing factor\n",
    "#         self.gamma = gamma  # Focusing parameter\n",
    "#         self.reduction = reduction\n",
    "\n",
    "#     def forward(self, inputs, targets):\n",
    "#         # Sigmoid activation for multi-label classification (logits to probabilities)\n",
    "#         inputs = torch.sigmoid(inputs)\n",
    "        \n",
    "#         # Ensure the targets are in the same shape as inputs\n",
    "#         targets = targets.float()\n",
    "\n",
    "#         # Cross entropy part\n",
    "#         cross_entropy = -targets * torch.log(inputs + 1e-8) - (1 - targets) * torch.log(1 - inputs + 1e-8)\n",
    "\n",
    "#         # Focal loss\n",
    "#         focal_loss = self.alpha * (1 - inputs) ** self.gamma * cross_entropy\n",
    "\n",
    "#         # Reduce (mean or sum)\n",
    "#         if self.reduction == 'mean':\n",
    "#             return torch.mean(focal_loss)\n",
    "#         elif self.reduction == 'sum':\n",
    "#             return torch.sum(focal_loss)\n",
    "#         else:\n",
    "#             return focal_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# class DiceLoss(torch.nn.Module):\n",
    "#     def __init__(self, smooth=1e-6):\n",
    "#         super(DiceLoss, self).__init__()\n",
    "#         self.smooth = smooth\n",
    "\n",
    "#     def forward(self, preds, target):\n",
    "#         # Flatten the input and target tensors\n",
    "#         preds = preds.view(-1)\n",
    "#         target = target.view(-1)\n",
    "\n",
    "#         intersection = (preds * target).sum()\n",
    "#         union = preds.sum() + target.sum()\n",
    "\n",
    "#         # Dice coefficient (with smoothing)\n",
    "#         dice_coeff = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "\n",
    "#         return 1 - dice_coeff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import os\n",
    "# import json\n",
    "# import torch\n",
    "# import optuna\n",
    "# import numpy as np\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import pytorch_lightning as pl\n",
    "# from pytorch_lightning.callbacks import EarlyStopping\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# # === SEED ===\n",
    "# SEED = 42\n",
    "# def seed_everything(seed: int):\n",
    "#     \"\"\"\n",
    "#     This function sets the seed for various libraries to ensure reproducibility.\n",
    "#     It seeds Python's built-in random module, NumPy, PyTorch (CPU and GPU), and PyTorch Lightning.\n",
    "#     \"\"\"\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.manual_seed_all(seed)\n",
    "#     pl.seed_everything(seed, workers=True)\n",
    "\n",
    "# # Ensure it's called!\n",
    "# seed_everything(SEED)\n",
    "\n",
    "\n",
    "# # === CONFIG ===\n",
    "# TRAIN_JSONL_PATH = \"data/train.jsonl\"  # Path to your training dataset\n",
    "# VAL_JSONL_PATH = \"data/val.jsonl\"  # Path to your validation dataset\n",
    "# MODEL_NAME = \"BAAI/bge-large-en-v1.5\"  # Base model\n",
    "# OUTPUT_PATH = \"data/fine_tuned_gaap_classifier\"  # Directory for saving outputs\n",
    "# os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "# OPTUNA_DB_PATH = os.path.join(OUTPUT_PATH, \"optuna_study.db\")\n",
    "# EPOCHS = 10\n",
    "# PATIENCE = 5\n",
    "\n",
    "# # Define the device (MPS or CPU)\n",
    "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# # === Load Data from JSONL files ===\n",
    "# def load_jsonl(filepath):\n",
    "#     with open(filepath, \"r\") as f:\n",
    "#         return [json.loads(line) for line in f]\n",
    "\n",
    "# train_data = load_jsonl(TRAIN_JSONL_PATH)\n",
    "# val_data = load_jsonl(VAL_JSONL_PATH)\n",
    "\n",
    "# # === Dynamically determine the number of possible categories ===\n",
    "# all_categories = set()\n",
    "# for entry in train_data + val_data:\n",
    "#     all_categories.update(entry[\"labels\"])\n",
    "\n",
    "# num_labels = max(all_categories)  # Dynamically find the highest category label number\n",
    "# print(f\"Number of categories: {num_labels}\")\n",
    "\n",
    "\n",
    "# # === Initialize Tokenizer and Encoder ===\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# encoder = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "\n",
    "# # === Prepare Label Embeddings ===\n",
    "# def cache_label_embeddings(label_descriptions, model_name=\"BAAI/bge-large-en-v1.5\", save_path=\"data/label_embeddings.pt\"):\n",
    "#     if os.path.exists(save_path):\n",
    "#         print(\"Loading cached label embeddings...\")\n",
    "#         return torch.load(save_path)\n",
    "    \n",
    "#     print(\"Generating new label embeddings...\")\n",
    "#     label_embeddings = {}\n",
    "#     for label_id, label_text in label_descriptions.items():\n",
    "#         label_embeddings[label_id] = generate_embeddings([label_text])[0]  # Generate embedding for each label\n",
    "    \n",
    "#     label_embeddings_tensor = torch.stack(list(label_embeddings.values()))\n",
    "#     torch.save(label_embeddings_tensor, save_path)\n",
    "#     print(f\"Label embeddings saved to {save_path}\")\n",
    "    \n",
    "#     return label_embeddings_tensor\n",
    "\n",
    "\n",
    "# def generate_embeddings(texts):\n",
    "#     \"\"\"\n",
    "#     This function generates embeddings for a given list of text descriptions.\n",
    "#     It uses the base model to generate the embeddings, specifically the [CLS] token representation.\n",
    "#     \"\"\"\n",
    "#     if isinstance(texts, str):\n",
    "#         texts = [texts]  # Convert single string to a list of strings\n",
    "#     elif not isinstance(texts, list):\n",
    "#         raise ValueError(\"Input must be a string or a list of strings.\")\n",
    "    \n",
    "#     texts = [str(text) if not isinstance(text, str) else text for text in texts]\n",
    "#     inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = encoder(**inputs)\n",
    "    \n",
    "#     embeddings = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token embedding\n",
    "#     return embeddings\n",
    "\n",
    "\n",
    "# # === Dataset Class ===\n",
    "# class MultiLabelDataset(Dataset):\n",
    "#     def __init__(self, data, tokenizer, encoder):\n",
    "#         self.samples = []\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.encoder = encoder\n",
    "#         for d in data:\n",
    "#             input_text = d[\"input_text\"]\n",
    "#             if not isinstance(input_text, str):\n",
    "#                 input_text = str(input_text)\n",
    "#             self.samples.append(input_text)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.samples)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         text = self.samples[idx]\n",
    "#         if not isinstance(text, str):\n",
    "#             text = str(text)\n",
    "        \n",
    "#         inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(self.encoder.device)\n",
    "#         with torch.no_grad():\n",
    "#             outputs = self.encoder(**inputs)\n",
    "#         embedding = outputs.last_hidden_state[:, 0, :]\n",
    "#         embedding.requires_grad_()  # Make sure gradients are tracked for embeddings\n",
    "#         return embedding\n",
    "\n",
    "\n",
    "# # === GAAP Classifier ===\n",
    "# class GAAPClassifier(pl.LightningModule):\n",
    "#     def __init__(self, model_name, dropout_rate, batch_size, lr, gradient_clip, weight_decay, label_embeddings, label_descriptions):\n",
    "#         super().__init__()\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         self.encoder = AutoModel.from_pretrained(model_name).to(device)\n",
    "#         self.label_embeddings = label_embeddings\n",
    "#         self.label_descriptions = label_descriptions\n",
    "#         self.batch_size = batch_size\n",
    "#         self.lr = lr\n",
    "#         self.gradient_clip = gradient_clip\n",
    "#         self.weight_decay = weight_decay\n",
    "#         self.save_hyperparameters()\n",
    "\n",
    "#         # Enable gradient tracking for model parameters\n",
    "#         for param in self.encoder.parameters():\n",
    "#             param.requires_grad = True  # Ensure all parameters in the encoder require gradients\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         outputs = self.encoder(**inputs)\n",
    "#         embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "#         embeddings.requires_grad_()  # Ensure gradients are tracked for embeddings\n",
    "#         return embeddings\n",
    "\n",
    "#     def compute_loss(self, outputs, labels):\n",
    "#         outputs = outputs.detach().cpu().numpy()\n",
    "#         labels = self.label_embeddings.detach().cpu().numpy()\n",
    "#         similarities = cosine_similarity(outputs, labels)\n",
    "#         loss = 1 - similarities.mean()\n",
    "#         return torch.tensor(loss, device=self.device)\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         texts = batch\n",
    "#         if isinstance(texts, torch.Tensor):\n",
    "#             texts = [str(t) for t in texts]\n",
    "#         inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(self.device)\n",
    "#         outputs = self(inputs)\n",
    "#         loss = self.compute_loss(outputs, self.label_embeddings)\n",
    "#         self.log(\"train/loss\", loss, prog_bar=True)\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         texts = batch\n",
    "#         if isinstance(texts, torch.Tensor):\n",
    "#             texts = [str(t) for t in texts]\n",
    "#         inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(self.device)\n",
    "#         outputs = self(inputs)\n",
    "#         loss = self.compute_loss(outputs, self.label_embeddings)\n",
    "#         self.log(\"val/loss\", loss, prog_bar=True)\n",
    "#         return loss\n",
    "    \n",
    "#     def configure_optimizers(self):\n",
    "#         return torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "\n",
    "# # === Cache Label Embeddings ===\n",
    "# label_descriptions = {label: f\"Description for label {label}\" for label in all_categories}\n",
    "# label_embeddings_tensor = cache_label_embeddings(label_descriptions)\n",
    "\n",
    "\n",
    "# # === Training Setup ===\n",
    "# def objective(trial):\n",
    "#     batch_size = trial.suggest_int(\"batch_size\", 8, 64, step=8)\n",
    "#     lr = trial.suggest_float(\"lr\", 1e-6, 1e-3, log=True)\n",
    "#     dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5, step=0.1)\n",
    "#     gradient_clip = trial.suggest_float(\"gradient_clip\", 0.0, 0.8, step=0.1)\n",
    "#     weight_decay = trial.suggest_float(\"weight_decay\", 1e-8, 1e-4, log=True)\n",
    "\n",
    "#     train_loader = DataLoader(MultiLabelDataset(train_data, tokenizer, encoder), batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(MultiLabelDataset(val_data, tokenizer, encoder), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     model = GAAPClassifier(MODEL_NAME, dropout_rate, batch_size, lr, gradient_clip, weight_decay, label_embeddings_tensor, label_descriptions)\n",
    "\n",
    "#     trainer = pl.Trainer(\n",
    "#         max_epochs=EPOCHS,\n",
    "#         callbacks=[EarlyStopping(monitor=\"val/loss\", patience=PATIENCE)],\n",
    "#         logger=TensorBoardLogger(OUTPUT_PATH),\n",
    "#         accelerator=\"auto\",\n",
    "#         devices=1,\n",
    "#         gradient_clip_val=gradient_clip\n",
    "#     )\n",
    "\n",
    "#     trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "#     return trainer.callback_metrics[\"val/loss\"].item()\n",
    "\n",
    "# # === Optuna Optimization ===\n",
    "# study = optuna.create_study(direction=\"minimize\", storage=f\"sqlite:///{OPTUNA_DB_PATH}\", load_if_exists=True)\n",
    "# study.optimize(objective, n_trials=50)\n",
    "\n",
    "# # Best Params\n",
    "# print(\"Best params:\", study.best_params)\n",
    "# best_trial = study.best_trial\n",
    "# print(f\"Best trial value: {best_trial.value}\")\n",
    "# for k, v in best_trial.params.items():\n",
    "#     print(f\"    {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
