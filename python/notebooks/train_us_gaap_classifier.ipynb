{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import optuna\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchmetrics import Precision, Recall\n",
    "\n",
    "\n",
    "# === SEED ===\n",
    "SEED = 42\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # In PyTorch, torch.manual_seed() applies to the CPU by default, so no extra handling is needed for CPU.\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # If using MPS (Apple Silicon), set the seed for MPS device\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    # If using CUDA (GPU), set the seed for CUDA device\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    pl.seed_everything(seed, workers=True)\n",
    "\n",
    "# Ensure it's called!\n",
    "seed_everything(SEED)\n",
    "\n",
    "# === CONFIG ===\n",
    "TRAIN_JSONL_PATH = \"data/train.jsonl\"  # Correct path for your dataset\n",
    "VAL_JSONL_PATH = \"data/val.jsonl\"  # Correct path for your dataset\n",
    "MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "OUTPUT_PATH = \"data/fine_tuned_gaap_classifier\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "OPTUNA_DB_PATH = os.path.join(OUTPUT_PATH, \"optuna_study.db\")\n",
    "EPOCHS = 20\n",
    "PATIENCE = 5\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# === Load Data from JSONL files ===\n",
    "def load_jsonl(filepath):\n",
    "    with open(filepath, \"r\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "train_data = load_jsonl(TRAIN_JSONL_PATH)\n",
    "val_data = load_jsonl(VAL_JSONL_PATH)\n",
    "\n",
    "# === Dynamically determine the number of possible categories ===\n",
    "all_categories = set()\n",
    "for entry in train_data + val_data:\n",
    "    all_categories.update(entry[\"labels\"])\n",
    "\n",
    "num_labels = max(all_categories)  # Dynamically find the highest category label number\n",
    "print(f\"Number of categories: {num_labels}\")\n",
    "\n",
    "# Dynamically calculate num_balance_types and num_period_types\n",
    "num_balance_types = len(set([d['balance_type_id'] for d in train_data + val_data]))  # Unique balance types\n",
    "num_period_types = len(set([d['period_type_id'] for d in train_data + val_data]))  # Unique period types\n",
    "\n",
    "print(f\"number of balance types: {num_balance_types}\")\n",
    "print(f\"number of period types: {num_period_types}\")\n",
    "\n",
    "\n",
    "# === Dataset Class ===\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.samples = []\n",
    "        for d in data:\n",
    "            input_text = d[\"input_text\"]\n",
    "            labels = d[\"labels\"]\n",
    "            balance_type_id = d[\"balance_type_id\"]\n",
    "            period_type_id = d[\"period_type_id\"]\n",
    "            \n",
    "            # Ensure labels are a list of integers, default to an empty list if not available\n",
    "            labels = [int(label) for label in labels] if labels else []\n",
    "            \n",
    "            self.samples.append((input_text, labels, balance_type_id, period_type_id))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text, labels, balance_type_id, period_type_id = self.samples[idx]\n",
    "        \n",
    "        # Convert labels to tensor\n",
    "        labels_tensor = torch.zeros(num_labels)  # Dynamic number of categories\n",
    "        for label in labels:\n",
    "            labels_tensor[label - 1] = 1  # Set category positions to 1 (0-indexed for PyTorch)\n",
    "        \n",
    "        # Return text, labels, balance_type_id, and period_type_id\n",
    "        return text, labels_tensor, balance_type_id, period_type_id\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels, balance_type_ids, period_type_ids = zip(*batch)\n",
    "    # Stack the labels as before\n",
    "    labels_tensor = torch.stack(labels)\n",
    "    # Stack the balance_type_ids and period_type_ids\n",
    "    balance_type_ids_tensor = torch.tensor(balance_type_ids, dtype=torch.long)\n",
    "    period_type_ids_tensor = torch.tensor(period_type_ids, dtype=torch.long)\n",
    "    return list(texts), labels_tensor, balance_type_ids_tensor, period_type_ids_tensor\n",
    "\n",
    "\n",
    "# === Model Definition ===\n",
    "class GAAPClassifier(pl.LightningModule):\n",
    "    def __init__(self, model_name, dropout_rate, num_labels, batch_size, lr, num_balance_types, num_period_types):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.encoder = SentenceTransformer(model_name, device=device)\n",
    "        self.dim = self.encoder.get_sentence_embedding_dimension()\n",
    "\n",
    "        # Embedding layers for balance_type_id and period_type_id\n",
    "        # self.balance_type_embedding = torch.nn.Embedding(num_balance_types, 8)  # Example embedding size\n",
    "        # self.period_type_embedding = torch.nn.Embedding(num_period_types, 8)  # Example embedding size\n",
    "\n",
    "        # Attention mechanism without changing dimensionality\n",
    "        # self.attn = torch.nn.MultiheadAttention(embed_dim=self.dim, num_heads=1)\n",
    "        # self.norm = torch.nn.LayerNorm(self.dim)\n",
    "\n",
    "        self.num_labels = num_labels  # Ensure num_labels is passed\n",
    "\n",
    "        # Adjust output layer for multi-label classification\n",
    "        self.head = torch.nn.Sequential(\n",
    "            # torch.nn.Linear(self.dim + 8 + 8, 128),  # 8 for each embedding size of balance_type_id and period_type_id\n",
    "            torch.nn.Linear(self.dim, 128),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(dropout_rate),\n",
    "            torch.nn.Linear(128, self.num_labels)  # Use num_labels for output layer\n",
    "        )\n",
    "        \n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Metrics\n",
    "        self.precision = Precision(num_labels=self.num_labels, average=\"macro\", task=\"multilabel\")\n",
    "        self.recall = Recall(num_labels=self.num_labels, average=\"macro\", task=\"multilabel\")\n",
    "\n",
    "        # Apply weight initialization\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, texts, balance_type_ids, period_type_ids):\n",
    "        # Ensure embeddings are correctly encoded (check the shape here)\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.encoder.encode(texts, convert_to_tensor=True, device=device)\n",
    "\n",
    "        # Reshape embeddings to ensure proper dimensionality for attention\n",
    "        # embeddings = embeddings.unsqueeze(0)  # Add batch dimension if necessary\n",
    "        # attended, _ = self.attn(embeddings, embeddings, embeddings)  # Apply attention\n",
    "        # or\n",
    "        attended = embeddings\n",
    "        \n",
    "        # After attention, reshape back to [batch_size, 1024]\n",
    "        # attended = attended.squeeze(0)  # Remove the extra batch dimension added by attention\n",
    "        \n",
    "        # attended = self.norm(attended)  # Apply normalization\n",
    "\n",
    "        # Embedding lookups for balance_type_id and period_type_id\n",
    "        # balance_embedding = self.balance_type_embedding(balance_type_ids)\n",
    "        # period_embedding = self.period_type_embedding(period_type_ids)\n",
    "\n",
    "        # Concatenate the embeddings\n",
    "        # concatenated = torch.cat((attended, balance_embedding, period_embedding), dim=-1)\n",
    "        # or\n",
    "        concatenated = attended\n",
    "\n",
    "        # Final output layer\n",
    "        return self.head(concatenated)  # Final output layer\n",
    "\n",
    "    def compute_loss(self, outputs, labels):\n",
    "        return self.loss_fn(outputs, labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        texts, labels, balance_type_id, period_type_id = batch  # Unpack batch\n",
    "        outputs = self(texts, balance_type_id, period_type_id)  # Get model outputs\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Train logits range: min={outputs.min()}, max={outputs.max()}\")\n",
    "\n",
    "        # Compute loss using raw logits (no need for sigmoid here)\n",
    "        loss = self.compute_loss(outputs, labels)  # BCEWithLogitsLoss already applies sigmoid\n",
    "        \n",
    "        self.log(\"train/loss\", loss, prog_bar=True)\n",
    "\n",
    "        # Log precision and recall for binary predictions (outputs are logits)\n",
    "        pred = outputs > 0.5  # Directly threshold the logits to binary values (no sigmoid)\n",
    "        self.log(\"train/precision\", self.precision(pred, labels))\n",
    "        self.log(\"train/recall\", self.recall(pred, labels))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        texts, labels, balance_type_id, period_type_id = batch  # Unpack batch\n",
    "        outputs = self(texts, balance_type_id, period_type_id)  # Get model outputs\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Val logits range: min={outputs.min()}, max={outputs.max()}\")\n",
    "\n",
    "        # Compute loss using raw logits (no need for sigmoid here)\n",
    "        loss = self.compute_loss(outputs, labels)  # BCEWithLogitsLoss already applies sigmoid\n",
    "        \n",
    "        self.log(\"val/loss\", loss, prog_bar=True)\n",
    "\n",
    "        # Log precision and recall for validation using raw logits\n",
    "        pred = outputs > 0.5  # Directly threshold the logits to binary values (no sigmoid)\n",
    "        self.log(\"val/precision\", self.precision(pred, labels))\n",
    "        self.log(\"val/recall\", self.recall(pred, labels))\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=1e-5)\n",
    "\n",
    "# === Objective Function for Optuna ===\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 8, 64, step=8)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-6, 1e-3, log=True)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0, 0.5, step=0.1)\n",
    "\n",
    "    # Load dataset\n",
    "    train_loader = DataLoader(MultiLabelDataset(train_data),\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(MultiLabelDataset(val_data),\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            collate_fn=collate_fn)\n",
    "\n",
    "    # Pass num_balance_types and num_period_types to GAAPClassifier\n",
    "    model = GAAPClassifier(MODEL_NAME, dropout_rate, num_labels, batch_size, lr, num_balance_types, num_period_types)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=EPOCHS,\n",
    "        callbacks=[EarlyStopping(monitor=\"val/loss\", patience=PATIENCE)],\n",
    "        logger=TensorBoardLogger(OUTPUT_PATH),\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        gradient_clip_val=0.75\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    return trainer.callback_metrics[\"val/loss\"].item()\n",
    "\n",
    "# === Optuna Optimization ===\n",
    "study = optuna.create_study(direction=\"minimize\", storage=f\"sqlite:///{OPTUNA_DB_PATH}\", load_if_exists=True)\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Best Params\n",
    "print(\"Best params:\", study.best_params)\n",
    "best_trial = study.best_trial\n",
    "print(f\"Best trial value: {best_trial.value}\")\n",
    "for k, v in best_trial.params.items():\n",
    "    print(f\"    {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
