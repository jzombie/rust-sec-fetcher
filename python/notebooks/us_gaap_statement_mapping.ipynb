{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "\n",
    "# https://www.fasb.org/page/detail?pageId=/projects/FASB-Taxonomies/2025-gaap-financial-reporting-taxonomy.html\n",
    "# 2025 Taxonomy downloads: https://xbrl.fasb.org/us-gaap/2025/us-gaap-2025.zip\n",
    "\n",
    "# === CONFIG ===\n",
    "TAXONOMY_DIR = \"data/us-gaap-2025/\"\n",
    "ELTS_XSD = os.path.join(TAXONOMY_DIR, \"elts\", \"us-gaap-2025.xsd\")\n",
    "STM_DIR = os.path.join(TAXONOMY_DIR, \"stm\")\n",
    "\n",
    "OUTPUT_PATH = \"data/WITH_TAXONOMY_HIERARCHY_us_gaap_2025_with_all_statements_and_hierarchy.csv\"\n",
    "\n",
    "FILENAME_STATEMENT_MAP = {\n",
    "    \"scf\": \"Cash Flow Statement\",\n",
    "    \"soi\": \"Income Statement\",\n",
    "    \"sfp\": \"Balance Sheet\",\n",
    "    \"sheci\": \"Equity Statement\",\n",
    "    \"soc\": \"Comprehensive Income\"\n",
    "}\n",
    "\n",
    "XBRLI_NS = \"http://www.xbrl.org/2003/instance\"\n",
    "BALANCE_KEY = f\"{{{XBRLI_NS}}}balance\"\n",
    "PERIOD_TYPE_KEY = f\"{{{XBRLI_NS}}}periodType\"\n",
    "\n",
    "\n",
    "def generate_description(tag_name):\n",
    "    return re.sub(\n",
    "        r'(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])',\n",
    "        ' ',\n",
    "        tag_name\n",
    "    ).lower()\n",
    "\n",
    "\n",
    "# === STEP 1: TAG METADATA ===\n",
    "tag_metadata = {}\n",
    "tree = ET.parse(ELTS_XSD)\n",
    "root = tree.getroot()\n",
    "for el in root.findall(\".//{http://www.w3.org/2001/XMLSchema}element\"):\n",
    "    name = el.attrib.get(\"name\")\n",
    "    if not name or el.attrib.get(\"abstract\") == \"true\":\n",
    "        continue\n",
    "    tag_metadata[name] = {\n",
    "        \"balance\": el.attrib.get(BALANCE_KEY, \"\"),\n",
    "        \"period_type\": el.attrib.get(PERIOD_TYPE_KEY, \"\"),\n",
    "        \"statements\": set()\n",
    "    }\n",
    "\n",
    "# === STEP 2: STATEMENT TYPE MAPPING ===\n",
    "for file in os.listdir(STM_DIR):\n",
    "    if not file.endswith(\".xml\") or \"-pre-\" not in file:\n",
    "        continue\n",
    "    parts = file.split(\"-\")\n",
    "    if len(parts) < 4:\n",
    "        continue\n",
    "    stmt_key = parts[3]\n",
    "    inferred_statement = FILENAME_STATEMENT_MAP.get(stmt_key)\n",
    "    if not inferred_statement:\n",
    "        continue\n",
    "\n",
    "    tree = ET.parse(os.path.join(STM_DIR, file))\n",
    "    root = tree.getroot()\n",
    "    for loc in root.findall(\".//{http://www.xbrl.org/2003/linkbase}loc\"):\n",
    "        href = loc.attrib.get(\"{http://www.w3.org/1999/xlink}href\", \"\")\n",
    "        tag = href.split(\"#\")[-1]\n",
    "        if tag.startswith(\"us-gaap_\"):\n",
    "            tag = tag.replace(\"us-gaap_\", \"\")\n",
    "        if tag in tag_metadata:\n",
    "            tag_metadata[tag][\"statements\"].add(inferred_statement)\n",
    "\n",
    "# === STEP 3: BUILD BASE OUTPUT ===\n",
    "final_output = []\n",
    "for tag, meta in tag_metadata.items():\n",
    "    for stmt in meta[\"statements\"]:\n",
    "        final_output.append({\n",
    "            \"tag\": tag,\n",
    "            \"statement_type\": stmt,\n",
    "            \"balance\": meta[\"balance\"],\n",
    "            \"period_type\": meta[\"period_type\"],\n",
    "            \"description\": generate_description(tag),\n",
    "            \"subcategory_path\": \"\"\n",
    "        })\n",
    "\n",
    "# === STEP 4: EXTRACT TAXONOMY HIERARCHY PATHS ===\n",
    "def extract_tag_hierarchy_paths(pre_files):\n",
    "    tag_paths = defaultdict(set)\n",
    "\n",
    "    for path in pre_files:\n",
    "        try:\n",
    "            tree = ET.parse(path)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            loc_map = {}\n",
    "            arcs_by_from = defaultdict(list)\n",
    "\n",
    "            for loc in root.findall(\".//{http://www.xbrl.org/2003/linkbase}loc\"):\n",
    "                loc_id = loc.attrib.get(\"{http://www.w3.org/1999/xlink}label\")\n",
    "                href = loc.attrib.get(\"{http://www.w3.org/1999/xlink}href\", \"\")\n",
    "                if loc_id and \"us-gaap_\" in href:\n",
    "                    tag = href.split(\"#\")[-1]\n",
    "                    loc_map[loc_id] = tag\n",
    "\n",
    "            for arc in root.findall(\".//{http://www.xbrl.org/2003/linkbase}presentationArc\"):\n",
    "                from_id = arc.attrib.get(\"{http://www.w3.org/1999/xlink}from\")\n",
    "                to_id = arc.attrib.get(\"{http://www.w3.org/1999/xlink}to\")\n",
    "                arcs_by_from[from_id].append(to_id)\n",
    "\n",
    "            all_from = set(arcs_by_from.keys())\n",
    "            all_to = {to for children in arcs_by_from.values() for to in children}\n",
    "            root_ids = all_from - all_to\n",
    "\n",
    "            def walk(current_id, path):\n",
    "                tag = loc_map.get(current_id)\n",
    "                current_path = path + [tag] if tag else path\n",
    "                for child_id in arcs_by_from.get(current_id, []):\n",
    "                    child_tag = loc_map.get(child_id)\n",
    "                    if child_tag and child_tag.startswith(\"us-gaap_\"):\n",
    "                        clean_tag = child_tag.replace(\"us-gaap_\", \"\")\n",
    "                        tag_paths[clean_tag].add(\" > \".join(current_path + [clean_tag]))\n",
    "                    walk(child_id, current_path)\n",
    "\n",
    "            for root_id in root_ids:\n",
    "                walk(root_id, [])\n",
    "\n",
    "        except ET.ParseError:\n",
    "            continue\n",
    "\n",
    "    return tag_paths\n",
    "\n",
    "\n",
    "# Gather all presentation files (stm + dis + others)\n",
    "pre_files = []\n",
    "for root, _, files in os.walk(TAXONOMY_DIR):\n",
    "    for file in files:\n",
    "        if \"-pre-\" in file and file.endswith(\".xml\"):\n",
    "            pre_files.append(os.path.join(root, file))\n",
    "\n",
    "tag_hierarchy = extract_tag_hierarchy_paths(pre_files)\n",
    "\n",
    "# === STEP 5: ATTACH HIERARCHY TO TAGS ===\n",
    "ALL_STATEMENTS = {\n",
    "    \"Cash Flow Statement\",\n",
    "    \"Income Statement\",\n",
    "    \"Balance Sheet\",\n",
    "    \"Equity Statement\",\n",
    "    \"Comprehensive Income\"\n",
    "}\n",
    "\n",
    "final_filtered = []\n",
    "for row in final_output:\n",
    "    if row[\"statement_type\"] in ALL_STATEMENTS:\n",
    "        tag = row[\"tag\"]\n",
    "        row[\"taxonomy_hierarchy\"] = (\n",
    "            list(tag_hierarchy.get(tag))[0] if tag in tag_hierarchy else \"\"\n",
    "        )\n",
    "        final_filtered.append(row)\n",
    "\n",
    "# === STEP 6: WRITE OUTPUT CSV ===\n",
    "# os.makedirs(\"output\", exist_ok=True)\n",
    "with open(OUTPUT_PATH, \"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        \"tag\", \"statement_type\", \"balance\", \"period_type\",\n",
    "        \"description\", \"subcategory_path\", \"taxonomy_hierarchy\"\n",
    "    ])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(final_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append OFSS IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rapidfuzz.fuzz import token_sort_ratio\n",
    "\n",
    "# === CONFIG ===\n",
    "MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "INPUT_CSV = \"data/WITH_TAXONOMY_HIERARCHY_us_gaap_2025_with_all_statements_and_hierarchy.csv\"\n",
    "OFSS_JSON = \"../shared/open_financial_statement_schema.json\"\n",
    "OUTPUT_CSV = \"data/with_ofss_ids.csv\"\n",
    "SIMILARITY_THRESHOLD = 0.7\n",
    "\n",
    "# === DEVICE SETUP ===\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "model = SentenceTransformer(MODEL_NAME, device=device)\n",
    "\n",
    "# === FLATTEN NESTED OFSS MAP ===\n",
    "def flatten_ofss(d, parent_key=\"\"):\n",
    "    flat = {}\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key} > {k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            flat.update(flatten_ofss(v, new_key))\n",
    "        else:\n",
    "            flat[new_key] = v\n",
    "    return flat\n",
    "\n",
    "def normalize_stmt_prefix(stmt):\n",
    "    return stmt.lower().replace(\" statement\", \"\")\n",
    "\n",
    "def clean_path(raw):\n",
    "    tokens = raw.split(\" > \")\n",
    "    tokens = [t for t in tokens if not t.lower().endswith(\"abstract\")]\n",
    "    tokens = [t.replace(\"us-gaap_\", \"\").replace(\"_\", \" \") for t in tokens]\n",
    "    return \" > \".join(tokens)\n",
    "\n",
    "# === LOAD OFSS ===\n",
    "with open(OFSS_JSON, \"r\") as f:\n",
    "    nested_ofss = json.load(f)\n",
    "\n",
    "flat_ofss = flatten_ofss(nested_ofss)\n",
    "ofss_names = list(flat_ofss.keys())\n",
    "ofss_ids = [flat_ofss[name] for name in ofss_names]\n",
    "ofss_embeddings = model.encode([s.lower() for s in ofss_names], convert_to_tensor=True)\n",
    "\n",
    "# === MATCH FUNCTION ===\n",
    "def try_match_multi(stmt, desc, tag, path_raw):\n",
    "    \"\"\"\n",
    "    Attempts multiple query formats and returns the best match above threshold.\n",
    "    \"\"\"\n",
    "    queries = []\n",
    "    if desc:\n",
    "        queries.append(f\"{stmt} > {desc}\")\n",
    "    if path_raw:\n",
    "        path = clean_path(path_raw)\n",
    "        queries.append(f\"{stmt} > {path}\")\n",
    "    if tag:\n",
    "        queries.append(f\"{stmt} > {tag}\")\n",
    "    if desc and tag:\n",
    "        queries.append(f\"{stmt} > {desc} > {tag}\")\n",
    "\n",
    "    best = None\n",
    "    best_score = -1\n",
    "\n",
    "    for query_str in queries:\n",
    "        query_emb = model.encode(query_str.lower(), convert_to_tensor=True)\n",
    "        cos_scores = util.cos_sim(query_emb, ofss_embeddings)[0]\n",
    "\n",
    "        for i, score in enumerate(cos_scores):\n",
    "            name = ofss_names[i]\n",
    "            if normalize_stmt_prefix(name.split(\" > \")[0]) != normalize_stmt_prefix(stmt):\n",
    "                continue\n",
    "\n",
    "            fuzzy_score = token_sort_ratio(query_str.lower(), name.lower()) / 100\n",
    "            combined_score = (0.85 * score.item()) + (0.15 * fuzzy_score)\n",
    "\n",
    "            if combined_score > SIMILARITY_THRESHOLD and combined_score > best_score:\n",
    "                best = (i, score.item(), name, query_str)\n",
    "                best_score = combined_score\n",
    "\n",
    "    if best:\n",
    "        idx, raw_score, name, query = best\n",
    "        return idx, raw_score, name, query\n",
    "    return None, None, None, None\n",
    "\n",
    "# === PROCESS CSV ===\n",
    "output_rows = []\n",
    "with open(INPUT_CSV, newline=\"\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in tqdm(reader, desc=\"Processing rows\"):\n",
    "        stmt = row[\"statement_type\"].strip()\n",
    "        tag = row[\"tag\"]\n",
    "        balance = row[\"balance\"]\n",
    "        desc = row.get(\"description\", \"\").strip()\n",
    "        path_raw = row[\"taxonomy_hierarchy\"].strip()\n",
    "\n",
    "        row[\"ofss_id\"] = \"\"\n",
    "        row[\"ofss_flattened_name\"] = \"\"\n",
    "        row[\"map_approach\"] = \"\"\n",
    "\n",
    "        if not stmt or not path_raw:\n",
    "            print(f\"{tag} | [SKIPPED: missing statement or path]\")\n",
    "            output_rows.append(row)\n",
    "            continue\n",
    "\n",
    "        idx, score, name, query_used = try_match_multi(stmt, desc, tag, path_raw)\n",
    "        if idx is not None:\n",
    "            row[\"ofss_id\"] = str(ofss_ids[idx])\n",
    "            row[\"ofss_flattened_name\"] = name.lower()\n",
    "            row[\"map_approach\"] = \"multi\"\n",
    "            row[\"score\"] = score\n",
    "            print(f\"{tag} | [mapped.multi ({score:.2f})] {query_used} → {row['ofss_flattened_name']}\")\n",
    "        else:\n",
    "            print(f\"{tag} | [unmapped.multi]\")\n",
    "\n",
    "        output_rows.append(row)\n",
    "\n",
    "# === SAVE TO CSV ===\n",
    "fieldnames = list(output_rows[0].keys())\n",
    "with open(OUTPUT_CSV, \"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
