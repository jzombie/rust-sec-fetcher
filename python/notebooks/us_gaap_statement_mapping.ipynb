{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "\n",
    "# https://www.fasb.org/page/detail?pageId=/projects/FASB-Taxonomies/2025-gaap-financial-reporting-taxonomy.html\n",
    "# 2025 Taxonomy downloads: https://xbrl.fasb.org/us-gaap/2025/us-gaap-2025.zip\n",
    "\n",
    "# === CONFIG ===\n",
    "TAXONOMY_DIR = \"data/us-gaap-2025/\"\n",
    "ELTS_XSD = os.path.join(TAXONOMY_DIR, \"elts\", \"us-gaap-2025.xsd\")\n",
    "STM_DIR = os.path.join(TAXONOMY_DIR, \"stm\")\n",
    "\n",
    "OUTPUT_PATH = \"data/WITH_TAXONOMY_HIERARCHY_us_gaap_2025_with_all_statements_and_hierarchy.csv\"\n",
    "\n",
    "FILENAME_STATEMENT_MAP = {\n",
    "    \"scf\": \"Cash Flow Statement\",\n",
    "    \"soi\": \"Income Statement\",\n",
    "    \"sfp\": \"Balance Sheet\",\n",
    "    \"sheci\": \"Equity Statement\",\n",
    "    \"soc\": \"Comprehensive Income\"\n",
    "}\n",
    "\n",
    "XBRLI_NS = \"http://www.xbrl.org/2003/instance\"\n",
    "BALANCE_KEY = f\"{{{XBRLI_NS}}}balance\"\n",
    "PERIOD_TYPE_KEY = f\"{{{XBRLI_NS}}}periodType\"\n",
    "\n",
    "\n",
    "def generate_description(tag_name):\n",
    "    return re.sub(\n",
    "        r'(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])',\n",
    "        ' ',\n",
    "        tag_name\n",
    "    ).lower()\n",
    "\n",
    "\n",
    "# === STEP 1: TAG METADATA ===\n",
    "tag_metadata = {}\n",
    "tree = ET.parse(ELTS_XSD)\n",
    "root = tree.getroot()\n",
    "for el in root.findall(\".//{http://www.w3.org/2001/XMLSchema}element\"):\n",
    "    name = el.attrib.get(\"name\")\n",
    "    if not name or el.attrib.get(\"abstract\") == \"true\":\n",
    "        continue\n",
    "    tag_metadata[name] = {\n",
    "        \"balance\": el.attrib.get(BALANCE_KEY, \"\"),\n",
    "        \"period_type\": el.attrib.get(PERIOD_TYPE_KEY, \"\"),\n",
    "        \"statements\": set()\n",
    "    }\n",
    "\n",
    "# === STEP 2: STATEMENT TYPE MAPPING ===\n",
    "for file in os.listdir(STM_DIR):\n",
    "    if not file.endswith(\".xml\") or \"-pre-\" not in file:\n",
    "        continue\n",
    "    parts = file.split(\"-\")\n",
    "    if len(parts) < 4:\n",
    "        continue\n",
    "    stmt_key = parts[3]\n",
    "    inferred_statement = FILENAME_STATEMENT_MAP.get(stmt_key)\n",
    "    if not inferred_statement:\n",
    "        continue\n",
    "\n",
    "    tree = ET.parse(os.path.join(STM_DIR, file))\n",
    "    root = tree.getroot()\n",
    "    for loc in root.findall(\".//{http://www.xbrl.org/2003/linkbase}loc\"):\n",
    "        href = loc.attrib.get(\"{http://www.w3.org/1999/xlink}href\", \"\")\n",
    "        tag = href.split(\"#\")[-1]\n",
    "        if tag.startswith(\"us-gaap_\"):\n",
    "            tag = tag.replace(\"us-gaap_\", \"\")\n",
    "        if tag in tag_metadata:\n",
    "            tag_metadata[tag][\"statements\"].add(inferred_statement)\n",
    "\n",
    "# === STEP 3: BUILD BASE OUTPUT ===\n",
    "final_output = []\n",
    "for tag, meta in tag_metadata.items():\n",
    "    for stmt in meta[\"statements\"]:\n",
    "        final_output.append({\n",
    "            \"tag\": tag,\n",
    "            \"statement_type\": stmt,\n",
    "            \"balance\": meta[\"balance\"],\n",
    "            \"period_type\": meta[\"period_type\"],\n",
    "            \"description\": generate_description(tag),\n",
    "            \"subcategory_path\": \"\"\n",
    "        })\n",
    "\n",
    "# === STEP 4: EXTRACT TAXONOMY HIERARCHY PATHS ===\n",
    "def extract_tag_hierarchy_paths(pre_files):\n",
    "    tag_paths = defaultdict(set)\n",
    "\n",
    "    for path in pre_files:\n",
    "        try:\n",
    "            tree = ET.parse(path)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            loc_map = {}\n",
    "            arcs_by_from = defaultdict(list)\n",
    "\n",
    "            for loc in root.findall(\".//{http://www.xbrl.org/2003/linkbase}loc\"):\n",
    "                loc_id = loc.attrib.get(\"{http://www.w3.org/1999/xlink}label\")\n",
    "                href = loc.attrib.get(\"{http://www.w3.org/1999/xlink}href\", \"\")\n",
    "                if loc_id and \"us-gaap_\" in href:\n",
    "                    tag = href.split(\"#\")[-1]\n",
    "                    loc_map[loc_id] = tag\n",
    "\n",
    "            for arc in root.findall(\".//{http://www.xbrl.org/2003/linkbase}presentationArc\"):\n",
    "                from_id = arc.attrib.get(\"{http://www.w3.org/1999/xlink}from\")\n",
    "                to_id = arc.attrib.get(\"{http://www.w3.org/1999/xlink}to\")\n",
    "                arcs_by_from[from_id].append(to_id)\n",
    "\n",
    "            all_from = set(arcs_by_from.keys())\n",
    "            all_to = {to for children in arcs_by_from.values() for to in children}\n",
    "            root_ids = all_from - all_to\n",
    "\n",
    "            def walk(current_id, path):\n",
    "                tag = loc_map.get(current_id)\n",
    "                current_path = path + [tag] if tag else path\n",
    "                for child_id in arcs_by_from.get(current_id, []):\n",
    "                    child_tag = loc_map.get(child_id)\n",
    "                    if child_tag and child_tag.startswith(\"us-gaap_\"):\n",
    "                        clean_tag = child_tag.replace(\"us-gaap_\", \"\")\n",
    "                        tag_paths[clean_tag].add(\" > \".join(current_path + [clean_tag]))\n",
    "                    walk(child_id, current_path)\n",
    "\n",
    "            for root_id in root_ids:\n",
    "                walk(root_id, [])\n",
    "\n",
    "        except ET.ParseError:\n",
    "            continue\n",
    "\n",
    "    return tag_paths\n",
    "\n",
    "\n",
    "# Gather all presentation files (stm + dis + others)\n",
    "pre_files = []\n",
    "for root, _, files in os.walk(TAXONOMY_DIR):\n",
    "    for file in files:\n",
    "        if \"-pre-\" in file and file.endswith(\".xml\"):\n",
    "            pre_files.append(os.path.join(root, file))\n",
    "\n",
    "tag_hierarchy = extract_tag_hierarchy_paths(pre_files)\n",
    "\n",
    "# === STEP 5: ATTACH HIERARCHY TO TAGS ===\n",
    "ALL_STATEMENTS = {\n",
    "    \"Cash Flow Statement\",\n",
    "    \"Income Statement\",\n",
    "    \"Balance Sheet\",\n",
    "    \"Equity Statement\",\n",
    "    \"Comprehensive Income\"\n",
    "}\n",
    "\n",
    "final_filtered = []\n",
    "for row in final_output:\n",
    "    if row[\"statement_type\"] in ALL_STATEMENTS:\n",
    "        tag = row[\"tag\"]\n",
    "        row[\"taxonomy_hierarchy\"] = (\n",
    "            list(tag_hierarchy.get(tag))[0] if tag in tag_hierarchy else \"\"\n",
    "        )\n",
    "        final_filtered.append(row)\n",
    "\n",
    "# === STEP 6: WRITE OUTPUT CSV ===\n",
    "# os.makedirs(\"output\", exist_ok=True)\n",
    "with open(OUTPUT_PATH, \"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\n",
    "        \"tag\", \"statement_type\", \"balance\", \"period_type\",\n",
    "        \"description\", \"subcategory_path\", \"taxonomy_hierarchy\"\n",
    "    ])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(final_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append OFSS IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# === CONFIG ===\n",
    "MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "INPUT_CSV = \"data/WITH_TAXONOMY_HIERARCHY_us_gaap_2025_with_all_statements_and_hierarchy.csv\"\n",
    "OFSS_JSON = \"../shared/open_financial_statement_schema.json\"\n",
    "OUTPUT_CSV = \"data/with_ofss_ids.csv\"\n",
    "SIMILARITY_THRESHOLD = 0.7\n",
    "\n",
    "# === DEVICE SETUP ===\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "model = SentenceTransformer(MODEL_NAME, device=device)\n",
    "\n",
    "# === FLATTEN NESTED OFSS MAP ===\n",
    "def flatten_ofss(d, parent_key=\"\"):\n",
    "    flat = {}\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}/{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            flat.update(flatten_ofss(v, new_key))\n",
    "        else:\n",
    "            flat[new_key] = v\n",
    "    return flat\n",
    "\n",
    "def normalize_stmt_prefix(stmt):\n",
    "    stmt_prefix = stmt.lower().replace(\" statement\", \"\")\n",
    "    return stmt_prefix\n",
    "\n",
    "with open(OFSS_JSON, \"r\") as f:\n",
    "    nested_ofss = json.load(f)\n",
    "\n",
    "flat_ofss = flatten_ofss(nested_ofss)\n",
    "ofss_names = list(flat_ofss.keys())\n",
    "ofss_ids = [flat_ofss[name] for name in ofss_names]\n",
    "ofss_embeddings = model.encode([s.lower() for s in ofss_names], convert_to_tensor=True)\n",
    "\n",
    "# === STATEMENT NORMALIZATION ===\n",
    "# STATEMENT_NORMALIZATION = {\n",
    "#     \"Equity Statement\": \"Balance Sheet\",\n",
    "#     \"Comprehensive Income\": \"Income Statement\"\n",
    "# }\n",
    "\n",
    "def clean_path(raw):\n",
    "    tokens = raw.split(\" > \")\n",
    "    tokens = [t for t in tokens if not t.lower().endswith(\"abstract\")]\n",
    "    tokens = [t.replace(\"us-gaap_\", \"\").replace(\"_\", \" \") for t in tokens]\n",
    "    return \" > \".join(tokens)\n",
    "\n",
    "# === MATCH FUNCTION ===\n",
    "def try_match(query_str, stmt):\n",
    "    query_emb = model.encode(query_str.lower(), convert_to_tensor=True)\n",
    "    allowed_indices = [\n",
    "        i for i, name in enumerate(ofss_names)\n",
    "        if normalize_stmt_prefix(name.lower().split(\"/\")[0]) == normalize_stmt_prefix(stmt)\n",
    "    ]\n",
    "    if not allowed_indices:\n",
    "        return None, None, None\n",
    "\n",
    "    filtered_embeddings = ofss_embeddings[allowed_indices]\n",
    "    cos_scores = util.cos_sim(query_emb, filtered_embeddings)[0]\n",
    "    best_local_idx = torch.argmax(cos_scores).item()\n",
    "    best_score = cos_scores[best_local_idx].item()\n",
    "\n",
    "    if best_score < SIMILARITY_THRESHOLD:\n",
    "        return None, None, None\n",
    "\n",
    "    best_idx = allowed_indices[best_local_idx]\n",
    "    return best_idx, best_score, ofss_names[best_idx]\n",
    "\n",
    "# === PROCESS CSV ===\n",
    "output_rows = []\n",
    "with open(INPUT_CSV, newline=\"\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in tqdm(reader, desc=\"Processing rows\"):\n",
    "        stmt = row[\"statement_type\"].strip()\n",
    "        # stmt = STATEMENT_NORMALIZATION.get(stmt, stmt)\n",
    "\n",
    "        tag = row[\"tag\"]\n",
    "        desc = row.get(\"description\", \"\").strip()\n",
    "        path_raw = row[\"taxonomy_hierarchy\"].strip()\n",
    "\n",
    "        row[\"ofss_id\"] = \"\"\n",
    "        row[\"ofss_flattened_name\"] = \"\"\n",
    "        row[\"map_approach\"] = \"\"\n",
    "\n",
    "        if not stmt or not path_raw:\n",
    "            print(f\"{tag} | [SKIPPED: missing statement or path]\")\n",
    "            output_rows.append(row)\n",
    "            continue\n",
    "\n",
    "        has_match = False\n",
    "\n",
    "        # --- Approach 1: Description only ---\n",
    "        if desc:\n",
    "            query1 = f\"{stmt} > {desc}\"\n",
    "            idx, score, name = try_match(query1, stmt)\n",
    "            if idx is not None:\n",
    "                has_match = True\n",
    "\n",
    "                row[\"ofss_id\"] = str(ofss_ids[idx])\n",
    "                row[\"ofss_flattened_name\"] = name.lower().replace(\"/\", \" > \")\n",
    "                row[\"map_approach\"] = \"1\"\n",
    "                row['score'] = score\n",
    "                print(f\"{tag} | [mapped.1 ({score:.2f})] {query1} → {row['ofss_flattened_name']}\")\n",
    "            else:\n",
    "                print(f\"{tag} | [unmapped] {query1}\")\n",
    "        else:\n",
    "            print(f\"{tag} | [unmapped: no description]\")\n",
    "\n",
    "        # --- Approach 2: Taxonomy path ---\n",
    "        if has_match == False:\n",
    "            path = clean_path(path_raw)\n",
    "            query2 = f\"{stmt} > {path}\"\n",
    "            idx, score, name = try_match(query2, stmt)\n",
    "            if idx is not None:\n",
    "                row[\"ofss_id\"] = str(ofss_ids[idx])\n",
    "                row[\"ofss_flattened_name\"] = name.lower().replace(\"/\", \" > \")\n",
    "                row[\"map_approach\"] = \"2\"\n",
    "                row['score'] = score\n",
    "                print(f\"{tag} | [mapped.2 ({score:.2f})] {query2} → {row['ofss_flattened_name']}\")\n",
    "\n",
    "        output_rows.append(row)\n",
    "\n",
    "# === SAVE TO CSV ===\n",
    "fieldnames = list(output_rows[0].keys())\n",
    "with open(OUTPUT_CSV, \"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load existing CSV\n",
    "df = pd.read_csv(OUTPUT_CSV)\n",
    "\n",
    "# Define explicit validation rules\n",
    "def is_valid_mapping(row):\n",
    "    stmt = row['statement_type'].lower()\n",
    "    balance = str(row['balance']).lower()\n",
    "    mapped = str(row['ofss_flattened_name']).lower()\n",
    "\n",
    "    # Balance sheet validation\n",
    "    if stmt == \"balance sheet\":\n",
    "        if balance == \"debit\" and any(x in mapped for x in [\"liabilities\", \"payable\", \"accrued\"]):\n",
    "            return False\n",
    "        if balance == \"credit\" and any(x in mapped for x in [\"assets\", \"cash\", \"inventory\", \"receivable\"]):\n",
    "            return False\n",
    "\n",
    "    # Cash Flow validation (investing and financing activities misclassified as operating)\n",
    "    if stmt == \"cash flow statement\":\n",
    "        investing_keywords = [\"acquire\", \"investment\", \"purchase\", \"building\", \"property\", \"fund\", \"financing\", \"debt\"]\n",
    "        if any(keyword in row['tag'].lower() for keyword in investing_keywords) and \"operating activities\" in mapped:\n",
    "            return False\n",
    "\n",
    "    # Comprehensive income validation\n",
    "    if stmt == \"comprehensive income\":\n",
    "        if \"comprehensive income\" not in mapped:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Identify invalid mappings\n",
    "invalid_mask = ~df.apply(is_valid_mapping, axis=1)\n",
    "invalid_entries = df[invalid_mask]\n",
    "\n",
    "# Display the number of invalid entries found\n",
    "print(f\"Found {len(invalid_entries)} invalid entries.\")\n",
    "\n",
    "# Clear mappings for invalid entries to allow manual review or reprocessing\n",
    "df.loc[invalid_mask, [\"ofss_id\", \"ofss_flattened_name\", \"map_approach\", \"score\"]] = \"\"\n",
    "\n",
    "# Save the corrected CSV\n",
    "df.to_csv(\"data/with_ofss_ids_corrected.csv\", index=False)\n",
    "\n",
    "# Show sample of corrections made for verification\n",
    "invalid_entries.sample(min(10, len(invalid_entries)), random_state=42)[['tag', 'statement_type', 'description', 'ofss_flattened_name', 'score']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
