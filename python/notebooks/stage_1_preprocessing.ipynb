{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa69b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# This snippet ensures consistent import paths across environments.\n",
    "# When running notebooks via JupyterLab's web UI, the current working\n",
    "# directory is often different (e.g., /notebooks) compared to VS Code,\n",
    "# which typically starts at the project root. This handles that by \n",
    "# retrying the import after changing to the parent directory.\n",
    "# \n",
    "# Include this at the top of every notebook to standardize imports\n",
    "# across development environments.\n",
    "\n",
    "try:\n",
    "    from utils.os import chdir_to_git_root\n",
    "except ModuleNotFoundError:\n",
    "    os.chdir(Path.cwd().parent)\n",
    "    print(f\"Retrying import from: {os.getcwd()}\")\n",
    "    from utils.os import chdir_to_git_root\n",
    "\n",
    "chdir_to_git_root(\"python\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982f0e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# from models.pytorch.narrative_stack.stage1.preprocessing import generate_concept_unit_embeddings, generate_concepts_report\n",
    "from db import DbUsGaap\n",
    "\n",
    "db_us_gaap = DbUsGaap()\n",
    "data_dir = \"../data/june-us-gaap\" # Where CSV data is read from (once CSV file per symbol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc8bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: For debugging / monitoring purposes only\n",
    "# Determine \"category stack\" depths\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from utils.csv import walk_us_gaap_csvs, get_filtered_us_gaap_form_rows_for_symbol\n",
    "\n",
    "class RunningStats:\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.total = 0\n",
    "        self.max_val = 0\n",
    "        self.values = []\n",
    "\n",
    "    def update(self, val: int):\n",
    "        self.count += 1\n",
    "        self.total += val\n",
    "        self.max_val = max(self.max_val, val)\n",
    "        self.values.append(val)  # Optional: remove this if median not needed\n",
    "\n",
    "    def finalize(self):\n",
    "        result = {\n",
    "            \"avg\": self.total / self.count if self.count else 0,\n",
    "            \"max\": self.max_val,\n",
    "        }\n",
    "        if self.values:\n",
    "            result[\"median\"] = float(np.median(self.values))\n",
    "        return result\n",
    "\n",
    "# Initialize running stats per key\n",
    "stats = defaultdict(RunningStats)\n",
    "\n",
    "gen = walk_us_gaap_csvs(data_dir, db_us_gaap, \"row\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        row = next(gen)\n",
    "        counter = defaultdict(int)\n",
    "        for entry in row.entries:\n",
    "            key = (entry.balance_type or \"none\", entry.period_type or \"none\")\n",
    "            counter[key] += 1\n",
    "        for key, val in counter.items():\n",
    "            stats[key].update(val)\n",
    "except StopIteration:\n",
    "    pass\n",
    "\n",
    "# Final summary\n",
    "summary = {key: stat.finalize() for key, stat in stats.items()}\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0891b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: For debugging / monitoring purposes only\n",
    "\n",
    "import numpy as np\n",
    "from utils.csv import walk_us_gaap_csvs, UsGaapRowRecord\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def generate_concepts_report_from_walker(\n",
    "    data_dir: Path,\n",
    "    db_us_gaap: DbUsGaap,\n",
    "    filtered_symbols: set[str] | None = None,\n",
    "):\n",
    "    gen = walk_us_gaap_csvs(\n",
    "        data_dir=data_dir,\n",
    "        db_us_gaap=db_us_gaap,\n",
    "        walk_type=\"row\",\n",
    "        filtered_symbols=filtered_symbols,\n",
    "    )\n",
    "\n",
    "    unit_stats = defaultdict(list)\n",
    "    concept_by_unit = defaultdict(set)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            row = next(gen)\n",
    "            if isinstance(row, UsGaapRowRecord):\n",
    "                for entry in row.entries:\n",
    "                    unit_stats[entry.uom].append(entry.value)\n",
    "                    concept_by_unit[entry.uom].add(entry.concept)\n",
    "    except StopIteration as stop:\n",
    "        summary = stop.value\n",
    "\n",
    "    print(f\"\\n✅ Scanned {len(summary.csv_files)} files.\")\n",
    "    print(\n",
    "        f\"📦 Found {len(unit_stats)} numeric units and \"\n",
    "        f\"{len(summary.non_numeric_units)} non-numeric units.\"\n",
    "    )\n",
    "\n",
    "    for unit, values in sorted(unit_stats.items()):\n",
    "        arr = np.array(values)\n",
    "        print(f\"🔹 {unit}\")\n",
    "        print(f\"   Count: {len(arr)}\")\n",
    "        print(f\"   Min:   {arr.min():,.4f}\")\n",
    "        print(f\"   Max:   {arr.max():,.4f}\")\n",
    "        print(f\"   Mean:  {arr.mean():,.4f}\")\n",
    "        print(f\"   Std:   {arr.std():,.4f}\")\n",
    "        print(f\"   Concepts: {', '.join(sorted(concept_by_unit[unit]))}\")\n",
    "\n",
    "    if summary.non_numeric_units:\n",
    "        print(\"\\n⚠️ Non-numeric units encountered:\")\n",
    "        for unit in sorted(summary.non_numeric_units):\n",
    "            print(f\"  - {unit}\")\n",
    "\n",
    "    total_values = sum(len(v) for v in unit_stats.values())\n",
    "    print(f\"\\n🧮 Total values extracted: {total_values:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a179faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_concepts_report_from_walker(data_dir, db_us_gaap, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8abebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Collecting valid concepts...\")\n",
    "\n",
    "from utils.csv import walk_us_gaap_csvs, get_filtered_us_gaap_form_rows_for_symbol\n",
    "\n",
    "\n",
    "gen = walk_us_gaap_csvs(data_dir, db_us_gaap, \"row\", {\"AAPL\"})\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        data = next(gen)\n",
    "        display(data)\n",
    "except StopIteration as stop:\n",
    "    summary = stop.value\n",
    "    display(summary)\n",
    "\n",
    "# for data in get_filtered_us_gaap_form_rows_for_symbol(data_dir, db_us_gaap, \"NVDA\", {\"10-K\", \"10-Q\"}):\n",
    "#     display(data)\n",
    "    \n",
    "\n",
    "\n",
    "# from simd_r_drive import DataStore\n",
    "\n",
    "# data_store = DataStore(\"proto.bin\")\n",
    "\n",
    "# logging.info(\"Extracting concept unit value tuples...\")\n",
    "# extracted_concept_unit_value_data = extract_concept_unit_value_tuples(data_dir, valid_concepts, data_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024539ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For deterministic hashing (TODO: Move to tests)\n",
    "\n",
    "# import hashlib\n",
    "# import pickle\n",
    "\n",
    "# # def hash_extracted_data(data: ExtractedConceptUnitValueData) -> str:\n",
    "# def hash_extracted_data(data) -> str:\n",
    "#     \"\"\"\n",
    "#     Computes a SHA-256 hash of the full extracted concept/unit/value data structure.\n",
    "#     This includes tuples, unit stats, and file list — all serialized deterministically.\n",
    "#     \"\"\"\n",
    "#     # Serialize using protocol=5 (highest and deterministic in modern Python)\n",
    "#     serialized = pickle.dumps(data.dict(), protocol=5)\n",
    "#     return hashlib.sha256(serialized).hexdigest()\n",
    "\n",
    "# hash_extracted_data(extracted_concept_unit_value_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb9fb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Fetching...\")\n",
    "# extracted_concept_unit_value_data_2 = extract_concept_unit_value_tuples(data_dir, valid_concepts)\n",
    "\n",
    "# print(\"Hashing...\")\n",
    "# hash_extracted_data(extracted_concept_unit_value_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a530ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View concepts report (not needed for preprocessing but contains useful information)\n",
    "\n",
    "generate_concepts_report(extracted_concept_unit_value_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb05ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pytorch import get_device, seed_everything\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "logging.info(\"Collecting concept unit pairs...\")\n",
    "concept_unit_pairs = collect_concept_unit_pairs(extracted_concept_unit_value_data)\n",
    "\n",
    "logging.info(f\"Total concept unit pairs: {len(concept_unit_pairs)}\")\n",
    "\n",
    "logging.info(\"Generating concept unit embeddings...\")\n",
    "concept_unit_embeddings = generate_concept_unit_embeddings(concept_unit_pairs, device=device)\n",
    "# concept_unit_embeddings_2 = generate_concept_unit_embeddings(concept_unit_pairs, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456fe7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Normalize for cosine similarity\n",
    "# A = concept_unit_embeddings_1\n",
    "# B = concept_unit_embeddings_2\n",
    "\n",
    "# A_norm = A / np.linalg.norm(A, axis=1, keepdims=True)\n",
    "# B_norm = B / np.linalg.norm(B, axis=1, keepdims=True)\n",
    "\n",
    "# # Cosine similarity per row\n",
    "# cos_sim = np.sum(A_norm * B_norm, axis=1)\n",
    "\n",
    "# # Report\n",
    "# print(f\"Cosine similarity:\")\n",
    "# print(f\"  Mean: {cos_sim.mean():.8f}\")\n",
    "# print(f\"  Min:  {cos_sim.min():.8f}\")\n",
    "# print(f\"  Std:  {cos_sim.std():.8f}\")\n",
    "\n",
    "# # Optional: show rows below threshold\n",
    "# threshold = 0.999\n",
    "# bad_indices = np.where(cos_sim < threshold)[0]\n",
    "# print(f\"\\n🔻 Below {threshold}: {len(bad_indices)} / {len(cos_sim)} rows\")\n",
    "# if len(bad_indices):\n",
    "#     for idx in bad_indices[:10]:\n",
    "#         print(f\"  Row {idx}: cosine = {cos_sim[idx]:.8f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concept_unit_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31d01ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pytorch.narrative_stack.stage1.preprocessing.plots import plot_pca_explanation\n",
    "\n",
    "dim = plot_pca_explanation(concept_unit_embeddings, variance_threshold=0.95)\n",
    "\n",
    "display(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43904af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ff5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pytorch.narrative_stack.stage1.preprocessing import pca_compress_concept_unit_embeddings\n",
    "\n",
    "\n",
    "pca_compressed_concept_unit_embeddings, pca = pca_compress_concept_unit_embeddings(concept_unit_embeddings, n_components=243)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c0946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Prototype this\n",
    "\n",
    "# import joblib\n",
    "# import io\n",
    "\n",
    "# # `pca` is the fitted PCA object\n",
    "# buffer = io.BytesIO()\n",
    "# joblib.dump(pca, buffer)\n",
    "# pca_bytes = buffer.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8218043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pytorch.narrative_stack.stage1.preprocessing.plots import plot_semantic_embeddings\n",
    "\n",
    "plot_semantic_embeddings(pca_compressed_concept_unit_embeddings, title=\"PCA Semantic Embedding Scatterplot\")\n",
    "plot_semantic_embeddings(concept_unit_embeddings, title=\"Raw Semantic Embedding Scatterplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30496feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_compressed_concept_unit_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe275055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# TODO: Add types\n",
    "def save_concept_unit_value_tuples(pca_compressed_concept_unit_embeddings, concept_unit_pairs, concept_unit_value_tuples, file_path):\n",
    "    assert len(pca_compressed_concept_unit_embeddings) == len(concept_unit_pairs), \\\n",
    "        f\"Mismatch: {len(pca_compressed_concept_unit_embeddings)} embeddings vs {len(concept_unit_pairs)} keys\"\n",
    "\n",
    "    # Save both embeddings and tuples\n",
    "    np.savez_compressed(\n",
    "        file_path,\n",
    "        keys=np.array([f\"{c}::{u}\" for c, u in concept_unit_pairs]),\n",
    "        embeddings=pca_compressed_concept_unit_embeddings,\n",
    "        concept_unit_value_tuples=np.array(concept_unit_value_tuples, dtype=object)\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Saved {len(concept_unit_value_tuples):,} tuples and {len(pca_compressed_concept_unit_embeddings):,} embeddings to '{file_path}'\")\n",
    "\n",
    "\n",
    "save_concept_unit_value_tuples(\n",
    "    pca_compressed_concept_unit_embeddings,\n",
    "    concept_unit_pairs,\n",
    "    extracted_concept_unit_value_data.concept_unit_value_tuples,\n",
    "    \"data/stage1_latents.npz\" # TODO: Rename! These are not latent vectors!\n",
    ")\n",
    "\n",
    "# save_concept_unit_value_tuples(\n",
    "#     pca_compressed_concept_unit_embeddings_2,\n",
    "#     concept_unit_pairs,\n",
    "#     extracted_concept_unit_value_data.concept_unit_value_tuples,\n",
    "#     \"data/stage1_latents_new_2.npz\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c33383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Validate subsequent\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# # Load from disk\n",
    "# new_data = np.load(\"data/stage1_latents_new.npz\", allow_pickle=True)\n",
    "# new_concept_unit_value_tuples = new_data[\"concept_unit_value_tuples\"].tolist()\n",
    "# new_embeddings = new_data[\"embeddings\"]\n",
    "\n",
    "# # Check shape match\n",
    "# assert len(pca_compressed_concept_unit_embeddings) == len(new_embeddings), \\\n",
    "#     \"Mismatch in embedding row counts\"\n",
    "\n",
    "# # Cosine similarity check\n",
    "# def cosine_similarity(a, b):\n",
    "#     a = a / np.linalg.norm(a)\n",
    "#     b = b / np.linalg.norm(b)\n",
    "#     return np.dot(a, b)\n",
    "\n",
    "# cos_sims = []\n",
    "# for a_vec, b_vec in zip(pca_compressed_concept_unit_embeddings, new_embeddings):\n",
    "#     sim = cosine_similarity(a_vec.astype(np.float64), b_vec.astype(np.float64))\n",
    "#     cos_sims.append(sim)\n",
    "\n",
    "# # Report\n",
    "# cos_sims = np.array(cos_sims)\n",
    "# print(f\"✅ Compared {len(cos_sims)} rows\")\n",
    "# print(f\"🔹 Mean cosine similarity: {cos_sims.mean():.8f}\")\n",
    "# print(f\"🔹 Min cosine similarity:  {cos_sims.min():.8f}\")\n",
    "# print(f\"🔹 Std dev:                {cos_sims.std():.8f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af223c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Load saved latent data\n",
    "# old_data = np.load(\"data/stage1_latents.npz\", allow_pickle=True)\n",
    "\n",
    "# # Build embedding map\n",
    "# embedding_map = {\n",
    "#     tuple(key.split(\"::\", 1)): vec\n",
    "#     for key, vec in zip(old_data[\"keys\"], old_data[\"embeddings\"])\n",
    "# }\n",
    "\n",
    "# # Load concept-unit-value tuples\n",
    "# old_concept_unit_value_tuples = old_data[\"concept_unit_value_tuples\"].tolist()\n",
    "\n",
    "# # Load saved latent data\n",
    "# new_data = np.load(\"data/stage1_latents_new.npz\", allow_pickle=True)\n",
    "\n",
    "# # Build embedding map\n",
    "# embedding_map = {\n",
    "#     tuple(key.split(\"::\", 1)): vec\n",
    "#     for key, vec in zip(new_data[\"keys\"], new_data[\"embeddings\"])\n",
    "# }\n",
    "\n",
    "# # Load concept-unit-value tuples\n",
    "# new_concept_unit_value_tuples = new_data[\"concept_unit_value_tuples\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d25157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from hashlib import sha256\n",
    "\n",
    "# # a = np.load(\"data/stage1_latents.npz\", allow_pickle=True)\n",
    "# a = np.load(\"data/stage1_latents_new_1.npz\", allow_pickle=True)\n",
    "# b = np.load(\"data/stage1_latents_new_2.npz\", allow_pickle=True)\n",
    "\n",
    "# # with open(\"data/stage1_latents_new_1.pkl\", \"rb\") as f:\n",
    "# #     a = pickle.load(f)\n",
    "\n",
    "# # with open(\"data/stage1_latents_new_2.pkl\", \"rb\") as f:\n",
    "# #     b = pickle.load(f)\n",
    "\n",
    "# def hash_array(arr):\n",
    "#     return sha256(np.ascontiguousarray(arr)).hexdigest()\n",
    "\n",
    "# print(\"Hash a[keys]:\", hash_array(a[\"keys\"]))\n",
    "# print(\"Hash b[keys]:\", hash_array(b[\"keys\"]))\n",
    "\n",
    "# for k in a.files:\n",
    "# # for k in a.keys():\n",
    "#     print(f\"Checking: {k}\")\n",
    "#     if k == \"embeddings\":\n",
    "#         A = a[k].astype(np.float32)\n",
    "#         B = b[k].astype(np.float32)\n",
    "#         assert A.shape == B.shape, \"Shape mismatch in embeddings\"\n",
    "\n",
    "#         # Normalize to unit vectors\n",
    "#         A_norm = A / np.linalg.norm(A, axis=1, keepdims=True)\n",
    "#         B_norm = B / np.linalg.norm(B, axis=1, keepdims=True)\n",
    "\n",
    "#         # Cosine similarity\n",
    "#         cos_sim = np.sum(A_norm * B_norm, axis=1)\n",
    "#         mean_sim = np.mean(cos_sim)\n",
    "#         min_sim = np.min(cos_sim)\n",
    "\n",
    "#         print(f\"Mean cosine similarity: {mean_sim:.8f}\")\n",
    "#         print(f\"Min cosine similarity:  {min_sim:.8f}\")\n",
    "#         print(f\"Std cosine similarity:  {cos_sim.std():.8f}\")\n",
    "#         assert min_sim > 0.999, f\"Cosine similarity too low in embeddings: {min_sim}\"\n",
    "#     else:\n",
    "#         assert np.array_equal(a[k], b[k]), f\"Mismatch in {k}\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
