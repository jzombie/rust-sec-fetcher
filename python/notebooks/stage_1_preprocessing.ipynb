{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa69b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# This snippet ensures consistent import paths across environments.\n",
    "# When running notebooks via JupyterLab's web UI, the current working\n",
    "# directory is often different (e.g., /notebooks) compared to VS Code,\n",
    "# which typically starts at the project root. This handles that by \n",
    "# retrying the import after changing to the parent directory.\n",
    "# \n",
    "# Include this at the top of every notebook to standardize imports\n",
    "# across development environments.\n",
    "\n",
    "try:\n",
    "    from utils.os import chdir_to_git_root\n",
    "except ModuleNotFoundError:\n",
    "    os.chdir(Path.cwd().parent)\n",
    "    print(f\"Retrying import from: {os.getcwd()}\")\n",
    "    from utils.os import chdir_to_git_root\n",
    "\n",
    "chdir_to_git_root(\"python\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982f0e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from models.pytorch.narrative_stack.stage1.preprocessing import extract_concept_unit_value_tuples, get_valid_concepts, collect_concept_unit_pairs, generate_concept_unit_embeddings, generate_concepts_report\n",
    "from db import DB\n",
    "\n",
    "db = DB()\n",
    "data_dir = \"../data/june-us-gaap\" # Where CSV data is read from (once CSV file per symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8abebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Collecting valid concepts...\")\n",
    "valid_concepts = get_valid_concepts(db)\n",
    "\n",
    "logging.info(\"Extracting concept unit value tuples...\")\n",
    "extracted_concept_unit_value_data = extract_concept_unit_value_tuples(data_dir, valid_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024539ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For deterministic hashing (TODO: Move to tests)\n",
    "\n",
    "# import hashlib\n",
    "# import pickle\n",
    "\n",
    "# # def hash_extracted_data(data: ExtractedConceptUnitValueData) -> str:\n",
    "# def hash_extracted_data(data) -> str:\n",
    "#     \"\"\"\n",
    "#     Computes a SHA-256 hash of the full extracted concept/unit/value data structure.\n",
    "#     This includes tuples, unit stats, and file list â€” all serialized deterministically.\n",
    "#     \"\"\"\n",
    "#     # Serialize using protocol=5 (highest and deterministic in modern Python)\n",
    "#     serialized = pickle.dumps(data.dict(), protocol=5)\n",
    "#     return hashlib.sha256(serialized).hexdigest()\n",
    "\n",
    "# hash_extracted_data(extracted_concept_unit_value_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb9fb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Fetching...\")\n",
    "# extracted_concept_unit_value_data_2 = extract_concept_unit_value_tuples(data_dir, valid_concepts)\n",
    "\n",
    "# print(\"Hashing...\")\n",
    "# hash_extracted_data(extracted_concept_unit_value_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a530ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View concepts report (not needed for preprocessing but contains useful information)\n",
    "\n",
    "generate_concepts_report(extracted_concept_unit_value_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb05ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pytorch import get_device, seed_everything\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "logging.info(\"Collecting concept unit pairs...\")\n",
    "concept_unit_pairs = collect_concept_unit_pairs(extracted_concept_unit_value_data)\n",
    "\n",
    "logging.info(f\"Total concept unit pairs: {len(concept_unit_pairs)}\")\n",
    "\n",
    "logging.info(\"Generating concept unit embeddings...\")\n",
    "concept_unit_embeddings = generate_concept_unit_embeddings(concept_unit_pairs, device=device)\n",
    "# concept_unit_embeddings_2 = generate_concept_unit_embeddings(concept_unit_pairs, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456fe7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Normalize for cosine similarity\n",
    "# A = concept_unit_embeddings_1\n",
    "# B = concept_unit_embeddings_2\n",
    "\n",
    "# A_norm = A / np.linalg.norm(A, axis=1, keepdims=True)\n",
    "# B_norm = B / np.linalg.norm(B, axis=1, keepdims=True)\n",
    "\n",
    "# # Cosine similarity per row\n",
    "# cos_sim = np.sum(A_norm * B_norm, axis=1)\n",
    "\n",
    "# # Report\n",
    "# print(f\"Cosine similarity:\")\n",
    "# print(f\"  Mean: {cos_sim.mean():.8f}\")\n",
    "# print(f\"  Min:  {cos_sim.min():.8f}\")\n",
    "# print(f\"  Std:  {cos_sim.std():.8f}\")\n",
    "\n",
    "# # Optional: show rows below threshold\n",
    "# threshold = 0.999\n",
    "# bad_indices = np.where(cos_sim < threshold)[0]\n",
    "# print(f\"\\nðŸ”» Below {threshold}: {len(bad_indices)} / {len(cos_sim)} rows\")\n",
    "# if len(bad_indices):\n",
    "#     for idx in bad_indices[:10]:\n",
    "#         print(f\"  Row {idx}: cosine = {cos_sim[idx]:.8f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concept_unit_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31d01ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pytorch.narrative_stack.stage1.preprocessing.plots import plot_pca_explanation\n",
    "\n",
    "dim = plot_pca_explanation(concept_unit_embeddings, variance_threshold=0.95)\n",
    "\n",
    "display(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ff5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pytorch.narrative_stack.stage1.preprocessing import pca_compress_concept_unit_embeddings\n",
    "\n",
    "# TODO: Capture PCA, and store the instance directly instead of refitting\n",
    "pca_compressed_concept_unit_embeddings, _ = pca_compress_concept_unit_embeddings(concept_unit_embeddings, n_components=243)\n",
    "# pca_compressed_concept_unit_embeddings_2, _ = pca_compress_concept_unit_embeddings(concept_unit_embeddings_2, n_components=243)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e187ffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Run embedding twice\n",
    "# # embeddings_3 = generate_concept_unit_embeddings(concept_unit_pairs, device=device)\n",
    "# # embeddings_4 = generate_concept_unit_embeddings(concept_unit_pairs, device=device)\n",
    "\n",
    "# # assert embeddings_1.shape == embeddings_2.shape, \"Shape mismatch\"\n",
    "\n",
    "# # Normalize for cosine similarity\n",
    "# A = pca_compressed_concept_unit_embeddings\n",
    "# B = pca_compressed_concept_unit_embeddings_2\n",
    "\n",
    "# A_norm = A / np.linalg.norm(A, axis=1, keepdims=True)\n",
    "# B_norm = B / np.linalg.norm(B, axis=1, keepdims=True)\n",
    "\n",
    "# # Cosine similarity per row\n",
    "# cos_sim = np.sum(A_norm * B_norm, axis=1)\n",
    "\n",
    "# # Report\n",
    "# print(f\"Cosine similarity:\")\n",
    "# print(f\"  Mean: {cos_sim.mean():.8f}\")\n",
    "# print(f\"  Min:  {cos_sim.min():.8f}\")\n",
    "# print(f\"  Std:  {cos_sim.std():.8f}\")\n",
    "\n",
    "# # Optional: show rows below threshold\n",
    "# threshold = 0.999\n",
    "# bad_indices = np.where(cos_sim < threshold)[0]\n",
    "# print(f\"\\nðŸ”» Below {threshold}: {len(bad_indices)} / {len(cos_sim)} rows\")\n",
    "# if len(bad_indices):\n",
    "#     for idx in bad_indices[:10]:\n",
    "#         print(f\"  Row {idx}: cosine = {cos_sim[idx]:.8f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8218043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pytorch.narrative_stack.stage1.preprocessing.plots import plot_semantic_embeddings\n",
    "\n",
    "plot_semantic_embeddings(pca_compressed_concept_unit_embeddings, title=\"PCA Semantic Embedding Scatterplot\")\n",
    "plot_semantic_embeddings(concept_unit_embeddings, title=\"Raw Semantic Embedding Scatterplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30496feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_compressed_concept_unit_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe275055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# TODO: Add types\n",
    "def save_concept_unit_value_tuples(pca_compressed_concept_unit_embeddings, concept_unit_pairs, concept_unit_value_tuples, file_path):\n",
    "    assert len(pca_compressed_concept_unit_embeddings) == len(concept_unit_pairs), \\\n",
    "        f\"Mismatch: {len(pca_compressed_concept_unit_embeddings)} embeddings vs {len(concept_unit_pairs)} keys\"\n",
    "\n",
    "    # Save both embeddings and tuples\n",
    "    np.savez_compressed(\n",
    "        file_path,\n",
    "        keys=np.array([f\"{c}::{u}\" for c, u in concept_unit_pairs]),\n",
    "        embeddings=pca_compressed_concept_unit_embeddings,\n",
    "        concept_unit_value_tuples=np.array(concept_unit_value_tuples, dtype=object)\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Saved {len(concept_unit_value_tuples):,} tuples and {len(pca_compressed_concept_unit_embeddings):,} embeddings to '{file_path}'\")\n",
    "\n",
    "\n",
    "save_concept_unit_value_tuples(\n",
    "    pca_compressed_concept_unit_embeddings,\n",
    "    concept_unit_pairs,\n",
    "    extracted_concept_unit_value_data.concept_unit_value_tuples,\n",
    "    \"data/stage1_latents.npz\"\n",
    ")\n",
    "\n",
    "# save_concept_unit_value_tuples(\n",
    "#     pca_compressed_concept_unit_embeddings_2,\n",
    "#     concept_unit_pairs,\n",
    "#     extracted_concept_unit_value_data.concept_unit_value_tuples,\n",
    "#     \"data/stage1_latents_new_2.npz\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c33383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Load from disk\n",
    "# new_data = np.load(\"data/stage1_latents_new.npz\", allow_pickle=True)\n",
    "# new_concept_unit_value_tuples = new_data[\"concept_unit_value_tuples\"].tolist()\n",
    "# new_embeddings = new_data[\"embeddings\"]\n",
    "\n",
    "# # Check shape match\n",
    "# assert len(pca_compressed_concept_unit_embeddings) == len(new_embeddings), \\\n",
    "#     \"Mismatch in embedding row counts\"\n",
    "\n",
    "# # Cosine similarity check\n",
    "# def cosine_similarity(a, b):\n",
    "#     a = a / np.linalg.norm(a)\n",
    "#     b = b / np.linalg.norm(b)\n",
    "#     return np.dot(a, b)\n",
    "\n",
    "# cos_sims = []\n",
    "# for a_vec, b_vec in zip(pca_compressed_concept_unit_embeddings, new_embeddings):\n",
    "#     sim = cosine_similarity(a_vec.astype(np.float64), b_vec.astype(np.float64))\n",
    "#     cos_sims.append(sim)\n",
    "\n",
    "# # Report\n",
    "# cos_sims = np.array(cos_sims)\n",
    "# print(f\"âœ… Compared {len(cos_sims)} rows\")\n",
    "# print(f\"ðŸ”¹ Mean cosine similarity: {cos_sims.mean():.8f}\")\n",
    "# print(f\"ðŸ”¹ Min cosine similarity:  {cos_sims.min():.8f}\")\n",
    "# print(f\"ðŸ”¹ Std dev:                {cos_sims.std():.8f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af223c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Load saved latent data\n",
    "# old_data = np.load(\"data/stage1_latents.npz\", allow_pickle=True)\n",
    "\n",
    "# # Build embedding map\n",
    "# embedding_map = {\n",
    "#     tuple(key.split(\"::\", 1)): vec\n",
    "#     for key, vec in zip(old_data[\"keys\"], old_data[\"embeddings\"])\n",
    "# }\n",
    "\n",
    "# # Load concept-unit-value tuples\n",
    "# old_concept_unit_value_tuples = old_data[\"concept_unit_value_tuples\"].tolist()\n",
    "\n",
    "# # Load saved latent data\n",
    "# new_data = np.load(\"data/stage1_latents_new.npz\", allow_pickle=True)\n",
    "\n",
    "# # Build embedding map\n",
    "# embedding_map = {\n",
    "#     tuple(key.split(\"::\", 1)): vec\n",
    "#     for key, vec in zip(new_data[\"keys\"], new_data[\"embeddings\"])\n",
    "# }\n",
    "\n",
    "# # Load concept-unit-value tuples\n",
    "# new_concept_unit_value_tuples = new_data[\"concept_unit_value_tuples\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d25157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from hashlib import sha256\n",
    "\n",
    "# # a = np.load(\"data/stage1_latents.npz\", allow_pickle=True)\n",
    "# a = np.load(\"data/stage1_latents_new_1.npz\", allow_pickle=True)\n",
    "# b = np.load(\"data/stage1_latents_new_2.npz\", allow_pickle=True)\n",
    "\n",
    "# # with open(\"data/stage1_latents_new_1.pkl\", \"rb\") as f:\n",
    "# #     a = pickle.load(f)\n",
    "\n",
    "# # with open(\"data/stage1_latents_new_2.pkl\", \"rb\") as f:\n",
    "# #     b = pickle.load(f)\n",
    "\n",
    "# def hash_array(arr):\n",
    "#     return sha256(np.ascontiguousarray(arr)).hexdigest()\n",
    "\n",
    "# print(\"Hash a[keys]:\", hash_array(a[\"keys\"]))\n",
    "# print(\"Hash b[keys]:\", hash_array(b[\"keys\"]))\n",
    "\n",
    "# for k in a.files:\n",
    "# # for k in a.keys():\n",
    "#     print(f\"Checking: {k}\")\n",
    "#     if k == \"embeddings\":\n",
    "#         A = a[k].astype(np.float32)\n",
    "#         B = b[k].astype(np.float32)\n",
    "#         assert A.shape == B.shape, \"Shape mismatch in embeddings\"\n",
    "\n",
    "#         # Normalize to unit vectors\n",
    "#         A_norm = A / np.linalg.norm(A, axis=1, keepdims=True)\n",
    "#         B_norm = B / np.linalg.norm(B, axis=1, keepdims=True)\n",
    "\n",
    "#         # Cosine similarity\n",
    "#         cos_sim = np.sum(A_norm * B_norm, axis=1)\n",
    "#         mean_sim = np.mean(cos_sim)\n",
    "#         min_sim = np.min(cos_sim)\n",
    "\n",
    "#         print(f\"Mean cosine similarity: {mean_sim:.8f}\")\n",
    "#         print(f\"Min cosine similarity:  {min_sim:.8f}\")\n",
    "#         print(f\"Std cosine similarity:  {cos_sim.std():.8f}\")\n",
    "#         assert min_sim > 0.999, f\"Cosine similarity too low in embeddings: {min_sim}\"\n",
    "#     else:\n",
    "#         assert np.array_equal(a[k], b[k]), f\"Mismatch in {k}\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
