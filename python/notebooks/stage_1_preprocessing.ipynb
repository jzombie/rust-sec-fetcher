{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa69b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# This snippet ensures consistent import paths across environments.\n",
    "# When running notebooks via JupyterLab's web UI, the current working\n",
    "# directory is often different (e.g., /notebooks) compared to VS Code,\n",
    "# which typically starts at the project root. This handles that by \n",
    "# retrying the import after changing to the parent directory.\n",
    "# \n",
    "# Include this at the top of every notebook to standardize imports\n",
    "# across development environments.\n",
    "\n",
    "try:\n",
    "    from utils.os import chdir_to_git_root\n",
    "except ModuleNotFoundError:\n",
    "    os.chdir(Path.cwd().parent)\n",
    "    print(f\"Retrying import from: {os.getcwd()}\")\n",
    "    from utils.os import chdir_to_git_root\n",
    "\n",
    "chdir_to_git_root(\"python\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982f0e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# from models.pytorch.narrative_stack.stage1.preprocessing import generate_concept_unit_embeddings, generate_concepts_report\n",
    "from db import DbUsGaap\n",
    "\n",
    "db_us_gaap = DbUsGaap()\n",
    "data_dir = \"../data/june-us-gaap\" # Where CSV data is read from (once CSV file per symbol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc8bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: For debugging / monitoring purposes only\n",
    "# Determine \"category stack\" depths\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from utils.csv import walk_us_gaap_csvs, get_filtered_us_gaap_form_rows_for_symbol\n",
    "\n",
    "class RunningStats:\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.total = 0\n",
    "        self.max_val = 0\n",
    "        self.values = []\n",
    "\n",
    "    def update(self, val: int):\n",
    "        self.count += 1\n",
    "        self.total += val\n",
    "        self.max_val = max(self.max_val, val)\n",
    "        self.values.append(val)  # Optional: remove this if median not needed\n",
    "\n",
    "    def finalize(self):\n",
    "        result = {\n",
    "            \"avg\": self.total / self.count if self.count else 0,\n",
    "            \"max\": self.max_val,\n",
    "        }\n",
    "        if self.values:\n",
    "            result[\"median\"] = float(np.median(self.values))\n",
    "        return result\n",
    "\n",
    "# Initialize running stats per key\n",
    "stats = defaultdict(RunningStats)\n",
    "\n",
    "gen = walk_us_gaap_csvs(data_dir, db_us_gaap, \"row\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        row = next(gen)\n",
    "        counter = defaultdict(int)\n",
    "        for entry in row.entries:\n",
    "            key = (entry.balance_type or \"none\", entry.period_type or \"none\")\n",
    "            counter[key] += 1\n",
    "        for key, val in counter.items():\n",
    "            stats[key].update(val)\n",
    "except StopIteration:\n",
    "    pass\n",
    "\n",
    "# Final summary\n",
    "summary = {key: stat.finalize() for key, stat in stats.items()}\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0891b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: For debugging / monitoring purposes only\n",
    "\n",
    "import numpy as np\n",
    "from utils.csv import walk_us_gaap_csvs, UsGaapRowRecord\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def generate_concepts_report_from_walker(\n",
    "    data_dir: Path,\n",
    "    db_us_gaap: DbUsGaap,\n",
    "    filtered_symbols: set[str] | None = None,\n",
    "):\n",
    "    gen = walk_us_gaap_csvs(\n",
    "        data_dir=data_dir,\n",
    "        db_us_gaap=db_us_gaap,\n",
    "        walk_type=\"row\",\n",
    "        filtered_symbols=filtered_symbols,\n",
    "    )\n",
    "\n",
    "    unit_stats = defaultdict(list)\n",
    "    concept_by_unit = defaultdict(set)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            row = next(gen)\n",
    "            if isinstance(row, UsGaapRowRecord):\n",
    "                for entry in row.entries:\n",
    "                    unit_stats[entry.uom].append(entry.value)\n",
    "                    concept_by_unit[entry.uom].add(entry.concept)\n",
    "    except StopIteration as stop:\n",
    "        summary = stop.value\n",
    "\n",
    "    print(f\"\\nâœ… Scanned {len(summary.csv_files)} files.\")\n",
    "    print(\n",
    "        f\"ðŸ“¦ Found {len(unit_stats)} numeric units and \"\n",
    "        f\"{len(summary.non_numeric_units)} non-numeric units.\"\n",
    "    )\n",
    "\n",
    "    for unit, values in sorted(unit_stats.items()):\n",
    "        arr = np.array(values)\n",
    "        print(f\"ðŸ”¹ {unit}\")\n",
    "        print(f\"   Count: {len(arr)}\")\n",
    "        print(f\"   Min:   {arr.min():,.4f}\")\n",
    "        print(f\"   Max:   {arr.max():,.4f}\")\n",
    "        print(f\"   Mean:  {arr.mean():,.4f}\")\n",
    "        print(f\"   Std:   {arr.std():,.4f}\")\n",
    "        print(f\"   Concepts: {', '.join(sorted(concept_by_unit[unit]))}\")\n",
    "\n",
    "    if summary.non_numeric_units:\n",
    "        print(\"\\nâš ï¸ Non-numeric units encountered:\")\n",
    "        for unit in sorted(summary.non_numeric_units):\n",
    "            print(f\"  - {unit}\")\n",
    "\n",
    "    total_values = sum(len(v) for v in unit_stats.values())\n",
    "    print(f\"\\nðŸ§® Total values extracted: {total_values:,}\")\n",
    "\n",
    "\n",
    "generate_concepts_report_from_walker(data_dir, db_us_gaap, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0438cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simd_r_drive import DataStore, NamespaceHasher\n",
    "from collections import defaultdict\n",
    "from pydantic import BaseModel\n",
    "import msgpack\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from utils.csv import walk_us_gaap_csvs\n",
    "\n",
    "# Open data store\n",
    "store = DataStore(\"proto.bin\")\n",
    "# store = DataStore(\"/Volumes/Expansion/proto.bin\")\n",
    "\n",
    "# Define immutable concept/unit pair model\n",
    "class ConceptUnitPair(BaseModel):\n",
    "    concept: str\n",
    "    uom: str\n",
    "\n",
    "    class Config:\n",
    "        # Enables hashing\n",
    "        frozen = True\n",
    "\n",
    "# Namespaces for storing structured data\n",
    "TRIPLET_REVERSE_INDEX_NAMESPACE = NamespaceHasher(b\"triplet-reverse-index\")\n",
    "UNSCALED_SEQUENTIAL_CELL_NAMESPACE = NamespaceHasher(b\"unscaled-sequential-cell\")\n",
    "SCALED_SEQUENTIAL_CELL_NAMESPACE = NamespaceHasher(b\"scaled-sequential-cell\")\n",
    "CELL_META_NAMESPACE = NamespaceHasher(b\"cell-meta\")\n",
    "CONCEPT_UNIT_PAIR_NAMESPACE = NamespaceHasher(b\"concept-unit-pair\")\n",
    "\n",
    "# Initialize CSV stream generator\n",
    "gen = walk_us_gaap_csvs(data_dir, db_us_gaap, \"row\")\n",
    "\n",
    "# Track per (concept, uom) the list of i_cell indices that use it\n",
    "concept_unit_pairs_i_cells: dict[ConceptUnitPair, list[int]] = defaultdict(list)\n",
    "pair_to_id: dict[ConceptUnitPair, int] = {}\n",
    "concept_unit_entries: list[tuple[bytes, bytes]] = []\n",
    "\n",
    "# Global sequential index for each cell value\n",
    "i_cell = -1\n",
    "next_pair_id = 0\n",
    "\n",
    "# Stream and store data\n",
    "try:\n",
    "    while True:\n",
    "        row = next(gen)\n",
    "        batch = []\n",
    "\n",
    "        for cell in row.entries:\n",
    "            i_cell += 1\n",
    "\n",
    "            pair = ConceptUnitPair(concept=cell.concept, uom=cell.uom)\n",
    "            i_bytes = i_cell.to_bytes(4, \"little\", signed=False)\n",
    "\n",
    "            # Assign ID to concept/unit pair if not already done\n",
    "            if pair not in pair_to_id:\n",
    "                pair_to_id[pair] = next_pair_id\n",
    "                pair_id_bytes = next_pair_id.to_bytes(4, \"little\", signed=False)\n",
    "                pair_key = CONCEPT_UNIT_PAIR_NAMESPACE.namespace(pair_id_bytes)\n",
    "                pair_val = msgpack.packb((pair.concept, pair.uom))\n",
    "                concept_unit_entries.append((pair_key, pair_val))\n",
    "                next_pair_id += 1\n",
    "\n",
    "            pair_id = pair_to_id[pair]\n",
    "            pair_id_bytes = pair_id.to_bytes(4, \"little\", signed=False)\n",
    "\n",
    "            # Track cell indices per (concept, uom)\n",
    "            concept_unit_pairs_i_cells[pair].append(i_cell)\n",
    "\n",
    "            # Store raw unscaled value\n",
    "            value_bytes = msgpack.packb(cell.value)\n",
    "            unscaled_key = UNSCALED_SEQUENTIAL_CELL_NAMESPACE.namespace(i_bytes)\n",
    "            batch.append((unscaled_key, value_bytes))\n",
    "\n",
    "            # Store reverse triplet â†’ i_cell mapping\n",
    "            triplet_bytes = msgpack.packb((cell.concept, cell.uom, cell.value))\n",
    "            triplet_key = TRIPLET_REVERSE_INDEX_NAMESPACE.namespace(triplet_bytes)\n",
    "            batch.append((triplet_key, i_bytes))\n",
    "\n",
    "            # Store cell meta (i_cell â†’ concept_unit_id)\n",
    "            cell_meta_key = CELL_META_NAMESPACE.namespace(i_bytes)\n",
    "            batch.append((cell_meta_key, pair_id_bytes))\n",
    "\n",
    "        # Write current batch of entries to store\n",
    "        store.batch_write(batch)\n",
    "\n",
    "        # TODO: Comment-out\n",
    "        # Optional cutoff for debugging\n",
    "        # if i_cell > 1000:\n",
    "        #     break\n",
    "\n",
    "except StopIteration as stop:\n",
    "    summary = stop.value\n",
    "    display(summary)\n",
    "\n",
    "total_triplets = i_cell + 1\n",
    "\n",
    "store.write(\n",
    "    b\"__triplet_count__\",\n",
    "    total_triplets.to_bytes(4, byteorder=\"little\", signed=False),\n",
    ")\n",
    "\n",
    "print(f\"Total triplets: {total_triplets}\")\n",
    "\n",
    "# Persist concept_unit_id â†’ (concept, uom) mapping\n",
    "store.batch_write(concept_unit_entries)\n",
    "\n",
    "total_pairs = len(concept_unit_pairs_i_cells)\n",
    "\n",
    "store.write(b\"__pair_count__\", total_pairs.to_bytes(4, byteorder=\"little\", signed=False))\n",
    "\n",
    "# Show number of unique concept/unit pairs\n",
    "print(f\"Total concept/unit pairs: {total_pairs}\")\n",
    "\n",
    "# Show binary keys for each concept/unit pair\n",
    "for pair in tqdm(concept_unit_pairs_i_cells, desc=\"Tracking concept/unit pairs\"):\n",
    "    print(msgpack.packb((pair.concept, pair.uom)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7071dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scale all values per concept/unit group\n",
    "for pair, i_cells in tqdm(concept_unit_pairs_i_cells.items(), desc=\"Scaling per concept/unit\"):\n",
    "    i_bytes_list = [i.to_bytes(4, \"little\", signed=False) for i in i_cells]\n",
    "    keys = [UNSCALED_SEQUENTIAL_CELL_NAMESPACE.namespace(i_bytes) for i_bytes in i_bytes_list]\n",
    "\n",
    "    values = [\n",
    "        msgpack.unpackb(store.read(key), raw=True)\n",
    "        for key in keys\n",
    "    ]\n",
    "\n",
    "    vals_np = np.array(values).reshape(-1, 1)\n",
    "\n",
    "    # Clamp quantiles based on sample size\n",
    "    n_q = min(len(values), 1000)\n",
    "    if n_q < 2 and len(values) >= 2:\n",
    "        n_q = 2\n",
    "\n",
    "    if len(values) < 2:\n",
    "        # Skip scaling for singleton values â€” nothing to transform\n",
    "        continue\n",
    "\n",
    "    scaler = QuantileTransformer(\n",
    "        output_distribution=\"normal\",\n",
    "        n_quantiles=n_q,\n",
    "        subsample=len(values),\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    scaled_vals = scaler.fit_transform(vals_np).flatten()\n",
    "\n",
    "    assert len(scaled_vals) == len(i_cells)\n",
    "\n",
    "    store.batch_write([\n",
    "        (\n",
    "            SCALED_SEQUENTIAL_CELL_NAMESPACE.namespace(i.to_bytes(4, \"little\", signed=False)),\n",
    "            msgpack.packb(val)\n",
    "        )\n",
    "        for i, val in zip(i_cells, scaled_vals)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85fb6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, NamedTuple, Tuple\n",
    "\n",
    "# TODO: Migrate to common type\n",
    "class ConceptUnitPair(NamedTuple):\n",
    "    concept: str\n",
    "    uom: str\n",
    "\n",
    "\n",
    "def iterate_concept_unit_pairs(\n",
    "    store: DataStore\n",
    ") -> Iterator[Tuple[int, ConceptUnitPair]]:\n",
    "    \"\"\"\n",
    "    Yields (pair_id, ConceptUnitPair) from the concept/unit pair namespace.\n",
    "    \"\"\"\n",
    "    raw = store.read(b\"__pair_count__\")\n",
    "    if raw is None:\n",
    "        raise ValueError(\"Missing __pair_count__ key in store\")\n",
    "\n",
    "    total_pairs = int.from_bytes(raw, \"little\", signed=False)\n",
    "\n",
    "    for pair_id in range(total_pairs):\n",
    "        key = CONCEPT_UNIT_PAIR_NAMESPACE.namespace(\n",
    "            pair_id.to_bytes(4, \"little\", signed=False)\n",
    "        )\n",
    "        val = store.read(key)\n",
    "        if val is None:\n",
    "            raise KeyError(f\"Missing concept/unit for pair_id={pair_id}\")\n",
    "        concept, uom = msgpack.unpackb(val, raw=True)\n",
    "        yield pair_id, ConceptUnitPair(\n",
    "            concept=concept.decode(\"utf-8\"),\n",
    "            uom=uom.decode(\"utf-8\")\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6cbd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, Tuple\n",
    "import torch\n",
    "import numpy as np\n",
    "import logging\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "from utils.pytorch import seed_everything, model_hash\n",
    "from utils import generate_us_gaap_description\n",
    "from simd_r_drive import DataStore\n",
    "# from your_model_defs import ConceptUnitPair  # Replace with actual import\n",
    "# from your_store_iterator import iterate_concept_unit_pairs  # Replace with actual import\n",
    "\n",
    "\n",
    "def generate_concept_unit_embeddings(\n",
    "    store: DataStore,\n",
    "    device: torch.device,\n",
    "    batch_size: int = 64,\n",
    ") -> Iterator[Tuple[int, ConceptUnitPair, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Yields (pair_id, concept_unit_pair, embedding) for each input concept/unit pair.\n",
    "    \"\"\"\n",
    "    pairs_iter = iterate_concept_unit_pairs(store)\n",
    "\n",
    "    model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    logging.info(f\"Embedding model hash: {model_hash(model)}\")\n",
    "\n",
    "    buffer_ids = []\n",
    "    buffer_pairs = []\n",
    "    buffer_texts = []\n",
    "\n",
    "    for pair_id, pair in pairs_iter:\n",
    "        text = f\"{generate_us_gaap_description(pair.concept)} measured in {pair.uom}\"\n",
    "        buffer_ids.append(pair_id)\n",
    "        buffer_pairs.append(pair)\n",
    "        buffer_texts.append(text)\n",
    "\n",
    "        if len(buffer_pairs) == batch_size:\n",
    "            yield from _embed_batch(buffer_ids, buffer_pairs, buffer_texts, model, device)\n",
    "            buffer_ids.clear()\n",
    "            buffer_pairs.clear()\n",
    "            buffer_texts.clear()\n",
    "\n",
    "    if buffer_pairs:\n",
    "        yield from _embed_batch(buffer_ids, buffer_pairs, buffer_texts, model, device)\n",
    "\n",
    "\n",
    "def _embed_batch(pair_ids, pairs, texts, model, device):\n",
    "    tokens = model.tokenize(texts)\n",
    "    tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "    with torch.no_grad():\n",
    "        output = model.forward(tokens)\n",
    "        embeddings = output[\"sentence_embedding\"].cpu().numpy()\n",
    "    for pair_id, pair, embedding in zip(pair_ids, pairs, embeddings):\n",
    "        yield pair_id, pair, embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af651bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pytorch import get_device\n",
    "from models.pytorch.narrative_stack.stage1.preprocessing import pca_compress_concept_unit_embeddings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Cache embeddings in RAM and apply PCA\n",
    "\n",
    "# TODO: Move these declarations\n",
    "PCA_MODEL_NAMESPACE = NamespaceHasher(b\"pca-model\")\n",
    "PCA_REDUCED_EMBEDDING_NAMESPACE = NamespaceHasher(b\"pca-reduced-embedding\")\n",
    "\n",
    "\n",
    "pairs = []\n",
    "embeddings = []\n",
    "\n",
    "for pair_id, pair, embedding in tqdm(generate_concept_unit_embeddings(store, get_device()), desc=\"Generating Semantic Embeddings\"):\n",
    "    pairs.append((pair_id, pair))\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "\n",
    "# Convert to NumPy array of shape (N, D)\n",
    "embedding_matrix = np.stack(embeddings, axis=0)\n",
    "\n",
    "# Now ready to pass `embedding_matrix` to PCA\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "\n",
    "# TODO: Reuse PCA if already existing (provide ability to pull from another store, etc.)\n",
    "\n",
    "pca_compressed_concept_unit_embeddings, pca = pca_compress_concept_unit_embeddings(embedding_matrix, n_components=234, pca=None, stable=True)\n",
    "\n",
    "assert len(pairs) == len(pca_compressed_concept_unit_embeddings)\n",
    "\n",
    "\n",
    "# TODO: Save PCA-reduced embeddings in store\n",
    "pca_embedding_entries = [\n",
    "    (\n",
    "        PCA_REDUCED_EMBEDDING_NAMESPACE.namespace(\n",
    "            pair_id.to_bytes(4, \"little\", signed=False)\n",
    "        ),\n",
    "        msgpack.packb(vec.astype(np.float32).tolist())  # Convert numpy array to list\n",
    "    )\n",
    "    for (pair_id, _), vec in zip(pairs, pca_compressed_concept_unit_embeddings)\n",
    "]\n",
    "\n",
    "store.batch_write(pca_embedding_entries)\n",
    "print(f\"Wrote {len(pca_embedding_entries)} PCA-compressed embeddings to store.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9660ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from io import BytesIO\n",
    "\n",
    "# Serialize PCA model into a byte stream\n",
    "pca_model_stream = BytesIO()\n",
    "joblib.dump(pca, pca_model_stream)\n",
    "pca_model_stream.seek(0)  # Move cursor to the beginning of the stream\n",
    "\n",
    "# Store PCA model in the DataStore\n",
    "store.write(PCA_MODEL_NAMESPACE.namespace(b\"model\"), pca_model_stream.read())\n",
    "\n",
    "print(\"Stored PCA model in store.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2d026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pytorch.narrative_stack.stage1.preprocessing.plots import plot_pca_explanation\n",
    "\n",
    "dim = plot_pca_explanation(embedding_matrix, variance_threshold=0.95)\n",
    "\n",
    "display(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e51a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplet_count(store: DataStore) -> int:\n",
    "    raw = store.read(b\"__triplet_count__\")\n",
    "    if raw is None:\n",
    "        raise KeyError(\"Triplet count key not found\")\n",
    "    return int.from_bytes(raw, \"little\", signed=False)\n",
    "\n",
    "def get_pair_count(store: DataStore) -> int:\n",
    "    raw = store.read(b\"__pair_count__\")\n",
    "    if raw is None:\n",
    "        raise KeyError(\"Pair count key not found\")\n",
    "    return int.from_bytes(raw, \"little\", signed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54740471",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(get_triplet_count(store))\n",
    "\n",
    "display(get_pair_count(store))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c534ce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_by_triplet(\n",
    "    store: DataStore,\n",
    "    concept: str,\n",
    "    uom: str,\n",
    "    unscaled_value: float\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Given a (concept, uom, value) triplet, return its i_cell, unscaled value,\n",
    "    and scaled value if available.\n",
    "\n",
    "    Returns a dict with keys: i_cell, unscaled_value, scaled_value\n",
    "    \"\"\"\n",
    "    # Encode the triplet as used in reverse index\n",
    "    triplet_key_bytes = msgpack.packb((concept, uom, unscaled_value))\n",
    "    triplet_key = TRIPLET_REVERSE_INDEX_NAMESPACE.namespace(triplet_key_bytes)\n",
    "\n",
    "    # Lookup i_cell\n",
    "    i_cell_bytes = store.read(triplet_key)\n",
    "    if i_cell_bytes is None:\n",
    "        raise KeyError(f\"Triplet ({concept}, {uom}, {unscaled_value}) not found in reverse index\")\n",
    "\n",
    "    i_cell = int.from_bytes(i_cell_bytes, \"little\", signed=False)\n",
    "\n",
    "    # Construct unscaled key\n",
    "    # unscaled_key = UNSCALED_SEQUENTIAL_CELL_NAMESPACE.namespace(\n",
    "    #     i_cell.to_bytes(4, \"little\", signed=False)\n",
    "    # )\n",
    "    # unscaled_value_bytes = store.read(unscaled_key)\n",
    "    # unscaled_value = msgpack.unpackb(unscaled_value_bytes, raw=True)\n",
    "\n",
    "    # Construct scaled key\n",
    "    scaled_key = SCALED_SEQUENTIAL_CELL_NAMESPACE.namespace(\n",
    "        i_cell.to_bytes(4, \"little\", signed=False)\n",
    "    )\n",
    "    scaled_value_bytes = store.read(scaled_key)\n",
    "    scaled_value = (\n",
    "        msgpack.unpackb(scaled_value_bytes, raw=True)\n",
    "        if scaled_value_bytes is not None\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"i_cell\": i_cell,\n",
    "        \"unscaled_value\": unscaled_value,\n",
    "        \"scaled_value\": scaled_value\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dbc0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_by_triplet(store, \"AccountsReceivableNetCurrent\", \"USD\", 1324000000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa385e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_by_index(store: DataStore, i_cell: int) -> dict:\n",
    "    \"\"\"\n",
    "    Look up the concept, uom, unscaled value, and scaled value for a given i_cell.\n",
    "\n",
    "    Returns:\n",
    "        dict with keys: i_cell, concept, uom, unscaled_value, scaled_value\n",
    "\n",
    "    Raises:\n",
    "        KeyError if any required value is missing.\n",
    "    \"\"\"\n",
    "    i_bytes = i_cell.to_bytes(4, \"little\", signed=False)\n",
    "\n",
    "    # Load concept_unit_id from cell meta\n",
    "    meta_key = CELL_META_NAMESPACE.namespace(i_bytes)\n",
    "    concept_unit_id_bytes = store.read(meta_key)\n",
    "    if concept_unit_id_bytes is None:\n",
    "        raise KeyError(f\"Missing concept_unit_id for i_cell {i_cell}\")\n",
    "\n",
    "    concept_unit_id = int.from_bytes(concept_unit_id_bytes, \"little\", signed=False)\n",
    "\n",
    "    # Load (concept, uom) from concept_unit_id\n",
    "    pair_key = CONCEPT_UNIT_PAIR_NAMESPACE.namespace(\n",
    "        concept_unit_id.to_bytes(4, \"little\", signed=False)\n",
    "    )\n",
    "    pair_bytes = store.read(pair_key)\n",
    "    if pair_bytes is None:\n",
    "        raise KeyError(f\"Missing (concept, uom) for concept_unit_id {concept_unit_id}\")\n",
    "\n",
    "    concept, uom = msgpack.unpackb(pair_bytes, raw=False)\n",
    "\n",
    "    # Load unscaled value\n",
    "    unscaled_key = UNSCALED_SEQUENTIAL_CELL_NAMESPACE.namespace(i_bytes)\n",
    "    unscaled_bytes = store.read(unscaled_key)\n",
    "    if unscaled_bytes is None:\n",
    "        raise KeyError(f\"Missing unscaled value for i_cell {i_cell}\")\n",
    "    unscaled_value = msgpack.unpackb(unscaled_bytes, raw=True)\n",
    "\n",
    "    # Load scaled value (optional)\n",
    "    scaled_key = SCALED_SEQUENTIAL_CELL_NAMESPACE.namespace(i_bytes)\n",
    "    scaled_bytes = store.read(scaled_key)\n",
    "    scaled_value = (\n",
    "        msgpack.unpackb(scaled_bytes, raw=True) if scaled_bytes is not None else None\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"i_cell\": i_cell,\n",
    "        \"concept\": concept,\n",
    "        \"uom\": uom,\n",
    "        \"unscaled_value\": unscaled_value,\n",
    "        \"scaled_value\": scaled_value\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6306e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, 10):\n",
    "#     display(lookup_by_index(store, i * 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5679eb30",
   "metadata": {},
   "source": [
    "# TODO: Continue prototyping from here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024539ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For deterministic hashing (TODO: Move to tests)\n",
    "\n",
    "# import hashlib\n",
    "# import pickle\n",
    "\n",
    "# # def hash_extracted_data(data: ExtractedConceptUnitValueData) -> str:\n",
    "# def hash_extracted_data(data) -> str:\n",
    "#     \"\"\"\n",
    "#     Computes a SHA-256 hash of the full extracted concept/unit/value data structure.\n",
    "#     This includes tuples, unit stats, and file list â€” all serialized deterministically.\n",
    "#     \"\"\"\n",
    "#     # Serialize using protocol=5 (highest and deterministic in modern Python)\n",
    "#     serialized = pickle.dumps(data.dict(), protocol=5)\n",
    "#     return hashlib.sha256(serialized).hexdigest()\n",
    "\n",
    "# hash_extracted_data(extracted_concept_unit_value_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb9fb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Fetching...\")\n",
    "# extracted_concept_unit_value_data_2 = extract_concept_unit_value_tuples(data_dir, valid_concepts)\n",
    "\n",
    "# print(\"Hashing...\")\n",
    "# hash_extracted_data(extracted_concept_unit_value_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a530ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View concepts report (not needed for preprocessing but contains useful information)\n",
    "\n",
    "generate_concepts_report(extracted_concept_unit_value_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb05ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pytorch import get_device, seed_everything\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "logging.info(\"Collecting concept unit pairs...\")\n",
    "concept_unit_pairs = collect_concept_unit_pairs(extracted_concept_unit_value_data)\n",
    "\n",
    "logging.info(f\"Total concept unit pairs: {len(concept_unit_pairs)}\")\n",
    "\n",
    "logging.info(\"Generating concept unit embeddings...\")\n",
    "concept_unit_embeddings = generate_concept_unit_embeddings(concept_unit_pairs, device=device)\n",
    "# concept_unit_embeddings_2 = generate_concept_unit_embeddings(concept_unit_pairs, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456fe7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Normalize for cosine similarity\n",
    "# A = concept_unit_embeddings_1\n",
    "# B = concept_unit_embeddings_2\n",
    "\n",
    "# A_norm = A / np.linalg.norm(A, axis=1, keepdims=True)\n",
    "# B_norm = B / np.linalg.norm(B, axis=1, keepdims=True)\n",
    "\n",
    "# # Cosine similarity per row\n",
    "# cos_sim = np.sum(A_norm * B_norm, axis=1)\n",
    "\n",
    "# # Report\n",
    "# print(f\"Cosine similarity:\")\n",
    "# print(f\"  Mean: {cos_sim.mean():.8f}\")\n",
    "# print(f\"  Min:  {cos_sim.min():.8f}\")\n",
    "# print(f\"  Std:  {cos_sim.std():.8f}\")\n",
    "\n",
    "# # Optional: show rows below threshold\n",
    "# threshold = 0.999\n",
    "# bad_indices = np.where(cos_sim < threshold)[0]\n",
    "# print(f\"\\nðŸ”» Below {threshold}: {len(bad_indices)} / {len(cos_sim)} rows\")\n",
    "# if len(bad_indices):\n",
    "#     for idx in bad_indices[:10]:\n",
    "#         print(f\"  Row {idx}: cosine = {cos_sim[idx]:.8f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concept_unit_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31d01ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pytorch.narrative_stack.stage1.preprocessing.plots import plot_pca_explanation\n",
    "\n",
    "dim = plot_pca_explanation(concept_unit_embeddings, variance_threshold=0.95)\n",
    "\n",
    "display(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43904af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ff5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pytorch.narrative_stack.stage1.preprocessing import pca_compress_concept_unit_embeddings\n",
    "\n",
    "\n",
    "pca_compressed_concept_unit_embeddings, pca = pca_compress_concept_unit_embeddings(concept_unit_embeddings, n_components=243)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c0946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Prototype this\n",
    "\n",
    "# import joblib\n",
    "# import io\n",
    "\n",
    "# # `pca` is the fitted PCA object\n",
    "# buffer = io.BytesIO()\n",
    "# joblib.dump(pca, buffer)\n",
    "# pca_bytes = buffer.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8218043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pytorch.narrative_stack.stage1.preprocessing.plots import plot_semantic_embeddings\n",
    "\n",
    "plot_semantic_embeddings(pca_compressed_concept_unit_embeddings, title=\"PCA Semantic Embedding Scatterplot\")\n",
    "plot_semantic_embeddings(concept_unit_embeddings, title=\"Raw Semantic Embedding Scatterplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30496feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_compressed_concept_unit_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe275055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# TODO: Add types\n",
    "def save_concept_unit_value_tuples(pca_compressed_concept_unit_embeddings, concept_unit_pairs, concept_unit_value_tuples, file_path):\n",
    "    assert len(pca_compressed_concept_unit_embeddings) == len(concept_unit_pairs), \\\n",
    "        f\"Mismatch: {len(pca_compressed_concept_unit_embeddings)} embeddings vs {len(concept_unit_pairs)} keys\"\n",
    "\n",
    "    # Save both embeddings and tuples\n",
    "    np.savez_compressed(\n",
    "        file_path,\n",
    "        keys=np.array([f\"{c}::{u}\" for c, u in concept_unit_pairs]),\n",
    "        embeddings=pca_compressed_concept_unit_embeddings,\n",
    "        concept_unit_value_tuples=np.array(concept_unit_value_tuples, dtype=object)\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Saved {len(concept_unit_value_tuples):,} tuples and {len(pca_compressed_concept_unit_embeddings):,} embeddings to '{file_path}'\")\n",
    "\n",
    "\n",
    "save_concept_unit_value_tuples(\n",
    "    pca_compressed_concept_unit_embeddings,\n",
    "    concept_unit_pairs,\n",
    "    extracted_concept_unit_value_data.concept_unit_value_tuples,\n",
    "    \"data/stage1_latents.npz\" # TODO: Rename! These are not latent vectors!\n",
    ")\n",
    "\n",
    "# save_concept_unit_value_tuples(\n",
    "#     pca_compressed_concept_unit_embeddings_2,\n",
    "#     concept_unit_pairs,\n",
    "#     extracted_concept_unit_value_data.concept_unit_value_tuples,\n",
    "#     \"data/stage1_latents_new_2.npz\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c33383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Validate subsequent\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# # Load from disk\n",
    "# new_data = np.load(\"data/stage1_latents_new.npz\", allow_pickle=True)\n",
    "# new_concept_unit_value_tuples = new_data[\"concept_unit_value_tuples\"].tolist()\n",
    "# new_embeddings = new_data[\"embeddings\"]\n",
    "\n",
    "# # Check shape match\n",
    "# assert len(pca_compressed_concept_unit_embeddings) == len(new_embeddings), \\\n",
    "#     \"Mismatch in embedding row counts\"\n",
    "\n",
    "# # Cosine similarity check\n",
    "# def cosine_similarity(a, b):\n",
    "#     a = a / np.linalg.norm(a)\n",
    "#     b = b / np.linalg.norm(b)\n",
    "#     return np.dot(a, b)\n",
    "\n",
    "# cos_sims = []\n",
    "# for a_vec, b_vec in zip(pca_compressed_concept_unit_embeddings, new_embeddings):\n",
    "#     sim = cosine_similarity(a_vec.astype(np.float64), b_vec.astype(np.float64))\n",
    "#     cos_sims.append(sim)\n",
    "\n",
    "# # Report\n",
    "# cos_sims = np.array(cos_sims)\n",
    "# print(f\"âœ… Compared {len(cos_sims)} rows\")\n",
    "# print(f\"ðŸ”¹ Mean cosine similarity: {cos_sims.mean():.8f}\")\n",
    "# print(f\"ðŸ”¹ Min cosine similarity:  {cos_sims.min():.8f}\")\n",
    "# print(f\"ðŸ”¹ Std dev:                {cos_sims.std():.8f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af223c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Load saved latent data\n",
    "# old_data = np.load(\"data/stage1_latents.npz\", allow_pickle=True)\n",
    "\n",
    "# # Build embedding map\n",
    "# embedding_map = {\n",
    "#     tuple(key.split(\"::\", 1)): vec\n",
    "#     for key, vec in zip(old_data[\"keys\"], old_data[\"embeddings\"])\n",
    "# }\n",
    "\n",
    "# # Load concept-unit-value tuples\n",
    "# old_concept_unit_value_tuples = old_data[\"concept_unit_value_tuples\"].tolist()\n",
    "\n",
    "# # Load saved latent data\n",
    "# new_data = np.load(\"data/stage1_latents_new.npz\", allow_pickle=True)\n",
    "\n",
    "# # Build embedding map\n",
    "# embedding_map = {\n",
    "#     tuple(key.split(\"::\", 1)): vec\n",
    "#     for key, vec in zip(new_data[\"keys\"], new_data[\"embeddings\"])\n",
    "# }\n",
    "\n",
    "# # Load concept-unit-value tuples\n",
    "# new_concept_unit_value_tuples = new_data[\"concept_unit_value_tuples\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d25157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from hashlib import sha256\n",
    "\n",
    "# # a = np.load(\"data/stage1_latents.npz\", allow_pickle=True)\n",
    "# a = np.load(\"data/stage1_latents_new_1.npz\", allow_pickle=True)\n",
    "# b = np.load(\"data/stage1_latents_new_2.npz\", allow_pickle=True)\n",
    "\n",
    "# # with open(\"data/stage1_latents_new_1.pkl\", \"rb\") as f:\n",
    "# #     a = pickle.load(f)\n",
    "\n",
    "# # with open(\"data/stage1_latents_new_2.pkl\", \"rb\") as f:\n",
    "# #     b = pickle.load(f)\n",
    "\n",
    "# def hash_array(arr):\n",
    "#     return sha256(np.ascontiguousarray(arr)).hexdigest()\n",
    "\n",
    "# print(\"Hash a[keys]:\", hash_array(a[\"keys\"]))\n",
    "# print(\"Hash b[keys]:\", hash_array(b[\"keys\"]))\n",
    "\n",
    "# for k in a.files:\n",
    "# # for k in a.keys():\n",
    "#     print(f\"Checking: {k}\")\n",
    "#     if k == \"embeddings\":\n",
    "#         A = a[k].astype(np.float32)\n",
    "#         B = b[k].astype(np.float32)\n",
    "#         assert A.shape == B.shape, \"Shape mismatch in embeddings\"\n",
    "\n",
    "#         # Normalize to unit vectors\n",
    "#         A_norm = A / np.linalg.norm(A, axis=1, keepdims=True)\n",
    "#         B_norm = B / np.linalg.norm(B, axis=1, keepdims=True)\n",
    "\n",
    "#         # Cosine similarity\n",
    "#         cos_sim = np.sum(A_norm * B_norm, axis=1)\n",
    "#         mean_sim = np.mean(cos_sim)\n",
    "#         min_sim = np.min(cos_sim)\n",
    "\n",
    "#         print(f\"Mean cosine similarity: {mean_sim:.8f}\")\n",
    "#         print(f\"Min cosine similarity:  {min_sim:.8f}\")\n",
    "#         print(f\"Std cosine similarity:  {cos_sim.std():.8f}\")\n",
    "#         assert min_sim > 0.999, f\"Cosine similarity too low in embeddings: {min_sim}\"\n",
    "#     else:\n",
    "#         assert np.array_equal(a[k], b[k]), f\"Mismatch in {k}\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
