{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the starting directory\n",
    "start_dir = \"../data/fund-holdings\"\n",
    "\n",
    "# Count CSV files\n",
    "csv_count = sum(\n",
    "    len(files) for root, _, files in os.walk(start_dir) if any(f.endswith(\".csv\") for f in files)\n",
    ")\n",
    "\n",
    "print(f\"Total CSV files found: {csv_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine Unmapped Fund CIK Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the starting directory\n",
    "start_dir = \"../data/fund-holdings\"\n",
    "\n",
    "# List to store unique entries without mapped company CIK number\n",
    "unique_entries = []\n",
    "\n",
    "# Initialize counter for iteration\n",
    "file_count = 0\n",
    "\n",
    "# Iterate through CSV files\n",
    "for root, _, files in os.walk(start_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            df = pd.read_csv(file_path, dtype=str)  # Read CSV as string to avoid conversion issues\n",
    "            \n",
    "            # Filter rows where mapped_company_cik_number is NaN or empty\n",
    "            filtered_df = df[df[\"mapped_company_cik_number\"].fillna(\"\").str.strip() == \"\"]\n",
    "\n",
    "            # Append unique rows to the list\n",
    "            unique_entries.extend(filtered_df.drop_duplicates().to_dict(orient=\"records\"))\n",
    "            \n",
    "            # Increment file counter and print progress\n",
    "            file_count += 1\n",
    "            print(f\"Processed {file_count} files...\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "unique_df = pd.DataFrame(unique_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df.to_csv(\"unmapped.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine Most Common (and currently maintained) US-GAAP Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the directory to search for CSV files\n",
    "start_dir = \"../data/orig.us-gaap\"\n",
    "\n",
    "# Dictionary to store column reporting frequency per form type\n",
    "column_distribution = defaultdict(lambda: {\"10-K\": 0, \"10-Q\": 0, \"latest_filed\": 0})\n",
    "\n",
    "# Track the number of processed files\n",
    "file_count = 0\n",
    "\n",
    "# Define the minimum year threshold\n",
    "current_year = pd.Timestamp.now().year\n",
    "min_year = current_year - 4  # Consider only filings within the last 4 years\n",
    "\n",
    "# Iterate through all CSV files in the directory\n",
    "for root, _, files in os.walk(start_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            try:\n",
    "                # Read CSV file with dtype=str to prevent automatic type conversion\n",
    "                df = pd.read_csv(file_path, dtype=str)\n",
    "\n",
    "                # Ensure required columns exist\n",
    "                if \"form\" not in df.columns or \"filed\" not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                # Convert \"filed\" column to numeric (year only)\n",
    "                df[\"filed\"] = pd.to_datetime(df[\"filed\"], errors=\"coerce\").dt.year\n",
    "\n",
    "                # Filter out entries older than min_year\n",
    "                df = df[df[\"filed\"] >= min_year]\n",
    "\n",
    "                if df.empty:\n",
    "                    continue\n",
    "\n",
    "                # Track the most recent year for each column\n",
    "                latest_filing_year = df[\"filed\"].max()\n",
    "\n",
    "                # Count occurrences of each column by form type\n",
    "                for column in df.columns:\n",
    "                    form_counts = df[\"form\"].value_counts()\n",
    "                    for form_type in [\"10-K\", \"10-Q\"]:\n",
    "                        if form_type in form_counts:\n",
    "                            column_distribution[column][form_type] += form_counts[form_type]\n",
    "\n",
    "                    # Update latest filing year\n",
    "                    if latest_filing_year:\n",
    "                        column_distribution[column][\"latest_filed\"] = max(\n",
    "                            column_distribution[column][\"latest_filed\"], latest_filing_year\n",
    "                        )\n",
    "\n",
    "                # Increment processed file counter\n",
    "                file_count += 1\n",
    "                if file_count % 10 == 0:\n",
    "                    print(f\"Processed {file_count} files...\")\n",
    "\n",
    "                # TODO: Remove\n",
    "                # if file_count > 500:\n",
    "                #     break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df_distribution = pd.DataFrame.from_dict(column_distribution, orient=\"index\")\n",
    "\n",
    "# Filter columns that haven't been reported in the last 4 years\n",
    "df_distribution = df_distribution[df_distribution[\"latest_filed\"] >= min_year]\n",
    "\n",
    "# Sort by most frequently reported in \"10-K\" and \"10-Q\"\n",
    "df_distribution.sort_values(by=[\"10-K\", \"10-Q\"], ascending=False, inplace=True)\n",
    "\n",
    "####\n",
    "\n",
    "# Compute total number of unique 10-K and 10-Q documents (not their sum)\n",
    "total_10k_docs = df_distribution[\"10-K\"].count()\n",
    "total_10q_docs = df_distribution[\"10-Q\"].count()\n",
    "\n",
    "# Compute percentage for each row based on unique document count\n",
    "df_distribution[\"10-K %\"] = df_distribution[\"10-K\"] / total_10k_docs\n",
    "df_distribution[\"10-Q %\"] = df_distribution[\"10-Q\"] / total_10q_docs\n",
    "\n",
    "# Normalize percentages so that the highest value is 100%\n",
    "df_distribution[\"10-K %\"] = (df_distribution[\"10-K %\"] / df_distribution[\"10-K %\"].max()) * 100\n",
    "df_distribution[\"10-Q %\"] = (df_distribution[\"10-Q %\"] / df_distribution[\"10-Q %\"].max()) * 100\n",
    "\n",
    "def classify_statement_type(column_name):\n",
    "    \"\"\"Assigns a financial statement type to a given column name using hierarchical classification.\"\"\"\n",
    "    best_match = (\"Unclassified\", 0, None)  # (Statement, Length, Category)\n",
    "\n",
    "    STATEMENT_TYPES = {\n",
    "        \"Balance Sheet\": {\n",
    "            \"Current Assets\": [\n",
    "                \"CashAndCashEquivalents\", \"MarketableSecurities\", \"AccountsReceivable\",\n",
    "                \"Inventory\", \"PrepaidExpenses\", \"LoansReceivable\", \"FairValueMeasurement\",\n",
    "                \"CustomerLoyaltyProgram\", \"FrequentFlierLiability\"\n",
    "            ],\n",
    "            \"Non-Current Assets\": [\n",
    "                \"PropertyPlantEquipment\", \"Goodwill\", \"IntangibleAssets\",\n",
    "                \"DeferredTaxAssets\", \"LongTermInvestments\", \"LiabilityForAsbestos\",\n",
    "                \"DeferredTaxLiabilities\", \"AmortizationOfIntangibleAssets\"\n",
    "            ],\n",
    "            \"Liabilities & Equity\": [\n",
    "                \"AccountsPayable\", \"AccruedLiabilities\", \"DeferredRevenue\",\n",
    "                \"ShortTermDebt\", \"LongTermDebt\", \"Equity\", \"RetainedEarnings\",\n",
    "                \"OperatingLeaseLiability\", \"PreferredStock\", \"UnrecognizedTaxBenefits\",\n",
    "                \"AdditionalPaidInCapital\"\n",
    "            ]\n",
    "        },\n",
    "        \"Income Statement\": {\n",
    "            \"Revenue\": [\n",
    "                \"Revenue\", \"Sales\", \"ServiceRevenue\", \"InterestAndFeeIncome\",\n",
    "                \"SecScheduleSupplementalInformation\"\n",
    "            ],\n",
    "            \"Expenses\": [\n",
    "                \"CostOfRevenue\", \"OperatingExpenses\", \"InterestExpense\", \"Depreciation\",\n",
    "                \"Amortization\", \"SellingGeneralAdministrative\", \"LossOnDisposal\",\n",
    "                \"ForeignCurrencyExchangeLoss\", \"StockBasedCompensationExpense\",\n",
    "                \"AdjustmentsForIncomeTaxExpense\", \"AdjustmentsForAmortizationExpense\",\n",
    "                \"OtherFinanceIncomeCost\", \"OperatingLeaseCost\"\n",
    "            ],\n",
    "            \"Profit & Loss\": [\n",
    "                \"GrossProfit\", \"NetIncome\", \"Loss\", \"ComprehensiveIncome\",\n",
    "                \"ComprehensiveIncomeNetOfTax\", \"ProfitLoss\"\n",
    "            ],\n",
    "            \"Cash-Related Adjustments\": [\n",
    "                \"InterestIncome\", \"InterestExpense\", \"Depreciation\",\n",
    "                \"Amortization\", \"StockBasedCompensation\"\n",
    "            ]\n",
    "        },\n",
    "        \"Cash Flow Statement\": {\n",
    "            \"Operating Activities\": [\n",
    "                \"CashFlowsFromOperatingActivities\", \"IncreaseInInventory\", \"Depreciation\",\n",
    "                \"StockBasedCompensation\", \"AccountsReceivableChanges\", \"OperatingActivities\",\n",
    "                \"NoncashAcquisition\", \"ReorganizationValueCash\", \"OperatingLeasePayments\"\n",
    "            ],\n",
    "            \"Investing Activities\": [\n",
    "                \"CapitalExpenditures\", \"Investments\", \"PurchaseOfProperty\", \"SaleOfInvestments\",\n",
    "                \"ProceedsFromIssuanceOfSecurities\", \"PaymentsForInvestment\"\n",
    "            ],\n",
    "            \"Financing Activities\": [\n",
    "                \"DebtIssuance\", \"StockRepurchase\", \"DividendsPaid\", \"ProceedsFromStockOptions\",\n",
    "                \"RepaymentOfLongTermDebt\", \"LiabilitiesArisingFromFinancingActivities\",\n",
    "                \"PaymentsForRepurchaseOfStock\"\n",
    "            ],\n",
    "            \"Other Cash Flow Items\": [\n",
    "                \"ForeignCurrencyExchangeEffects\", \"InterestPaid\", \"IncomeTaxesPaid\",\n",
    "                \"NonCashItems\", \"ChangesInWorkingCapital\", \"PaymentsForRestrictedCash\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for statement, categories in STATEMENT_TYPES.items():\n",
    "        for category, keywords in categories.items():\n",
    "            for keyword in sorted(keywords, key=len, reverse=True):  # Prioritize longest match\n",
    "                if keyword in column_name and len(keyword) > best_match[1]:\n",
    "                    best_match = (statement, len(keyword), category)\n",
    "\n",
    "    return best_match[0], best_match[2]  # Returns (Statement Type, Category)\n",
    "\n",
    "\n",
    "# Apply classification to each column\n",
    "df_distribution[\"Statement Type\"] = df_distribution.index.map(classify_statement_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distribution.to_csv(\"column_distribution.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Field: inventory\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processed Terms</th>\n",
       "      <th>Statement</th>\n",
       "      <th>Main Category</th>\n",
       "      <th>Subcategory</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Balance Sheet Assets Total Inventory inventori...</td>\n",
       "      <td>Balance Sheet</td>\n",
       "      <td>Assets</td>\n",
       "      <td>Total Inventory</td>\n",
       "      <td>[[tensor(-0.7997), tensor(0.1299), tensor(-0.2...</td>\n",
       "      <td>0.789605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Balance Sheet Assets Total Receivables, Net re...</td>\n",
       "      <td>Balance Sheet</td>\n",
       "      <td>Assets</td>\n",
       "      <td>Total Receivables, Net</td>\n",
       "      <td>[[tensor(-0.5908), tensor(0.1530), tensor(0.17...</td>\n",
       "      <td>0.707444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Balance Sheet Assets Accounts Receivable - Tra...</td>\n",
       "      <td>Balance Sheet</td>\n",
       "      <td>Assets</td>\n",
       "      <td>Accounts Receivable - Trade, Net</td>\n",
       "      <td>[[tensor(-0.3655), tensor(0.1514), tensor(0.21...</td>\n",
       "      <td>0.693635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Processed Terms      Statement  \\\n",
       "3  Balance Sheet Assets Total Inventory inventori...  Balance Sheet   \n",
       "2  Balance Sheet Assets Total Receivables, Net re...  Balance Sheet   \n",
       "1  Balance Sheet Assets Accounts Receivable - Tra...  Balance Sheet   \n",
       "\n",
       "  Main Category                       Subcategory  \\\n",
       "3        Assets                   Total Inventory   \n",
       "2        Assets            Total Receivables, Net   \n",
       "1        Assets  Accounts Receivable - Trade, Net   \n",
       "\n",
       "                                           Embedding  Similarity  \n",
       "3  [[tensor(-0.7997), tensor(0.1299), tensor(-0.2...    0.789605  \n",
       "2  [[tensor(-0.5908), tensor(0.1530), tensor(0.17...    0.707444  \n",
       "1  [[tensor(-0.3655), tensor(0.1514), tensor(0.21...    0.693635  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load better embedding model optimized for semantic similarity\n",
    "model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Category structure (maintained as provided)\n",
    "STATEMENT_TYPES = {\n",
    "    \"Balance Sheet\": {\n",
    "        \"Assets\": {\n",
    "            \"Cash and Short Term Assets\": [\n",
    "                \"Cash\", \"Cash & Equivalents\", \"Short Term Investments\"\n",
    "            ],\n",
    "            \"Accounts Receivable - Trade, Net\": [\"Accounts Receivable - Trade, Gross\"],\n",
    "            \"Total Receivables, Net\": [\"Receivables, Other\"],\n",
    "            \"Total Inventory\": [\"Inventories - Finished Goods\", \"Inventories - Work in Progress\"],\n",
    "            \"Prepaid Expenses\": [],\n",
    "            \"Other Currrent Assets, Total\": [\"Restricted Cash - Current\", \"Other Current Assets\"]\n",
    "        },\n",
    "        \"Property/Plant/Equipment, Total - Gross\": [\n",
    "            \"Buildings - Gross\", \"Land & Improvements - Gross\",\n",
    "            \"Machinery & Equipment - Gross\", \"Other Property/Plant/Equipment - Gross\"\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Function to generate embeddings with pooling\n",
    "def get_embedding(text):\n",
    "    \"\"\"Generates embeddings using mean pooling from a BGE model.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "    masked_embeddings = embeddings * attention_mask\n",
    "    summed = masked_embeddings.sum(1)\n",
    "    counted = attention_mask.sum(1)\n",
    "    return summed / counted\n",
    "\n",
    "# Prepare category embeddings\n",
    "def prepare_category_embeddings():\n",
    "    match_terms = []\n",
    "    for statement, categories in STATEMENT_TYPES.items():\n",
    "        for category, subcategories in categories.items():\n",
    "            if isinstance(subcategories, dict):\n",
    "                for subcategory, terms in subcategories.items():\n",
    "                    combined_terms = f\"{statement} {category} {subcategory} \" + \" \".join(terms).lower()\n",
    "                    match_terms.append((combined_terms, statement, category, subcategory))\n",
    "            else:\n",
    "                combined_terms = f\"{statement} {category} \" + \" \".join(subcategories).lower()\n",
    "                match_terms.append((combined_terms, statement, category, \"\"))\n",
    "\n",
    "    match_df = pd.DataFrame(match_terms, columns=[\"Processed Terms\", \"Statement\", \"Main Category\", \"Subcategory\"])\n",
    "    match_df[\"Embedding\"] = match_df[\"Processed Terms\"].apply(get_embedding)\n",
    "    return match_df\n",
    "\n",
    "# Function to classify US-GAAP field names using embeddings\n",
    "def classify_us_gaap_field(field_name, match_df, top_n=3):\n",
    "    processed_field = re.sub(r'(?<!^)(?=[A-Z])', ' ', field_name).lower()\n",
    "    field_embedding = get_embedding(processed_field)\n",
    "\n",
    "    print(f\"Processed Field: {processed_field}\")\n",
    "\n",
    "    match_df[\"Similarity\"] = match_df[\"Embedding\"].apply(\n",
    "        lambda emb: F.cosine_similarity(field_embedding, emb).item()\n",
    "    )\n",
    "    return match_df.sort_values(by=\"Similarity\", ascending=False).head(top_n)\n",
    "\n",
    "# Example usage\n",
    "match_df = prepare_category_embeddings()\n",
    "us_gaap_field = \"Inventory\"\n",
    "top_matches = classify_us_gaap_field(us_gaap_field, match_df)\n",
    "\n",
    "# Display the results\n",
    "# top_matches[[\"Statement\", \"Main Category\", \"Subcategory\", \"Similarity\"]]\n",
    "top_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US-GAAP 2025 Hierarchy\n",
    "\n",
    "https://www.sec.gov/data-research/standard-taxonomies/operating-companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
