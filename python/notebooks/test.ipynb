{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the starting directory\n",
    "start_dir = \"../data/fund-holdings\"\n",
    "\n",
    "# Count CSV files\n",
    "csv_count = sum(\n",
    "    len(files) for root, _, files in os.walk(start_dir) if any(f.endswith(\".csv\") for f in files)\n",
    ")\n",
    "\n",
    "print(f\"Total CSV files found: {csv_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine Unmapped Fund CIK Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the starting directory\n",
    "start_dir = \"../data/fund-holdings\"\n",
    "\n",
    "# List to store unique entries without mapped company CIK number\n",
    "unique_entries = []\n",
    "\n",
    "# Initialize counter for iteration\n",
    "file_count = 0\n",
    "\n",
    "# Iterate through CSV files\n",
    "for root, _, files in os.walk(start_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            df = pd.read_csv(file_path, dtype=str)  # Read CSV as string to avoid conversion issues\n",
    "            \n",
    "            # Filter rows where mapped_company_cik_number is NaN or empty\n",
    "            filtered_df = df[df[\"mapped_company_cik_number\"].fillna(\"\").str.strip() == \"\"]\n",
    "\n",
    "            # Append unique rows to the list\n",
    "            unique_entries.extend(filtered_df.drop_duplicates().to_dict(orient=\"records\"))\n",
    "            \n",
    "            # Increment file counter and print progress\n",
    "            file_count += 1\n",
    "            print(f\"Processed {file_count} files...\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "unique_df = pd.DataFrame(unique_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df.to_csv(\"unmapped.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine Most Common (and currently maintained) US-GAAP Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the directory to search for CSV files\n",
    "start_dir = \"../data/orig.us-gaap\"\n",
    "\n",
    "# Dictionary to store column reporting frequency per form type\n",
    "column_distribution = defaultdict(lambda: {\"10-K\": 0, \"10-Q\": 0, \"latest_filed\": 0})\n",
    "\n",
    "# Track the number of processed files\n",
    "file_count = 0\n",
    "\n",
    "# Define the minimum year threshold\n",
    "current_year = pd.Timestamp.now().year\n",
    "min_year = current_year - 4  # Consider only filings within the last 4 years\n",
    "\n",
    "# Iterate through all CSV files in the directory\n",
    "for root, _, files in os.walk(start_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            try:\n",
    "                # Read CSV file with dtype=str to prevent automatic type conversion\n",
    "                df = pd.read_csv(file_path, dtype=str)\n",
    "\n",
    "                # Ensure required columns exist\n",
    "                if \"form\" not in df.columns or \"filed\" not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                # Convert \"filed\" column to numeric (year only)\n",
    "                df[\"filed\"] = pd.to_datetime(df[\"filed\"], errors=\"coerce\").dt.year\n",
    "\n",
    "                # Filter out entries older than min_year\n",
    "                df = df[df[\"filed\"] >= min_year]\n",
    "\n",
    "                if df.empty:\n",
    "                    continue\n",
    "\n",
    "                # Track the most recent year for each column\n",
    "                latest_filing_year = df[\"filed\"].max()\n",
    "\n",
    "                # Count occurrences of each column by form type\n",
    "                for column in df.columns:\n",
    "                    form_counts = df[\"form\"].value_counts()\n",
    "                    for form_type in [\"10-K\", \"10-Q\"]:\n",
    "                        if form_type in form_counts:\n",
    "                            column_distribution[column][form_type] += form_counts[form_type]\n",
    "\n",
    "                    # Update latest filing year\n",
    "                    if latest_filing_year:\n",
    "                        column_distribution[column][\"latest_filed\"] = max(\n",
    "                            column_distribution[column][\"latest_filed\"], latest_filing_year\n",
    "                        )\n",
    "\n",
    "                # Increment processed file counter\n",
    "                file_count += 1\n",
    "                if file_count % 10 == 0:\n",
    "                    print(f\"Processed {file_count} files...\")\n",
    "\n",
    "                # TODO: Remove\n",
    "                # if file_count > 500:\n",
    "                #     break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df_distribution = pd.DataFrame.from_dict(column_distribution, orient=\"index\")\n",
    "\n",
    "# Filter columns that haven't been reported in the last 4 years\n",
    "df_distribution = df_distribution[df_distribution[\"latest_filed\"] >= min_year]\n",
    "\n",
    "# Sort by most frequently reported in \"10-K\" and \"10-Q\"\n",
    "df_distribution.sort_values(by=[\"10-K\", \"10-Q\"], ascending=False, inplace=True)\n",
    "\n",
    "####\n",
    "\n",
    "# Compute total number of unique 10-K and 10-Q documents (not their sum)\n",
    "total_10k_docs = df_distribution[\"10-K\"].count()\n",
    "total_10q_docs = df_distribution[\"10-Q\"].count()\n",
    "\n",
    "# Compute percentage for each row based on unique document count\n",
    "df_distribution[\"10-K %\"] = df_distribution[\"10-K\"] / total_10k_docs\n",
    "df_distribution[\"10-Q %\"] = df_distribution[\"10-Q\"] / total_10q_docs\n",
    "\n",
    "# Normalize percentages so that the highest value is 100%\n",
    "df_distribution[\"10-K %\"] = (df_distribution[\"10-K %\"] / df_distribution[\"10-K %\"].max()) * 100\n",
    "df_distribution[\"10-Q %\"] = (df_distribution[\"10-Q %\"] / df_distribution[\"10-Q %\"].max()) * 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distribution.to_csv(\"column_distribution.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
