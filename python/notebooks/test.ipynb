{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the starting directory\n",
    "start_dir = \"../data/fund-holdings\"\n",
    "\n",
    "# Count CSV files\n",
    "csv_count = sum(\n",
    "    len(files) for root, _, files in os.walk(start_dir) if any(f.endswith(\".csv\") for f in files)\n",
    ")\n",
    "\n",
    "print(f\"Total CSV files found: {csv_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine Unmapped Fund CIK Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the starting directory\n",
    "start_dir = \"../data/fund-holdings\"\n",
    "\n",
    "# List to store unique entries without mapped company CIK number\n",
    "unique_entries = []\n",
    "\n",
    "# Initialize counter for iteration\n",
    "file_count = 0\n",
    "\n",
    "# Iterate through CSV files\n",
    "for root, _, files in os.walk(start_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            df = pd.read_csv(file_path, dtype=str)  # Read CSV as string to avoid conversion issues\n",
    "            \n",
    "            # Filter rows where mapped_company_cik_number is NaN or empty\n",
    "            filtered_df = df[df[\"mapped_company_cik_number\"].fillna(\"\").str.strip() == \"\"]\n",
    "\n",
    "            # Append unique rows to the list\n",
    "            unique_entries.extend(filtered_df.drop_duplicates().to_dict(orient=\"records\"))\n",
    "            \n",
    "            # Increment file counter and print progress\n",
    "            file_count += 1\n",
    "            print(f\"Processed {file_count} files...\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "unique_df = pd.DataFrame(unique_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df.to_csv(\"unmapped.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine Most Common (and currently maintained) US-GAAP Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the directory to search for CSV files\n",
    "start_dir = \"../data/orig.us-gaap\"\n",
    "\n",
    "# Dictionary to store column reporting frequency per form type\n",
    "column_distribution = defaultdict(lambda: {\"10-K\": 0, \"10-Q\": 0, \"latest_filed\": 0})\n",
    "\n",
    "# Track the number of processed files\n",
    "file_count = 0\n",
    "\n",
    "# Define the minimum year threshold\n",
    "current_year = pd.Timestamp.now().year\n",
    "min_year = current_year - 4  # Consider only filings within the last 4 years\n",
    "\n",
    "# Iterate through all CSV files in the directory\n",
    "for root, _, files in os.walk(start_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            try:\n",
    "                # Read CSV file with dtype=str to prevent automatic type conversion\n",
    "                df = pd.read_csv(file_path, dtype=str)\n",
    "\n",
    "                # Ensure required columns exist\n",
    "                if \"form\" not in df.columns or \"filed\" not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                # Convert \"filed\" column to numeric (year only)\n",
    "                df[\"filed\"] = pd.to_datetime(df[\"filed\"], errors=\"coerce\").dt.year\n",
    "\n",
    "                # Filter out entries older than min_year\n",
    "                df = df[df[\"filed\"] >= min_year]\n",
    "\n",
    "                if df.empty:\n",
    "                    continue\n",
    "\n",
    "                # Track the most recent year for each column\n",
    "                latest_filing_year = df[\"filed\"].max()\n",
    "\n",
    "                # Count occurrences of each column by form type\n",
    "                for column in df.columns:\n",
    "                    form_counts = df[\"form\"].value_counts()\n",
    "                    for form_type in [\"10-K\", \"10-Q\"]:\n",
    "                        if form_type in form_counts:\n",
    "                            column_distribution[column][form_type] += form_counts[form_type]\n",
    "\n",
    "                    # Update latest filing year\n",
    "                    if latest_filing_year:\n",
    "                        column_distribution[column][\"latest_filed\"] = max(\n",
    "                            column_distribution[column][\"latest_filed\"], latest_filing_year\n",
    "                        )\n",
    "\n",
    "                # Increment processed file counter\n",
    "                file_count += 1\n",
    "                if file_count % 10 == 0:\n",
    "                    print(f\"Processed {file_count} files...\")\n",
    "\n",
    "                # TODO: Remove\n",
    "                # if file_count > 500:\n",
    "                #     break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df_distribution = pd.DataFrame.from_dict(column_distribution, orient=\"index\")\n",
    "\n",
    "# Filter columns that haven't been reported in the last 4 years\n",
    "df_distribution = df_distribution[df_distribution[\"latest_filed\"] >= min_year]\n",
    "\n",
    "# Sort by most frequently reported in \"10-K\" and \"10-Q\"\n",
    "df_distribution.sort_values(by=[\"10-K\", \"10-Q\"], ascending=False, inplace=True)\n",
    "\n",
    "####\n",
    "\n",
    "# Compute total number of unique 10-K and 10-Q documents (not their sum)\n",
    "total_10k_docs = df_distribution[\"10-K\"].count()\n",
    "total_10q_docs = df_distribution[\"10-Q\"].count()\n",
    "\n",
    "# Compute percentage for each row based on unique document count\n",
    "df_distribution[\"10-K %\"] = df_distribution[\"10-K\"] / total_10k_docs\n",
    "df_distribution[\"10-Q %\"] = df_distribution[\"10-Q\"] / total_10q_docs\n",
    "\n",
    "# Normalize percentages so that the highest value is 100%\n",
    "df_distribution[\"10-K %\"] = (df_distribution[\"10-K %\"] / df_distribution[\"10-K %\"].max()) * 100\n",
    "df_distribution[\"10-Q %\"] = (df_distribution[\"10-Q %\"] / df_distribution[\"10-Q %\"].max()) * 100\n",
    "\n",
    "def classify_statement_type(column_name):\n",
    "    \"\"\"Assigns a financial statement type to a given column name using hierarchical classification.\"\"\"\n",
    "    best_match = (\"Unclassified\", 0, None)  # (Statement, Length, Category)\n",
    "\n",
    "    STATEMENT_TYPES = {\n",
    "        \"Balance Sheet\": {\n",
    "            \"Current Assets\": [\n",
    "                \"CashAndCashEquivalents\", \"MarketableSecurities\", \"AccountsReceivable\",\n",
    "                \"Inventory\", \"PrepaidExpenses\", \"LoansReceivable\", \"FairValueMeasurement\",\n",
    "                \"CustomerLoyaltyProgram\", \"FrequentFlierLiability\"\n",
    "            ],\n",
    "            \"Non-Current Assets\": [\n",
    "                \"PropertyPlantEquipment\", \"Goodwill\", \"IntangibleAssets\",\n",
    "                \"DeferredTaxAssets\", \"LongTermInvestments\", \"LiabilityForAsbestos\",\n",
    "                \"DeferredTaxLiabilities\", \"AmortizationOfIntangibleAssets\"\n",
    "            ],\n",
    "            \"Liabilities & Equity\": [\n",
    "                \"AccountsPayable\", \"AccruedLiabilities\", \"DeferredRevenue\",\n",
    "                \"ShortTermDebt\", \"LongTermDebt\", \"Equity\", \"RetainedEarnings\",\n",
    "                \"OperatingLeaseLiability\", \"PreferredStock\", \"UnrecognizedTaxBenefits\",\n",
    "                \"AdditionalPaidInCapital\"\n",
    "            ]\n",
    "        },\n",
    "        \"Income Statement\": {\n",
    "            \"Revenue\": [\n",
    "                \"Revenue\", \"Sales\", \"ServiceRevenue\", \"InterestAndFeeIncome\",\n",
    "                \"SecScheduleSupplementalInformation\"\n",
    "            ],\n",
    "            \"Expenses\": [\n",
    "                \"CostOfRevenue\", \"OperatingExpenses\", \"InterestExpense\", \"Depreciation\",\n",
    "                \"Amortization\", \"SellingGeneralAdministrative\", \"LossOnDisposal\",\n",
    "                \"ForeignCurrencyExchangeLoss\", \"StockBasedCompensationExpense\",\n",
    "                \"AdjustmentsForIncomeTaxExpense\", \"AdjustmentsForAmortizationExpense\",\n",
    "                \"OtherFinanceIncomeCost\", \"OperatingLeaseCost\"\n",
    "            ],\n",
    "            \"Profit & Loss\": [\n",
    "                \"GrossProfit\", \"NetIncome\", \"Loss\", \"ComprehensiveIncome\",\n",
    "                \"ComprehensiveIncomeNetOfTax\", \"ProfitLoss\"\n",
    "            ],\n",
    "            \"Cash-Related Adjustments\": [\n",
    "                \"InterestIncome\", \"InterestExpense\", \"Depreciation\",\n",
    "                \"Amortization\", \"StockBasedCompensation\"\n",
    "            ]\n",
    "        },\n",
    "        \"Cash Flow Statement\": {\n",
    "            \"Operating Activities\": [\n",
    "                \"CashFlowsFromOperatingActivities\", \"IncreaseInInventory\", \"Depreciation\",\n",
    "                \"StockBasedCompensation\", \"AccountsReceivableChanges\", \"OperatingActivities\",\n",
    "                \"NoncashAcquisition\", \"ReorganizationValueCash\", \"OperatingLeasePayments\"\n",
    "            ],\n",
    "            \"Investing Activities\": [\n",
    "                \"CapitalExpenditures\", \"Investments\", \"PurchaseOfProperty\", \"SaleOfInvestments\",\n",
    "                \"ProceedsFromIssuanceOfSecurities\", \"PaymentsForInvestment\"\n",
    "            ],\n",
    "            \"Financing Activities\": [\n",
    "                \"DebtIssuance\", \"StockRepurchase\", \"DividendsPaid\", \"ProceedsFromStockOptions\",\n",
    "                \"RepaymentOfLongTermDebt\", \"LiabilitiesArisingFromFinancingActivities\",\n",
    "                \"PaymentsForRepurchaseOfStock\"\n",
    "            ],\n",
    "            \"Other Cash Flow Items\": [\n",
    "                \"ForeignCurrencyExchangeEffects\", \"InterestPaid\", \"IncomeTaxesPaid\",\n",
    "                \"NonCashItems\", \"ChangesInWorkingCapital\", \"PaymentsForRestrictedCash\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for statement, categories in STATEMENT_TYPES.items():\n",
    "        for category, keywords in categories.items():\n",
    "            for keyword in sorted(keywords, key=len, reverse=True):  # Prioritize longest match\n",
    "                if keyword in column_name and len(keyword) > best_match[1]:\n",
    "                    best_match = (statement, len(keyword), category)\n",
    "\n",
    "    return best_match[0], best_match[2]  # Returns (Statement Type, Category)\n",
    "\n",
    "\n",
    "# Apply classification to each column\n",
    "df_distribution[\"Statement Type\"] = df_distribution.index.map(classify_statement_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distribution.to_csv(\"column_distribution.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load FinBERT model and tokenizer\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Extended category structure (4 levels deep)\n",
    "STATEMENT_TYPES = {\n",
    "    \"Balance Sheet\": {\n",
    "        \"Assets\": {\n",
    "            \"Cash and Short Term Assets\": [\"Cash\", \"Cash & Equivalents\", \"Short Term Investments\"],\n",
    "            \"Accounts Receivable - Trade, Net\": [\"Accounts Receivable - Trade, Gross\"],\n",
    "            \"Total Receivables, Net\": [\"Receivables, Other\"],\n",
    "            \"Total Inventory\": [\"Inventories - Finished Goods\", \"Inventories - Work in Progress\"],\n",
    "            \"Prepaid Expenses\": [],\n",
    "            \"Other Currrent Assets, Total\": [\"Restricted Cash - Current\", \"Other Current Assets\"]\n",
    "        },\n",
    "        \"Property/Plant/Equipment, Total - Gross\": [\"Buildings - Gross\", \"Land & Improvements - Gross\", \"Machinery & Equipment - Gross\", \"Other Property/Plant/Equipment - Gross\"],\n",
    "        # \"Liabilities & Equity\": {\n",
    "        #     \"Current Liabilities\": [\"Accounts Payable\", \"Short Term Debt\"],\n",
    "        #     \"Non-Current Liabilities\": [\"Long Term Debt\", \"Deferred Tax Liabilities\"]\n",
    "        # }\n",
    "    },\n",
    "    # \"Income Statement\": {\n",
    "    #     \"Revenue\": [\"Sales\", \"Service Revenue\", \"Interest Income\"],\n",
    "    #     \"Expenses\": {\n",
    "    #         \"Cost of Revenue\": [\"Direct Costs\", \"COGS\"],\n",
    "    #         \"Operating Expenses\": [\"Selling, General and Administrative\", \"R&D\"]\n",
    "    #     },\n",
    "    #     \"Profit & Loss\": [\"Gross Profit\", \"Net Income\"]\n",
    "    # },\n",
    "    # \"Cash Flow Statement\": {\n",
    "    #     \"Operating Activities\": [\"Cash Received from Customers\", \"Interest Paid\"],\n",
    "    #     \"Investing Activities\": [\"Capital Expenditures\", \"Investments\"],\n",
    "    #     \"Financing Activities\": [\"Debt Issuance\", \"Stock Repurchase\"]\n",
    "    # }\n",
    "}\n",
    "\n",
    "# Function to get embeddings from FinBERT\n",
    "def get_embedding(text):\n",
    "    \"\"\"Generates an embedding for the given text using FinBERT.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]  # Use CLS token embedding\n",
    "\n",
    "# Prepare category embeddings (combine terms for each category)\n",
    "def prepare_category_embeddings():\n",
    "    match_terms = []\n",
    "    for statement, categories in STATEMENT_TYPES.items():\n",
    "        for category, subcategories in categories.items():\n",
    "            # TODO: I'm not sure if including statement and category is beneficial or is deteriorating the performance\n",
    "            if isinstance(subcategories, dict):  # If it has further subcategories\n",
    "                for subcategory, terms in subcategories.items():\n",
    "                    # Combine the category and subcategory with the terms\n",
    "                    combined_terms = f\"{statement} {category} {subcategory} \" + \" \".join(terms).lower()\n",
    "                    match_terms.append((combined_terms, statement, category, subcategory))\n",
    "            else:\n",
    "                # If no subcategories, just combine the category name and terms\n",
    "                combined_terms = f\"{statement} {category} \" + \" \".join(subcategories).lower()\n",
    "                match_terms.append((combined_terms, statement, category, \"\"))\n",
    "    \n",
    "    # Print to verify\n",
    "    print(match_terms)\n",
    "    \n",
    "    # Create DataFrame with all combined terms\n",
    "    match_df = pd.DataFrame(match_terms, columns=[\"Processed Terms\", \"Statement\", \"Main Category\", \"Subcategory\"])\n",
    "    \n",
    "    # Generate embeddings for each combined category of terms\n",
    "    match_df[\"Embedding\"] = match_df[\"Processed Terms\"].apply(lambda x: get_embedding(x))\n",
    "    \n",
    "    return match_df\n",
    "\n",
    "\n",
    "# Function to classify US-GAAP field names using FinBERT embeddings\n",
    "def classify_us_gaap_field(field_name, match_df, top_n=3):\n",
    "    \"\"\"Classifies a US-GAAP field using FinBERT embeddings.\"\"\"\n",
    "    # Preprocess input (split camel case)\n",
    "    processed_field = re.sub(r'(?<!^)(?=[A-Z])', ' ', field_name).lower()\n",
    "\n",
    "    print(processed_field)\n",
    "    \n",
    "    # Generate embedding for input field\n",
    "    field_embedding = get_embedding(processed_field)\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    similarities = [F.cosine_similarity(field_embedding, emb).item() for emb in match_df[\"Embedding\"]]\n",
    "    \n",
    "    # Rank and return top matches\n",
    "    match_df[\"Similarity\"] = similarities\n",
    "    return match_df.sort_values(by=\"Similarity\", ascending=False).head(top_n)\n",
    "\n",
    "# Example usage\n",
    "match_df = prepare_category_embeddings()  # Precompute category embeddings\n",
    "us_gaap_field = \"PropertyPlantEquipment\"\n",
    "top_matches = classify_us_gaap_field(us_gaap_field, match_df)\n",
    "\n",
    "# Display the results\n",
    "top_matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US-GAAP 2025 Hierarchy\n",
    "\n",
    "https://www.sec.gov/data-research/standard-taxonomies/operating-companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
