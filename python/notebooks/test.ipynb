{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the starting directory\n",
    "start_dir = \"../data/fund-holdings\"\n",
    "\n",
    "# Count CSV files\n",
    "csv_count = sum(\n",
    "    len(files) for root, _, files in os.walk(start_dir) if any(f.endswith(\".csv\") for f in files)\n",
    ")\n",
    "\n",
    "print(f\"Total CSV files found: {csv_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine Unmapped Fund CIK Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the starting directory\n",
    "start_dir = \"../data/fund-holdings\"\n",
    "\n",
    "# List to store unique entries without mapped company CIK number\n",
    "unique_entries = []\n",
    "\n",
    "# Initialize counter for iteration\n",
    "file_count = 0\n",
    "\n",
    "# Iterate through CSV files\n",
    "for root, _, files in os.walk(start_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            df = pd.read_csv(file_path, dtype=str)  # Read CSV as string to avoid conversion issues\n",
    "            \n",
    "            # Filter rows where mapped_company_cik_number is NaN or empty\n",
    "            filtered_df = df[df[\"mapped_company_cik_number\"].fillna(\"\").str.strip() == \"\"]\n",
    "\n",
    "            # Append unique rows to the list\n",
    "            unique_entries.extend(filtered_df.drop_duplicates().to_dict(orient=\"records\"))\n",
    "            \n",
    "            # Increment file counter and print progress\n",
    "            file_count += 1\n",
    "            print(f\"Processed {file_count} files...\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "unique_df = pd.DataFrame(unique_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df.to_csv(\"unmapped.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine Most Common (and currently maintained) US-GAAP Columns\n",
    "\n",
    "See also: https://xbrlview.fasb.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())  # True if MPS is available\n",
    "print(torch.backends.mps.is_built())      # True if PyTorch is built with MPS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Load better embedding model optimized for semantic similarity\n",
    "model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Category structure (maintained as provided)\n",
    "STATEMENT_TYPES = {\n",
    "    \"Balance Sheet\": {\n",
    "        \"Assets\": {\n",
    "            \"Cash and Short Term Assets\": [\n",
    "                \"Cash\", \"Cash & Equivalents\", \"Short Term Investments\"\n",
    "            ],\n",
    "            \"Accounts Receivable - Trade, Net\": [\"Accounts Receivable - Trade, Gross\"],\n",
    "            \"Total Receivables, Net\": [\"Receivables, Other\"],\n",
    "            \"Total Inventory\": [\"Inventories - Finished Goods\", \"Inventories - Work in Progress\"],\n",
    "            \"Prepaid Expenses\": [],\n",
    "            \"Other Current Assets, Total\": [\"Restricted Cash - Current\", \"Other Current Assets\"],\n",
    "            ########################################\n",
    "            \"Total Current Assets\": [],\n",
    "            ########################################\n",
    "            \"Property/Plant/Equipment, Total - Gross\": [\n",
    "                \"Buildings - Gross\",\n",
    "                \"Land/Improvements - Gross\",\n",
    "                \"Machinery/Equipment - Gross\",\n",
    "                \"Other Property/Plant/Equipment - Gross\"\n",
    "            ],\n",
    "            \"Property/Plant/Equipment, Total - Net\": [\"Accumulated Depreciation, Total\"],\n",
    "            \"Goodwill, Net\": [],\n",
    "            \"Intangibles, Net\": [],\n",
    "            \"Long Term Investments\": [\"Long Term Investments - Other\"],\n",
    "            \"Note Receivable - Long Term\": [],\n",
    "            \"Other Long Term Assets, Total\": [\n",
    "                \"Restricted Cash - Long Term\",\n",
    "                \"Other Long Term Assets\"\n",
    "            ],\n",
    "            ########################################\n",
    "            \"Total Assets\": []\n",
    "            ########################################\n",
    "        },\n",
    "        \"Liabilities\": {\n",
    "            \"Accounts Payable\": [],\n",
    "            \"Payable/Accrued\": [],\n",
    "            \"Accrued Expenses\": [],\n",
    "            \"Notes Payable/Short Term Debt\": [],\n",
    "            \"Current Portion of Long Term Debt and Capital Leases\": [],\n",
    "            \"Other Current Liabilities, Total\": [\"Customer Advances\", \"Other Current Liabilities\"],\n",
    "            ########################################\n",
    "            \"Total Current Liabilities\": [],\n",
    "            ########################################\n",
    "            \"Total Long Term Debt\": [\"Long Term Debt\", \"Capital Lease Obligations\"],\n",
    "            \"Total Debt\": [],\n",
    "            \"Deferred Income Tax\": [],\n",
    "            \"Minority Interest\": [],\n",
    "            \"Other Liabilities, Total\": [\"Other Long Term Liabilities\"],\n",
    "            ########################################\n",
    "            \"Total Liabilities\": []\n",
    "            ########################################\n",
    "        },\n",
    "        \"Shareholders' Equity\": {\n",
    "            \"Redeemable Preferred Stock, Total\": [],\n",
    "            \"Preferred Stock - Non Redeemable, Net\": [],\n",
    "            \"Common Stock, Total\": [\"Common Stock\"],\n",
    "            \"Additional Paid-In Capital\": [],\n",
    "            \"Retained Earnings (Accumulated Deficit)\": [],\n",
    "            \"Treasury Stock - Common\": [],\n",
    "            \"Employee Stock Ownership Plan Debt Guarantee\": [],\n",
    "            \"Unrealized Gain (Loss)\": [],\n",
    "            \"Other Equity, Total\": [\"Translation Adjustment\", \"Other Comprehensive Income\"],\n",
    "            ########################################\n",
    "            \"Total Equity\": [],\n",
    "            ########################################\n",
    "        },\n",
    "        \"Total Liabilities & Shareholders' Equity\": []\n",
    "        \n",
    "    },\n",
    "    \"Income Statement\": {\n",
    "        \"Revenue\": {\n",
    "            \"Net Sales\": []\n",
    "        },\n",
    "        \"Other Revenue, Total\": [],\n",
    "        \"Cost of Revenue, Total\": [\"Cost of Revenue\"],\n",
    "        ########################################\n",
    "        \"Gross Profit\": [],\n",
    "        ########################################\n",
    "        \"Selling/General/Admin. Expenses, Total\": [\"Selling/General/Administrative Expenses\"],\n",
    "        \"Research & Development\": [],\n",
    "        \"Depreciation/Amortization\": [],\n",
    "        \"Interest Expense, Net - Operating\": [],\n",
    "        \"Interest/Investment Income - Operating\": [],\n",
    "        \"Interest Expense (Income) - Net Operating\": [],\n",
    "        \"Interest Expense (Income) - Net Operating, Total\": [],\n",
    "        \"Unusual Expense (Income)\": [],\n",
    "        \"Other Operating Expenses, Total\": [],\n",
    "        \"Total Operating Expense\": [],\n",
    "        ########################################\n",
    "        \"Operating Income\": [],\n",
    "        ########################################\n",
    "        \"Interest Expense, Net Non-Operating\": [\"Interest Expense - Non-Operating\"],\n",
    "        \"Interest/Invest Income - Non-Operating\": [\"Interest Income - Non-Operating\"],\n",
    "        \"Interest Income (Expense), Net Non-Operating\": [],\n",
    "        \"Interest Income (Expense), Net-Non-Operating, Total\": [],\n",
    "        \"Gain (Loss) on Sale of Assets\": [],\n",
    "        \"Other, Net\": [\"Other Non-Operating Income (Expense)\"],\n",
    "        \"Net Income Before Taxes\": [],\n",
    "        \"Provision for Income Taxes\": [],\n",
    "        \"Net Income After Taxes\": [],\n",
    "        \"Minority Interest\": [],\n",
    "        \"Equity In Affiliates\": [],\n",
    "        \"U.S. GAAP Adjustment\": [],\n",
    "        ########################################\n",
    "        \"Net Income Before Extra. Items\": [],\n",
    "        ########################################\n",
    "        \"Accounting Change\": [],\n",
    "        \"Discontinued Operations\": [],\n",
    "        \"Extraordinary Item\": [],\n",
    "        \"Tax on Extraordinary Items\": [],\n",
    "        \"Total Extraordinary Items\": [],\n",
    "        ########################################\n",
    "        \"Net Income\": [],\n",
    "        ########################################\n",
    "        \"Preferred Dividends\": [],\n",
    "        \"General Partners' Distributions\": [],\n",
    "        \"Miscellaneous Earnings Adjustment\": [],\n",
    "        \"Pro Forma Adjustment\": [],\n",
    "        \"Interest Adjustment - Primary Earnings Per Share\": [],\n",
    "        \"Total Adjustments to Net Income\": [],\n",
    "        \"Income Available to Common Shareholders Excluding Extraordinary\": [],\n",
    "        \"Income Available to Common Shareholders Including Extraordinary\": [],\n",
    "        \"Basic Weighted Average Shares\": [],\n",
    "        \"Basic Earnings Per Share Excluding Extraordinary Items\": [],\n",
    "        \"Basic Earnings Per Share Including Extraordinary Items\": [],\n",
    "        \"Dilution Adjustment\": [],\n",
    "        \"Diluted Net Income\": [],\n",
    "        \"Diluted Weighted Average Shares\": [],\n",
    "        \"Diluted Earnings Per Share Excluding Extraordinary Items\": [],\n",
    "        \"Diluted Earnings Per Share Including Extraordinary Items\": []\n",
    "    },\n",
    "    \"Cash Flow\": {\n",
    "        \"Operating Activities\": {\n",
    "            \"Net Income/Starting Line\": [],\n",
    "            \"Depreciation/Depletion\": [\"Depreciation\"],\n",
    "            \"Amortization\": [],\n",
    "            \"Deferred Taxes\": [],\n",
    "            \"Non-Cash Items\": [\"Other Non-Cash Items\"],\n",
    "            \"Changes in Working Capital\": [\n",
    "                \"Accounts Receivable\",\n",
    "                \"Inventories\",\n",
    "                \"Other Assets\",\n",
    "                \"Accounts Payable\",\n",
    "                \"Other Liabilities\"\n",
    "            ],\n",
    "            ########################################\n",
    "            \"Cash from Operating Activities\": []\n",
    "            ########################################\n",
    "        },\n",
    "        \"Investing\": {\n",
    "            \"Capital Expenditures\": [\"Purchase of Fixed Assets\"],\n",
    "            \"Other Investing Cash Flow Items, Total\": [\n",
    "                \"Acquisition of Business\",\n",
    "                \"Sale/Maturity of Investment\",\n",
    "                \"Purchase of Investments\",\n",
    "                \"Other Investing Cash Flow\"\n",
    "            ],\n",
    "            ########################################\n",
    "            \"Cash from Investing Activities\": [],\n",
    "            ########################################\n",
    "        },\n",
    "        \"Financing\": {\n",
    "            \"Financing Cash Flow Items\": [\"Other Financing Cash Flow\"],\n",
    "            \"Total Cash Dividends Paid\": [\n",
    "                \"Cash Dividends Paid - Common\"\n",
    "            ],\n",
    "            \"Issuance (Retirement) of Stock, Net\": [\n",
    "                \"Sale/Issuance of Common\",\n",
    "                \"Repurchase/Retirement of Common\",\n",
    "                \"Common Stock, Net\"\n",
    "            ],\n",
    "            \"Issuance (Retirement) of Debt, Net\": [\n",
    "                \"Short Term Debt Issued\",\n",
    "                \"Short Term Debt, Net\",\n",
    "                \"Long Term Debt Reduction\",\n",
    "                \"Long Term Debt, Net\",\n",
    "                \"Total Debt Reduction\",\n",
    "            ],\n",
    "            ########################################\n",
    "            \"Cash from Financing Activities\": []\n",
    "            ########################################\n",
    "        },\n",
    "        \"Foreign Exchange Effects\": [],\n",
    "        ########################################\n",
    "        \"Net Change in Cash\": [],\n",
    "        ########################################\n",
    "        \"Cash Interest Paid\": [],\n",
    "        \"Cash Taxes Paid\": [],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to generate embeddings with pooling\n",
    "def get_embedding(text):\n",
    "    \"\"\"Generates embeddings using mean pooling from a BGE model.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "    masked_embeddings = embeddings * attention_mask\n",
    "    summed = masked_embeddings.sum(1)\n",
    "    counted = attention_mask.sum(1)\n",
    "    return (summed / counted).cpu() # Move the memory back to the CPU\n",
    "\n",
    "# Prepare category embeddings\n",
    "def prepare_category_embeddings():\n",
    "    match_terms = []\n",
    "    for statement, categories in STATEMENT_TYPES.items():\n",
    "        for category, subcategories in categories.items():\n",
    "            if isinstance(subcategories, dict):\n",
    "                for subcategory, terms in subcategories.items():\n",
    "                    combined_terms = f\"{statement} {category} {subcategory} \" + \" \".join(terms).lower()\n",
    "                    match_terms.append((combined_terms, statement, category, subcategory))\n",
    "            else:\n",
    "                combined_terms = f\"{statement} {category} \" + \" \".join(subcategories).lower()\n",
    "                match_terms.append((combined_terms, statement, category, \"\"))\n",
    "\n",
    "    match_df = pd.DataFrame(match_terms, columns=[\"Processed Terms\", \"Statement\", \"Main Category\", \"Subcategory\"])\n",
    "    match_df[\"Embedding\"] = match_df[\"Processed Terms\"].apply(get_embedding)\n",
    "    return match_df\n",
    "\n",
    "# Function to classify US-GAAP field names using embeddings\n",
    "def classify_us_gaap_field(field_name, match_df, top_n=3):\n",
    "    # Also handles acronymns\n",
    "    processed_field = re.sub(\n",
    "        r'(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])',\n",
    "        ' ',\n",
    "        field_name\n",
    "    ).lower()\n",
    "\n",
    "    field_embedding = get_embedding(processed_field)\n",
    "\n",
    "    print(f\"Processed Field: {processed_field}\")\n",
    "\n",
    "    match_df[\"Similarity\"] = match_df[\"Embedding\"].apply(\n",
    "        lambda emb: F.cosine_similarity(field_embedding, emb).item()\n",
    "    )\n",
    "    return match_df.sort_values(by=\"Similarity\", ascending=False).head(top_n)\n",
    "\n",
    "\n",
    "match_df = prepare_category_embeddings()\n",
    "\n",
    "def get_classified_us_gaap_tuple(us_gaap_field):\n",
    "\n",
    "\n",
    "    top_matches = classify_us_gaap_field(us_gaap_field, match_df)\n",
    "\n",
    "    # Display the results\n",
    "    # top_matches[[\"Statement\", \"Main Category\", \"Subcategory\", \"Similarity\"]]\n",
    "    # top_matches\n",
    "\n",
    "    result_tuple = tuple(top_matches.iloc[0][[\"Statement\", \"Main Category\", \"Subcategory\", \"Similarity\"]])\n",
    "\n",
    "    return result_tuple\n",
    "\n",
    "# get_classified_us_gaap_tuple(\"SalesRevenueGoodsNet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the directory to search for CSV files\n",
    "start_dir = \"../data/orig.us-gaap\"\n",
    "\n",
    "# Dictionary to store column reporting frequency per form type\n",
    "column_distribution = defaultdict(lambda: {\"10-K\": 0, \"10-Q\": 0, \"latest_filed\": 0})\n",
    "\n",
    "# Track the number of processed files\n",
    "file_count = 0\n",
    "\n",
    "# Define the minimum year threshold\n",
    "current_year = pd.Timestamp.now().year\n",
    "min_year = current_year - 4  # Consider only filings within the last 4 years\n",
    "\n",
    "# Iterate through all CSV files in the directory\n",
    "for root, _, files in os.walk(start_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            try:\n",
    "                # Read CSV file with dtype=str to prevent automatic type conversion\n",
    "                df = pd.read_csv(file_path, dtype=str)\n",
    "\n",
    "                # Ensure required columns exist\n",
    "                if \"form\" not in df.columns or \"filed\" not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                # Convert \"filed\" column to numeric (year only)\n",
    "                df[\"filed\"] = pd.to_datetime(df[\"filed\"], errors=\"coerce\").dt.year\n",
    "\n",
    "                # Filter out entries older than min_year\n",
    "                df = df[df[\"filed\"] >= min_year]\n",
    "\n",
    "                if df.empty:\n",
    "                    continue\n",
    "\n",
    "                # Track the most recent year for each column\n",
    "                latest_filing_year = df[\"filed\"].max()\n",
    "\n",
    "                # Count occurrences of each column by form type\n",
    "                for column in df.columns:\n",
    "                    form_counts = df[\"form\"].value_counts()\n",
    "                    for form_type in [\"10-K\", \"10-Q\"]:\n",
    "                        if form_type in form_counts:\n",
    "                            column_distribution[column][form_type] += form_counts[form_type]\n",
    "\n",
    "                    # Update latest filing year\n",
    "                    if latest_filing_year:\n",
    "                        column_distribution[column][\"latest_filed\"] = max(\n",
    "                            column_distribution[column][\"latest_filed\"], latest_filing_year\n",
    "                        )\n",
    "\n",
    "                # Increment processed file counter\n",
    "                file_count += 1\n",
    "                if file_count % 10 == 0:\n",
    "                    print(f\"Processed {file_count} files...\")\n",
    "\n",
    "                # TODO: Remove\n",
    "                # if file_count > 500:\n",
    "                #     break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df_distribution = pd.DataFrame.from_dict(column_distribution, orient=\"index\")\n",
    "\n",
    "# Filter columns that haven't been reported in the last 4 years\n",
    "df_distribution = df_distribution[df_distribution[\"latest_filed\"] >= min_year]\n",
    "\n",
    "# Sort by most frequently reported in \"10-K\" and \"10-Q\"\n",
    "df_distribution.sort_values(by=[\"10-K\", \"10-Q\"], ascending=False, inplace=True)\n",
    "\n",
    "####\n",
    "\n",
    "# Compute total number of unique 10-K and 10-Q documents (not their sum)\n",
    "total_10k_docs = df_distribution[\"10-K\"].count()\n",
    "total_10q_docs = df_distribution[\"10-Q\"].count()\n",
    "\n",
    "# Compute percentage for each row based on unique document count\n",
    "df_distribution[\"10-K %\"] = df_distribution[\"10-K\"] / total_10k_docs\n",
    "df_distribution[\"10-Q %\"] = df_distribution[\"10-Q\"] / total_10q_docs\n",
    "\n",
    "# Normalize percentages so that the highest value is 100%\n",
    "df_distribution[\"10-K %\"] = (df_distribution[\"10-K %\"] / df_distribution[\"10-K %\"].max()) * 100\n",
    "df_distribution[\"10-Q %\"] = (df_distribution[\"10-Q %\"] / df_distribution[\"10-Q %\"].max()) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "save_every_n = 100\n",
    "total_rows = len(df_distribution)\n",
    "\n",
    "# Maps (statement, category, subcategory) → unique integer ID\n",
    "combination_id_map = {}\n",
    "\n",
    "# Tracks how many times each combination has occurred\n",
    "combination_counts = defaultdict(int)\n",
    "\n",
    "# Iterate with enumeration for progress monitoring\n",
    "for i, row in enumerate(df_distribution.itertuples()):\n",
    "    field_name = row[0]  # First column is the field name\n",
    "\n",
    "    if field_name in {\"fy\", \"fp\", \"form\", \"filed\", \"accn\"}:\n",
    "        continue\n",
    "\n",
    "    statement, category, subcategory, similarity = get_classified_us_gaap_tuple(field_name)\n",
    "\n",
    "    df_distribution.at[row.Index, \"Statement\"] = statement\n",
    "    df_distribution.at[row.Index, \"Main Category\"] = category\n",
    "    df_distribution.at[row.Index, \"Subcategory\"] = subcategory\n",
    "    df_distribution.at[row.Index, \"Similarity\"] = similarity\n",
    "\n",
    "    combination = (statement, category, subcategory)\n",
    "\n",
    "    if combination not in combination_id_map:\n",
    "        combination_id_map[combination] = len(combination_id_map) + 1\n",
    "\n",
    "    combination_id = combination_id_map[combination]\n",
    "    df_distribution.at[row.Index, \"Combination ID\"] = combination_id\n",
    "\n",
    "    combination_counts[combination] += 1\n",
    "    try_order = combination_counts[combination]\n",
    "    df_distribution.at[row.Index, \"Try Order\"] = try_order\n",
    "\n",
    "    # Coerce to integer\n",
    "    df_distribution[\"Combination ID\"] = pd.to_numeric(\n",
    "        df_distribution[\"Combination ID\"], errors=\"coerce\"\n",
    "    ).astype(\"Int64\")\n",
    "    df_distribution[\"Try Order\"] = pd.to_numeric(\n",
    "        df_distribution[\"Try Order\"], errors=\"coerce\"\n",
    "    ).astype(\"Int64\")\n",
    "\n",
    "    print(f\"Processed row {i + 1}/{total_rows}\")\n",
    "    print(\n",
    "        f\"Field: {field_name}, Statement: {statement}, Category: {category}, \"\n",
    "        f\"Subcategory: {subcategory}, Similarity: {similarity:.4f}, \"\n",
    "        f\"Combination ID: {combination_id}, Try Order: {try_order}\"\n",
    "    )\n",
    "    \n",
    "     # Save every `n` rows and also on the last iteration\n",
    "    if i % save_every_n == 0 or i == total_rows:\n",
    "        print(\"Saving...\")\n",
    "        df_distribution.to_csv(\"column_distribution.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US-GAAP 2025 Hierarchy\n",
    "\n",
    "https://www.sec.gov/data-research/standard-taxonomies/operating-companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
