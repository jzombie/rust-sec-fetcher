{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from db import DB\n",
    "from utils import generate_us_gaap_description\n",
    "from utils.pytorch import get_device, seed_everything\n",
    "from tqdm import tqdm  # For progress bar\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# === Setup for BGE model ===\n",
    "MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "encoder = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move the model to the selected device\n",
    "encoder = encoder.to(device)\n",
    "encoder.eval()  # Ensure model is in evaluation mode (no gradients needed)\n",
    "\n",
    "# Database setup\n",
    "db = DB()\n",
    "\n",
    "queries = {\n",
    "    \"concept_variations\": \"\"\"\n",
    "        SELECT\n",
    "            t.id AS us_gaap_concept_id,\n",
    "            t.name AS us_gaap_concept_name,\n",
    "            v.text AS variation_text,\n",
    "            GROUP_CONCAT(DISTINCT m.ofss_category_id ORDER BY m.ofss_category_id) AS ofss_category_ids\n",
    "        FROM us_gaap_concept t\n",
    "        JOIN us_gaap_concept_description_variation v ON v.us_gaap_concept_id = t.id\n",
    "        LEFT JOIN us_gaap_concept_ofss_category m ON m.us_gaap_concept_id = t.id\n",
    "        WHERE m.ofss_category_id IS NOT NULL -- Ensure we only select rows that have an associated ofss_category_id\n",
    "        GROUP BY t.id, v.text\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "def generate_embeddings(texts, batch_size=16):\n",
    "    \"\"\"\n",
    "    Generate embeddings for texts using the transformer model on the MPS device.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating Embeddings\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "\n",
    "        # Tokenize the batch of texts\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # No gradients required for inference\n",
    "        with torch.no_grad():\n",
    "            outputs = encoder(**inputs)\n",
    "\n",
    "        # Extract embeddings (use [CLS] token, first token in the sequence)\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        embeddings.extend(batch_embeddings)\n",
    "\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def build_concept_dataset(query: str):\n",
    "    # Fetch data from the database\n",
    "    df = db.get(query, [\"us_gaap_concept_id\", \"us_gaap_concept_name\", \"variation_text\", \"ofss_category_ids\"])\n",
    "\n",
    "    # Apply generate_us_gaap_description to the concept names\n",
    "    df[\"us_gaap_concept_description\"] = df[\"us_gaap_concept_name\"].apply(generate_us_gaap_description)\n",
    "\n",
    "    # Generate embeddings for variation text and description\n",
    "    print(\"Generating embeddings for variation text...\")\n",
    "    variation_embeddings = generate_embeddings(df[\"variation_text\"].tolist())\n",
    "    \n",
    "    print(\"Generating embeddings for concept descriptions...\")\n",
    "    description_embeddings = generate_embeddings(df[\"us_gaap_concept_description\"].tolist())\n",
    "\n",
    "    # Add embeddings as columns directly to the DataFrame\n",
    "    df[\"variation_embedding\"] = list(variation_embeddings)\n",
    "    df[\"description_embedding\"] = list(description_embeddings)\n",
    "\n",
    "    # Optionally process the `ofss_category_ids` if needed\n",
    "    df[\"ofss_category_ids\"] = df[\"ofss_category_ids\"].apply(lambda s: [int(x) for x in s.split(\",\")] if s else [])\n",
    "\n",
    "    # Limit to a maximum of 2 labels per row\n",
    "    df[\"ofss_category_ids\"] = df[\"ofss_category_ids\"].apply(lambda x: x[:2])\n",
    "\n",
    "    # Save the dataset as JSONL (with embeddings and category IDs included)\n",
    "    output_file = \"data/us_gaap_concepts_with_variations_and_embeddings.jsonl\"\n",
    "    df.to_json(output_file, orient=\"records\", lines=True)\n",
    "\n",
    "    print(f\"Dataset saved to {output_file} with {len(df)} rows, including embeddings and categories.\")\n",
    "\n",
    "# Run the function to build the dataset\n",
    "if __name__ == \"__main__\":\n",
    "    build_concept_dataset(queries[\"concept_variations\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For inference\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from db import DB\n",
    "# from utils import generate_us_gaap_description\n",
    "# from utils.pytorch import get_device\n",
    "# from tqdm import tqdm\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "# import os\n",
    "\n",
    "# # === Setup for BGE model ===\n",
    "# MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# encoder = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# device = get_device()\n",
    "# print(f\"Using device: {device}\")\n",
    "# encoder = encoder.to(device)\n",
    "# encoder.eval()\n",
    "\n",
    "# # Database setup\n",
    "# db = DB()\n",
    "\n",
    "# queries = {\n",
    "#     \"reference_concepts\": \"\"\"\n",
    "#         SELECT\n",
    "#             t.id AS us_gaap_concept_id,\n",
    "#             t.name AS us_gaap_concept_name\n",
    "#         FROM us_gaap_concept t\n",
    "#     \"\"\"\n",
    "# }\n",
    "\n",
    "# def generate_embeddings(texts, batch_size=16):\n",
    "#     \"\"\"\n",
    "#     Generate embeddings for texts using the transformer model.\n",
    "#     \"\"\"\n",
    "#     embeddings = []\n",
    "#     for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating Embeddings\"):\n",
    "#         batch_texts = texts[i:i+batch_size]\n",
    "\n",
    "#         inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "#         with torch.no_grad():\n",
    "#             outputs = encoder(**inputs)\n",
    "#         batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "#         embeddings.extend(batch_embeddings)\n",
    "\n",
    "#     return np.array(embeddings)\n",
    "\n",
    "# def build_reference_embedding_dataset(query: str):\n",
    "#     df = db.get(query, [\"us_gaap_concept_id\", \"us_gaap_concept_name\"])\n",
    "\n",
    "#     # Apply the description generator\n",
    "#     df[\"us_gaap_concept_description\"] = df[\"us_gaap_concept_name\"].apply(generate_us_gaap_description)\n",
    "\n",
    "#     # Generate embeddings only for concept descriptions\n",
    "#     print(\"Generating embeddings for reference concept descriptions...\")\n",
    "#     description_embeddings = generate_embeddings(df[\"us_gaap_concept_description\"].tolist())\n",
    "#     df[\"description_embedding\"] = list(description_embeddings)\n",
    "\n",
    "#     # Keep only the necessary columns\n",
    "#     ref_df = df[[\"us_gaap_concept_id\", \"us_gaap_concept_name\", \"description_embedding\"]]\n",
    "\n",
    "#     output_file = \"data/us_gaap_concept_reference_embeddings.jsonl\"\n",
    "#     ref_df.to_json(output_file, orient=\"records\", lines=True)\n",
    "\n",
    "#     print(f\"Reference dataset saved to {output_file} with {len(ref_df)} rows.\")\n",
    "\n",
    "# # Run the function to build the reference dataset\n",
    "# if __name__ == \"__main__\":\n",
    "#     build_reference_embedding_dataset(queries[\"reference_concepts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from db import DB\n",
    "# from utils import generate_us_gaap_description\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Database setup\n",
    "# db = DB()\n",
    "\n",
    "# queries = {\n",
    "#     \"gaap_concepts\": \"\"\"\n",
    "#         SELECT DISTINCT\n",
    "#             t.id AS us_gaap_concept_id,\n",
    "#             t.name AS us_gaap_concept_name\n",
    "#         FROM us_gaap_concept t\n",
    "#         JOIN us_gaap_concept_description_variation v ON v.us_gaap_concept_id = t.id\n",
    "#     \"\"\"\n",
    "# }\n",
    "\n",
    "# # Function to load the data from the database using the above query\n",
    "# def build_gaap_concept_description_dataset(query: str):\n",
    "#     # Fetch concept names and ids from the database where concepts have variations\n",
    "#     df = db.get(query, [\"us_gaap_concept_id\", \"us_gaap_concept_name\"])\n",
    "    \n",
    "#     # Apply generate_us_gaap_description to generate descriptions for the concept names\n",
    "#     df[\"us_gaap_concept_description\"] = df[\"us_gaap_concept_name\"].apply(generate_us_gaap_description)\n",
    "\n",
    "#     # Save the dataset as JSONL\n",
    "#     df.to_json(\"data/us_gaap_concepts_with_descriptions.jsonl\", orient=\"records\", lines=True)\n",
    "#     print(f\"Dataset saved with {len(df)} rows.\")\n",
    "\n",
    "# # Run the function to build the dataset\n",
    "# if __name__ == \"__main__\":\n",
    "#     build_gaap_concept_description_dataset(queries[\"gaap_concepts\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from db import DB\n",
    "# from utils import generate_us_gaap_description \n",
    "\n",
    "# db = DB()\n",
    "\n",
    "# queries = {\n",
    "#     \"train\": \"\"\"\n",
    "#         SELECT\n",
    "#             v.text AS input_text,\n",
    "#             COALESCE(t.balance_type_id, 0) AS balance_type_id,\n",
    "#             COALESCE(t.period_type_id, 0) AS period_type_id,\n",
    "#             GROUP_CONCAT(DISTINCT m.ofss_category_id ORDER BY m.ofss_category_id) AS labels\n",
    "#         FROM us_gaap_concept_description_variation v\n",
    "#         JOIN us_gaap_concept t ON t.id = v.us_gaap_concept_id\n",
    "#         JOIN us_gaap_concept_ofss_category m ON m.us_gaap_concept_id = t.id\n",
    "#         GROUP BY v.text, t.balance_type_id, t.period_type_id\n",
    "#     \"\"\",\n",
    "#     \"val\": \"\"\"\n",
    "#         SELECT\n",
    "#             t.name AS input_text,\n",
    "#             COALESCE(t.balance_type_id, 0) AS balance_type_id,\n",
    "#             COALESCE(t.period_type_id, 0) AS period_type_id,\n",
    "#             GROUP_CONCAT(DISTINCT m.ofss_category_id ORDER BY m.ofss_category_id) AS labels\n",
    "#         FROM us_gaap_concept t\n",
    "#         JOIN us_gaap_concept_ofss_category m ON m.us_gaap_concept_id = t.id\n",
    "#         GROUP BY t.name, t.balance_type_id, t.period_type_id\n",
    "#     \"\"\"\n",
    "# }\n",
    "\n",
    "# def build_dataset(name: str, query: str):\n",
    "#     df = db.get(query, [\"input_text\", \"balance_type_id\", \"period_type_id\", \"labels\"])\n",
    "    \n",
    "#     if name == \"val\":\n",
    "#         # Apply generate_us_gaap_description to input_text for validation dataset\n",
    "#         df[\"input_text\"] = df[\"input_text\"].apply(generate_us_gaap_description)\n",
    "\n",
    "#     # Process the labels\n",
    "#     df[\"labels\"] = df[\"labels\"].apply(lambda s: [int(x) for x in s.split(\",\")] if s else [])\n",
    "\n",
    "#     # Limit to a maximum of 2 labels per row\n",
    "#     df[\"labels\"] = df[\"labels\"].apply(lambda x: x[:2])\n",
    "    \n",
    "#     # Save as JSONL\n",
    "#     df.to_json(f\"data/{name}.jsonl\", orient=\"records\", lines=True)\n",
    "#     print(f\"{name}.jsonl saved with {len(df)} rows.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     for split, sql in queries.items():\n",
    "#         build_dataset(split, sql)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from db import DB\n",
    "from utils import generate_us_gaap_description  # Importing the necessary function\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# Setup for BGE model\n",
    "MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "encoder = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "# Database setup\n",
    "db = DB()\n",
    "\n",
    "queries = {\n",
    "    \"concept_variations\": \"\"\"\n",
    "        SELECT\n",
    "            v.text AS input_text,\n",
    "            t.name AS us_gaap_description\n",
    "        FROM us_gaap_concept_description_variation v\n",
    "        JOIN us_gaap_concept t ON t.id = v.us_gaap_concept_id\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Cache embeddings for each text individually\n",
    "embedding_cache = {}\n",
    "\n",
    "# Generate embeddings for text using the BGE model\n",
    "def generate_embeddings(texts):\n",
    "    \"\"\"\n",
    "    This function generates embeddings for a given list of text descriptions.\n",
    "    It uses the BGE model to generate the embeddings, specifically the [CLS] token representation.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        if text in embedding_cache:  # Check if the embedding is cached\n",
    "            embeddings.append(embedding_cache[text])  # Use the cached embedding\n",
    "        else:\n",
    "            # If not cached, generate and cache the embedding\n",
    "            if isinstance(text, str):\n",
    "                texts = [text]  # Convert single string to a list of strings\n",
    "            elif not isinstance(texts, list):\n",
    "                raise ValueError(\"Input must be a string or a list of strings.\")\n",
    "\n",
    "            texts = [str(text) if not isinstance(text, str) else text for text in texts]\n",
    "            inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = encoder(**inputs)\n",
    "            \n",
    "            text_embedding = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token embedding\n",
    "            embedding_cache[text] = text_embedding  # Cache the generated embedding\n",
    "            embeddings.append(text_embedding)\n",
    "    return torch.stack(embeddings)\n",
    "\n",
    "# Function to find the most similar us_gaap_description to a given variation (input text)\n",
    "def find_closest_description(variation, descriptions):\n",
    "    variation_embedding = generate_embeddings([variation]).squeeze(0)  # Remove extra dimensions\n",
    "    \n",
    "    # Generate embeddings for all us_gaap_descriptions\n",
    "    description_embeddings = generate_embeddings(descriptions).squeeze(1)  # Remove extra dimensions\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(variation_embedding.cpu().numpy(), description_embeddings.cpu().numpy())\n",
    "    \n",
    "    # Find the index of the description with the highest cosine similarity\n",
    "    closest_idx = similarity.argmax()\n",
    "    \n",
    "    return descriptions[closest_idx], similarity[0][closest_idx]\n",
    "\n",
    "def build_dataset(query: str):\n",
    "    df = db.get(query, [\"input_text\", \"us_gaap_description\"])\n",
    "\n",
    "    # Apply the generate_us_gaap_description function to the us_gaap_description\n",
    "    df[\"us_gaap_description\"] = df[\"us_gaap_description\"].apply(generate_us_gaap_description)\n",
    "\n",
    "    # For each variation in the dataset, find the closest match to the us_gaap_description\n",
    "    closest_matches = []\n",
    "    is_correct_matches = []\n",
    "    similarities = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing data\"):\n",
    "        # Find the closest us_gaap_description for the current variation\n",
    "        closest_match, similarity = find_closest_description(row[\"input_text\"], df[\"us_gaap_description\"].tolist())\n",
    "        closest_matches.append(closest_match)\n",
    "        similarities.append(similarity)\n",
    "        \n",
    "        # Check if the closest match is the same as the original us_gaap_description\n",
    "        is_correct = closest_match == row[\"us_gaap_description\"]\n",
    "        is_correct_matches.append(is_correct)\n",
    "\n",
    "    # Add the closest match and correctness flag to the dataframe\n",
    "    df[\"closest_match\"] = closest_matches\n",
    "    df[\"similarity\"] = similarities\n",
    "    df[\"is_correct_match\"] = is_correct_matches\n",
    "\n",
    "    # Calculate error rate (percentage of incorrect matches)\n",
    "    total_entries = len(df)\n",
    "    correct_matches = sum(is_correct_matches)\n",
    "    accuracy = correct_matches / total_entries\n",
    "    error_rate = 1 - accuracy\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Error Rate: {error_rate * 100:.2f}%\")\n",
    "    \n",
    "    # Save as JSONL\n",
    "    df.to_json(f\"data/matched_tags.jsonl\", orient=\"records\", lines=True)\n",
    "    print(f\"Matched data saved to 'matched_tags.jsonl' with {len(df)} rows.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_dataset(queries[\"concept_variations\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
