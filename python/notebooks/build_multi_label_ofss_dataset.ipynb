{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import optuna\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchmetrics import Precision, Recall\n",
    "\n",
    "\n",
    "# === SEED ===\n",
    "SEED = 42\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    pl.seed_everything(seed, workers=True)\n",
    "\n",
    "# Ensure it's called!\n",
    "seed_everything(SEED)\n",
    "\n",
    "# === CONFIG ===\n",
    "TRAIN_JSONL_PATH = \"data/train.jsonl\"  # Correct path for your dataset\n",
    "VAL_JSONL_PATH = \"data/val.jsonl\"  # Correct path for your dataset\n",
    "MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "OUTPUT_PATH = \"data/fine_tuned_gaap_classifier\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "OPTUNA_DB_PATH = os.path.join(OUTPUT_PATH, \"optuna_study.db\")\n",
    "EPOCHS = 200\n",
    "PATIENCE = 5\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# === Load Data from JSONL files ===\n",
    "def load_jsonl(filepath):\n",
    "    with open(filepath, \"r\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "train_data = load_jsonl(TRAIN_JSONL_PATH)\n",
    "val_data = load_jsonl(VAL_JSONL_PATH)\n",
    "\n",
    "# === Dynamically determine the number of possible categories ===\n",
    "all_categories = set()\n",
    "for entry in train_data + val_data:\n",
    "    all_categories.update(entry[\"labels\"])\n",
    "\n",
    "num_labels = max(all_categories)  # Dynamically find the highest category label number\n",
    "print(f\"Number of categories: {num_labels}\")\n",
    "\n",
    "\n",
    "# === Dataset Class ===\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.samples = []\n",
    "        for d in data:\n",
    "            input_text = d[\"input_text\"]\n",
    "            labels = d[\"labels\"]\n",
    "            # Ensure labels are a list of integers, default to an empty list if not available\n",
    "            labels = [int(label) for label in labels] if labels else []\n",
    "            self.samples.append((input_text, labels))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text, labels = self.samples[idx]\n",
    "        labels_tensor = torch.zeros(num_labels)  # Dynamic number of categories\n",
    "        for label in labels:\n",
    "            labels_tensor[label - 1] = 1  # Set category positions to 1 (0-indexed for PyTorch)\n",
    "        return text, labels_tensor\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    return list(texts), torch.stack(labels)\n",
    "\n",
    "# === Model Definition ===\n",
    "class GAAPClassifier(pl.LightningModule):\n",
    "    def __init__(self, model_name, dropout_rate, num_labels, lr):\n",
    "        super().__init__()\n",
    "        self.encoder = SentenceTransformer(model_name, device=device)\n",
    "        self.dim = self.encoder.get_sentence_embedding_dimension()\n",
    "        self.attn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.dim, self.dim),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(self.dim, self.dim)  # Keep the same dimension\n",
    "        )\n",
    "\n",
    "        self.norm = torch.nn.LayerNorm(self.dim)\n",
    "\n",
    "        self.num_labels = num_labels  # Ensure num_labels is passed\n",
    "\n",
    "        # Adjust output layer for multi-label classification\n",
    "        self.head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.dim, 128),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(dropout_rate),\n",
    "            torch.nn.Linear(128, self.num_labels)  # Use num_labels for output layer\n",
    "        )\n",
    "        \n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Metrics (removed compute_on_step argument)\n",
    "        self.precision = Precision(num_labels=self.num_labels, average=\"macro\", task=\"multilabel\")\n",
    "        self.recall = Recall(num_labels=self.num_labels, average=\"macro\", task=\"multilabel\")\n",
    "\n",
    "    def forward(self, texts):\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.encoder.encode(texts, convert_to_tensor=True, device=device)        \n",
    "        attended = self.attn(embeddings)\n",
    "        attended = self.norm(attended)\n",
    "        return self.head(attended)\n",
    "\n",
    "\n",
    "    def compute_loss(self, outputs, labels):\n",
    "        return self.loss_fn(outputs, labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        texts, labels = batch\n",
    "        outputs = self(texts)\n",
    "        loss = self.compute_loss(outputs, labels)\n",
    "        self.log(\"train/loss\", loss, prog_bar=True)\n",
    "\n",
    "        pred = torch.sigmoid(outputs) > 0.5\n",
    "        self.log(\"train/precision\", self.precision(pred, labels))\n",
    "        self.log(\"train/recall\", self.recall(pred, labels))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        texts, labels = batch\n",
    "        outputs = self(texts)\n",
    "        loss = self.compute_loss(outputs, labels)\n",
    "        self.log(\"val/loss\", loss, prog_bar=True)\n",
    "\n",
    "        pred = torch.sigmoid(outputs) > 0.5\n",
    "        self.log(\"val/precision\", self.precision(pred, labels))\n",
    "        self.log(\"val/recall\", self.recall(pred, labels))\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "# === Objective Function for Optuna ===\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 8, 64, step=8)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-6, 1e-2, log=True)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0, 0.5, step=0.1)\n",
    "\n",
    "    # Load dataset\n",
    "    train_loader = DataLoader(MultiLabelDataset(train_data),\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(MultiLabelDataset(val_data),\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            collate_fn=collate_fn)\n",
    "\n",
    "    model = GAAPClassifier(MODEL_NAME, dropout_rate, num_labels, lr)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=EPOCHS,\n",
    "        callbacks=[EarlyStopping(monitor=\"val/loss\", patience=PATIENCE)],\n",
    "        logger=TensorBoardLogger(OUTPUT_PATH),\n",
    "        accelerator=\"auto\",\n",
    "        devices=1\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    return trainer.callback_metrics[\"val/loss\"].item()\n",
    "\n",
    "# === Optuna Optimization ===\n",
    "study = optuna.create_study(direction=\"minimize\", storage=f\"sqlite:///{OPTUNA_DB_PATH}\", load_if_exists=True)\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "# Best Params\n",
    "print(\"Best params:\", study.best_params)\n",
    "best_trial = study.best_trial\n",
    "print(f\"Best trial value: {best_trial.value}\")\n",
    "for k, v in best_trial.params.items():\n",
    "    print(f\"    {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
