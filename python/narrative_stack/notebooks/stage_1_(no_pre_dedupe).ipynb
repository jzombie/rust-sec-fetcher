{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6ff429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from simd_r_drive import DataStore\n",
    "from us_gaap_store import UsGaapStore\n",
    "\n",
    "from simd_r_drive_ws_client import DataStoreWsClient\n",
    "\n",
    "WEBSOCKET_ADDRESS = \"127.0.0.1:58086\"\n",
    "data_store = DataStoreWsClient(WEBSOCKET_ADDRESS)\n",
    "us_gaap_store = UsGaapStore(data_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_embedding = us_gaap_store.lookup_by_index(0)[\"embedding\"]\n",
    "\n",
    "sample_embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98055128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Move to preprocessing\n",
    "\n",
    "# # UMAP visualization\n",
    "# umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, metric=\"cosine\")\n",
    "# umap_2d = umap_model.fit_transform(compressed)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(umap_2d[:, 0], umap_2d[:, 1], c=labels, cmap=\"tab10\", s=5)\n",
    "# plt.title(\"Concept/UOM Embeddings Clustered\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e52bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({\n",
    "#     \"concept\": [c for c, _ in concept_unit_pairs],\n",
    "#     \"unit\": [u for _, u in concept_unit_pairs],\n",
    "#     \"cluster\": labels\n",
    "# })\n",
    "# grouped = df.groupby(\"cluster\")\n",
    "\n",
    "# for cluster_id, group in grouped:\n",
    "#     print(f\"\\nCluster {cluster_id} ({len(group)} items):\")\n",
    "#     print(group.head(10).to_string(index=False))\n",
    "\n",
    "# noise = df[df[\"cluster\"] == -1]\n",
    "\n",
    "# print(f\"Noise points: {len(noise)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_points = df[df[\"cluster\"] == -1][[\"concept\", \"unit\"]].reset_index(drop=True)\n",
    "\n",
    "# noise_points.to_csv(\"noise_points.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a051cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Save both embeddings and tuples\n",
    "# np.savez_compressed(\n",
    "#     \"data/stage1_latents.npz\",\n",
    "#     keys=np.array([f\"{c}::{u}\" for c, u in concept_unit_pairs]),\n",
    "#     embeddings=compressed,\n",
    "#     concept_unit_value_tuples=np.array(concept_unit_value_tuples, dtype=object)\n",
    "# )\n",
    "\n",
    "# print(f\"âœ… Saved {len(concept_unit_value_tuples):,} tuples and {len(compressed):,} embeddings to 'stage1_latents.npz'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b489720e-de8d-4a98-b6ba-65e24051c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Move\n",
    "\n",
    "# # Debug scaled stats\n",
    "\n",
    "# from collections import defaultdict\n",
    "# import numpy as np\n",
    "\n",
    "# # Reconstruct per (concept, unit) from scaled_tuples\n",
    "# reconstructed = defaultdict(list)\n",
    "# all_vals = []\n",
    "\n",
    "# for concept, unit, val in scaled_tuples:\n",
    "#     key = (concept, unit)\n",
    "#     reconstructed[key].append(val)\n",
    "#     all_vals.append(val)\n",
    "\n",
    "# total_items = 0\n",
    "# total_no_variance_items = 0\n",
    "\n",
    "# # Analyze per group\n",
    "# for key, vals in reconstructed.items():\n",
    "#     vals_np = np.array(vals)\n",
    "\n",
    "#     if not np.all(np.isfinite(vals_np)):\n",
    "#         print(f\"[BAD SCALE] {key} has non-finite scaled values!\")\n",
    "#         print(\"  Sample:\", vals_np[:5])\n",
    "#         continue\n",
    "\n",
    "#     max_val = np.max(vals_np)\n",
    "#     min_val = np.min(vals_np)\n",
    "#     mean_val = np.mean(vals_np)\n",
    "\n",
    "#     if max_val > 1e6 or min_val < -1e6:\n",
    "#         print(f\"[OUTLIER SCALE] {key} range is extreme:\")\n",
    "#         print(\"  min:\", min_val, \"max:\", max_val, \"mean:\", mean_val)\n",
    "\n",
    "#     if np.allclose(max_val, min_val):\n",
    "#         # print(f\"[FLAT SCALE] {key} has no variance ({len(vals)} item(s)):\")\n",
    "#         # print(\"  value:\", max_val)\n",
    "#         total_no_variance_items += len(vals)\n",
    "\n",
    "#     total_items += len(vals)\n",
    "\n",
    "# # Global stats\n",
    "# all_vals_np = np.array(all_vals)\n",
    "\n",
    "# print(\"\\n=== GLOBAL STATS ===\")\n",
    "# print(f\"Global max: {np.max(all_vals_np)}\")\n",
    "# print(f\"Global min: {np.min(all_vals_np)}\")\n",
    "# print(f\"Global mean: {np.mean(all_vals_np)}\")\n",
    "# print(f\"Global std: {np.std(all_vals_np)}\")\n",
    "\n",
    "# print(f\"Total items: {total_items}\")\n",
    "# print(f\"Total no variance items: {total_no_variance_items}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be6e26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pytorch.narrative_stack.stage1.dataset import IterableConceptValueDataset, collate_with_scaler\n",
    "from models.pytorch.narrative_stack.stage1 import Stage1Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccdb991-b887-4847-968b-8d9fb4b9517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning\n",
    "\n",
    "# import os\n",
    "# import optuna\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "# from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from torch.utils.data import DataLoader\n",
    "# from utils.pytorch import get_device\n",
    "\n",
    "# device = get_device()\n",
    "\n",
    "# # === CONFIG ===\n",
    "# OUTPUT_PATH = \"data/stage1\"\n",
    "# os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "# OPTUNA_DB_PATH = os.path.join(OUTPUT_PATH, \"optuna_study.db\")\n",
    "# EPOCHS = 3\n",
    "# PATIENCE = 5\n",
    "# VAL_SPLIT = 0.2\n",
    "\n",
    "# def objective(trial):\n",
    "#     batch_size = trial.suggest_int(\"batch_size\", 8, 64, step=8)\n",
    "#     lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "#     latent_dim = trial.suggest_int(\"latent_dim\", 32, 128, step=32)\n",
    "#     dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.2, step=0.1)\n",
    "#     weight_decay = trial.suggest_float(\"weight_decay\", 1e-8, 1e-4, log=True)\n",
    "#     gradient_clip = trial.suggest_float(\"gradient_clip\", 0.0, 1.0, step=0.1)\n",
    "\n",
    "#     # # 80/20 Train/Val Split\n",
    "#     # split = int(len(scaled_tuples) * (1 - VAL_SPLIT))\n",
    "#     # train_data = scaled_tuples[:split]\n",
    "#     # val_data = scaled_tuples[split:]\n",
    "\n",
    "#      # === Sample Subset for Faster Debugging ===\n",
    "#     SAMPLE_SIZE = 500_000\n",
    "#     subset = scaled_tuples[:SAMPLE_SIZE]\n",
    "\n",
    "#     # 80/20 Train/Val Split\n",
    "#     split = int(len(subset) * (1 - VAL_SPLIT))\n",
    "#     train_data = subset[:split]\n",
    "#     val_data = subset[split:]\n",
    "\n",
    "#     train_loader = DataLoader(\n",
    "#         ConceptValueDataset(train_data, embedding_map, device=device, value_noise_std=0.005, train=True),\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=True\n",
    "#     )\n",
    "\n",
    "#     val_loader = DataLoader(\n",
    "#         ConceptValueDataset(val_data, embedding_map, device=device, value_noise_std=0.00, train=False),\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=False\n",
    "#     )\n",
    "\n",
    "#     input_dim = len(next(iter(embedding_map.values()))) + 1\n",
    "\n",
    "#     model = Stage1Autoencoder(\n",
    "#         input_dim=input_dim,\n",
    "#         latent_dim=latent_dim,\n",
    "#         dropout_rate=dropout_rate,\n",
    "#         lr=lr,\n",
    "#         batch_size=batch_size,\n",
    "#         weight_decay=weight_decay,\n",
    "#         gradient_clip=gradient_clip\n",
    "#     )\n",
    "\n",
    "#     early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, verbose=True, mode=\"min\")\n",
    "\n",
    "#     model_checkpoint = ModelCheckpoint(\n",
    "#         dirpath=OUTPUT_PATH,\n",
    "#         filename=\"best_model_trial_{trial.number}\",\n",
    "#         monitor=\"val_loss\",\n",
    "#         mode=\"min\",\n",
    "#         save_top_k=1,\n",
    "#         verbose=True\n",
    "#     )\n",
    "\n",
    "#     trainer = pl.Trainer(\n",
    "#         max_epochs=EPOCHS,\n",
    "#         logger=TensorBoardLogger(OUTPUT_PATH, name=\"stage1_autoencoder\"),\n",
    "#         callbacks=[early_stop_callback, model_checkpoint],\n",
    "#         accelerator=\"auto\",\n",
    "#         devices=1,\n",
    "#         gradient_clip_val=gradient_clip\n",
    "#     )\n",
    "\n",
    "#     trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "#     return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "# # === Optuna Study ===\n",
    "# study = optuna.create_study(direction=\"minimize\",\n",
    "#                             storage=f\"sqlite:///{OPTUNA_DB_PATH}\",\n",
    "#                             load_if_exists=True)\n",
    "# study.optimize(objective, n_trials=25)\n",
    "\n",
    "# print(\"Best params:\", study.best_params)\n",
    "# print(\"Best trial value:\", study.best_trial.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c278a0d0-19d7-47ca-95ba-5d972553b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Instantiate dataset\n",
    "# dataset = ConceptValueDataset(scaled_tuples, embedding_map)\n",
    "\n",
    "# # Sample inspection\n",
    "# sample_x, sample_y = dataset[0]\n",
    "# print(\"Sample input:\", sample_x)\n",
    "# print(\"Min:\", sample_x.min().item(), \"Max:\", sample_x.max().item())\n",
    "# print(\"Mean:\", sample_x.mean().item(), \"Std:\", sample_x.std().item())\n",
    "# print(\"Input dim:\", sample_x.shape[0], \"Target dim:\", sample_y.shape[0])\n",
    "\n",
    "# # Optional: test batch loading\n",
    "# loader = DataLoader(dataset, batch_size=4)\n",
    "# for xb, yb in loader:\n",
    "#     print(\"Batch shape:\", xb.shape)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c250f332-069a-4a69-af69-fc73b521529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = list(scaled_tuples)\n",
    "# val_data = train_data\n",
    "\n",
    "# # TODO: Get rid of most of this and just evaluate on the full train_data set\n",
    "\n",
    "# from collections import defaultdict\n",
    "# from sklearn.utils import shuffle\n",
    "# from sklearn.model_selection import train_test_split # Make sure this is imported\n",
    "# import numpy as np # For potential use, though train_test_split handles counts\n",
    "\n",
    "# # Target fraction for the validation set from each group\n",
    "# VALIDATION_FRACTION = 0.25\n",
    "# RANDOM_STATE = 42\n",
    "\n",
    "# # Assume `scaled_tuples` is the list of (concept, unit, scaled_value)\n",
    "# # generated by your previous preprocessing script. It contains ALL your scaled data.\n",
    "\n",
    "# # 1. Training data will be 100% of scaled_tuples.\n",
    "# # We create a list copy for clarity, though direct use is also possible.\n",
    "# # The DataLoader for training will typically shuffle this.\n",
    "# train_data = list(scaled_tuples)\n",
    "\n",
    "# # 2. Validation data will be a stratified sample, aiming for ~25% of each group's items.\n",
    "# val_data = []\n",
    "\n",
    "# # Group all tuples by (concept, unit) to perform stratified sampling for the validation set.\n",
    "# # This grouping is based on the full scaled_tuples list.\n",
    "# grouped_for_val_sampling = defaultdict(list)\n",
    "# for concept, unit, value in scaled_tuples: # Iterate over the full dataset\n",
    "#     grouped_for_val_sampling[(concept, unit)].append(value)\n",
    "\n",
    "# # For each group, select a VALIDATION_FRACTION sample of its values to contribute to val_data.\n",
    "# # These samples will also be part of the 100% train_data.\n",
    "# for (concept, unit), values_in_group in grouped_for_val_sampling.items():\n",
    "#     if not values_in_group: # Skip if group is empty\n",
    "#         continue\n",
    "\n",
    "#     num_samples_in_group = len(values_in_group)\n",
    "#     group_val_values = [] # To store validation values from this specific group\n",
    "\n",
    "#     if num_samples_in_group == 1:\n",
    "#         # If a group has only one sample, and we want validation data (VALIDATION_FRACTION > 0),\n",
    "#         # we include this single sample in the validation set.\n",
    "#         # This means all single-sample groups will be represented in this stratified validation set.\n",
    "#         if VALIDATION_FRACTION > 0:\n",
    "#             group_val_values = list(values_in_group) # Take the single value\n",
    "#     elif num_samples_in_group > 1:\n",
    "#         # For groups with more than one sample, use train_test_split to get\n",
    "#         # the desired fraction for the validation set from this group.\n",
    "#         # The '_group_train_dummy' part is not used for constructing val_data here.\n",
    "#         # We ensure shuffling before split if not done by train_test_split, but it shuffles by default.\n",
    "#         _group_train_dummy, sampled_val_values_for_group = train_test_split(\n",
    "#             values_in_group,       # Values from the current (concept, unit) group\n",
    "#             test_size=VALIDATION_FRACTION,\n",
    "#             random_state=RANDOM_STATE,\n",
    "#             shuffle=True          # Ensure shuffling for random sampling within the group\n",
    "#         )\n",
    "#         group_val_values = sampled_val_values_for_group\n",
    "\n",
    "#     # Extend the global val_data list with the (concept, unit, value) tuples from this group's sample\n",
    "#     if group_val_values: # If any values were selected for validation from this group\n",
    "#         val_data.extend([(concept, unit, v) for v in group_val_values])\n",
    "\n",
    "# print(f\"Total items for training (100%): {len(train_data)}\")\n",
    "# print(f\"Total items for validation (stratified ~25% sample): {len(val_data)}\")\n",
    "\n",
    "# # Now you have:\n",
    "# # - `train_data`: A list of all your (concept, unit, scaled_value) tuples (100%).\n",
    "# # - `val_data`: A list containing a stratified sample, where each (concept, unit) group\n",
    "# #               contributes approximately 25% of its items to this validation set.\n",
    "# #               The total size will be roughly 25% of the original dataset, but may vary\n",
    "# #               slightly due to per-group integer rounding and handling of small groups.\n",
    "\n",
    "# # You would then use `train_data` and `val_data` in your PyTorch DataLoaders.\n",
    "# # The `train_loader` will use `train_data` (100%) with shuffle=True.\n",
    "# # The `val_loader` will use `val_data` (the ~25% stratified sample) with shuffle=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c5e738-ff3a-4e99-bdfb-0479288faed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging, only\n",
    "\n",
    "# Example: keep only 10,000 training and 2,000 validation samples\n",
    "# train_data = train_data[:100_000]\n",
    "# val_data = val_data[:2_000]\n",
    "# val_data = train_data\n",
    "\n",
    "# print(f\"Truncated train_data: {len(train_data)}\")\n",
    "# print(f\"Truncated val_data: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd4f758-e1d6-4e8b-bbb2-0060ffbdadc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from utils.pytorch import get_device # Assuming you have this\n",
    "\n",
    "# device = get_device() # Make sure device is defined\n",
    "\n",
    "# # Define Median Scaled Value (which is 0.0 after RobustScaler)\n",
    "# median_scaled_val_tensor = torch.tensor(0.0, dtype=torch.float32, device=device)\n",
    "# print(f\"Using median_scaled_val: {median_scaled_val_tensor.item()}\")\n",
    "\n",
    "# # === Calculate Mean Embedding (mean_emb) ===\n",
    "# # (Calculation for mean_emb_tensor remains the same as before)\n",
    "# # Get all unique (concept, unit) keys from train_data\n",
    "# train_keys = set((item[0], item[1]) for item in train_data)\n",
    "# # Get the corresponding embeddings\n",
    "# train_embeddings = [embedding_map[key] for key in train_keys if key in embedding_map]\n",
    "\n",
    "# if not train_embeddings:\n",
    "#     raise ValueError(\"No valid embeddings found for training data keys!\")\n",
    "\n",
    "# # Stack embeddings into a numpy array and calculate the mean vector\n",
    "# train_embeddings_np = np.stack(train_embeddings)\n",
    "# mean_emb_numpy = np.mean(train_embeddings_np, axis=0)\n",
    "# # Convert to a tensor on the correct device\n",
    "# mean_emb_tensor = torch.tensor(mean_emb_numpy, dtype=torch.float32, device=device)\n",
    "# print(f\"Calculated mean_emb shape: {mean_emb_tensor.shape}\")\n",
    "# print(f\"Calculated mean_emb value (first 10 elements): {mean_emb_tensor[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47484ee4-1c73-48be-8024-e368d977c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "import os\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.pytorch import get_device\n",
    "import functools\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "# === CONFIG ===\n",
    "OUTPUT_PATH = \"data/stage1_23_(no_pre_dedupe)\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "OPTUNA_DB_PATH = os.path.join(OUTPUT_PATH, \"optuna_study.db\")\n",
    "EPOCHS = 1000\n",
    "PATIENCE = 20  # Compensate for long annealing period + some\n",
    "\n",
    "# ckpt_path = f\"{OUTPUT_PATH}/stage1_resume-v8.ckpt\"\n",
    "# ckpt_path = f\"{OUTPUT_PATH}/manual_resumed_checkpoint.ckpt\"\n",
    "ckpt_path = None\n",
    "\n",
    "# model = Stage1Autoencoder.load_from_checkpoint(ckpt_path,\n",
    "#     # lr=5e-5,\n",
    "#     lr=2.5e-6,\n",
    "#     # min_lr=1e-6,\n",
    "#     min_lr=1e-7\n",
    "# )\n",
    "model = Stage1Autoencoder()\n",
    "\n",
    "batch_size = model.hparams.batch_size\n",
    "gradient_clip = model.hparams.gradient_clip\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     ConceptValueDataset(train_data, embedding_map, device=device, value_noise_std=0.005, train=True),\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# val_loader = DataLoader(\n",
    "#     ConceptValueDataset(val_data, embedding_map, device=device, value_noise_std=0.00, train=False),\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False\n",
    "# )\n",
    "train_loader = DataLoader(\n",
    "    IterableConceptValueDataset(WEBSOCKET_ADDRESS, internal_batch_size=16, return_scaler=True, shuffle=True),\n",
    "    batch_size=batch_size,\n",
    "    # shuffle=True, # Moved to dataset\n",
    "    collate_fn=collate_with_scaler,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    IterableConceptValueDataset(WEBSOCKET_ADDRESS, internal_batch_size=16, return_scaler=True, shuffle=False),\n",
    "    batch_size=batch_size,\n",
    "    # shuffle=False, # Moved to dataset\n",
    "    collate_fn=collate_with_scaler,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "# input_dim = len(next(iter(embedding_map.values()))) + 1\n",
    "sample_embedding = us_gaap_store.lookup_by_index(0)[\"embedding\"]\n",
    "input_dim = sample_embedding.shape[0]\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss_epoch\", patience=PATIENCE, verbose=True, mode=\"min\"\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    dirpath=OUTPUT_PATH,\n",
    "    filename=\"stage1_resume\",\n",
    "    monitor=\"val_loss_epoch\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    logger=TensorBoardLogger(OUTPUT_PATH, name=\"stage1_autoencoder\"),\n",
    "    callbacks=[early_stop_callback, model_checkpoint],\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    gradient_clip_val=gradient_clip,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    #\n",
    "    # ckpt_path=ckpt_path # TODO: Uncomment if resuming training AND wanting to restore existing model configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5769c4-97ea-4638-a2bc-de631ce1f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_checkpoint(f\"{OUTPUT_PATH}/manual_resumed_checkpoint.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca87a09-65c9-42ee-9310-42558e6df0dc",
   "metadata": {},
   "source": [
    "# Conceptual Draft\n",
    "\n",
    "Stage 1 learns semantic+quantitative embeddings for individual concept/unit/value triplets.\n",
    "\n",
    "Stage 2 learns how to aggregate and contextualize those embeddings into higher-order units (i.e., financial statements).\n",
    "\n",
    "Stage 3 learns how to model temporal dynamics and structural evolution across filings â€” a full hierarchy of understanding.\n",
    "\n",
    "This pipeline could encode an entire company's financial narrative into vector space.\n",
    "\n",
    "Itâ€™s structured like language modeling, but for accounting â€” and thatâ€™s what makes it potentially groundbreaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70426775-f40b-4b32-ac8f-2957ae841798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# # Where correlation matrix is on the full z, and the `corr_value` is derived specifically from the input value dimension\n",
    "# # IMPORTANT: This should only be used with this \"stage 1\" model\n",
    "# def analyze_latent_correlation_matrix_streaming(model, val_loader, device):\n",
    "#     model.eval()\n",
    "#     model.to(device)\n",
    "\n",
    "#     latent_dim = model.hparams.latent_dim\n",
    "\n",
    "#     count = 0\n",
    "#     mean_z = np.zeros(latent_dim)\n",
    "#     m2_z = np.zeros(latent_dim)\n",
    "#     cov_z = np.zeros((latent_dim, latent_dim))\n",
    "\n",
    "#     # For value correlation\n",
    "#     mean_val = 0.0\n",
    "#     m2_val = 0.0\n",
    "#     cov_val = np.zeros(latent_dim)\n",
    "\n",
    "#     for batch in val_loader:\n",
    "#         x, y, _ = batch\n",
    "#         x = x.to(device)\n",
    "#         y = y.to(device)\n",
    "\n",
    "#         z = model.encode(x).detach().cpu().numpy()  # [B, D]\n",
    "#         v = y[:, -1].detach().cpu().numpy()         # [B]\n",
    "\n",
    "#         for zi, vi in zip(z, v):\n",
    "#             count += 1\n",
    "\n",
    "#             # === Update latent stats (Welford) ===\n",
    "#             delta_z = zi - mean_z\n",
    "#             mean_z += delta_z / count\n",
    "#             m2_z += delta_z * (zi - mean_z)\n",
    "\n",
    "#             # === Update cov_z (outer product) ===\n",
    "#             cov_z += np.outer(delta_z, zi - mean_z)\n",
    "\n",
    "#             # === Update value stats ===\n",
    "#             delta_v = vi - mean_val\n",
    "#             mean_val += delta_v / count\n",
    "#             m2_val += delta_v * (vi - mean_val)\n",
    "\n",
    "#             # === Update cov_val ===\n",
    "#             cov_val += delta_z * (vi - mean_val)\n",
    "\n",
    "#         # break\n",
    "\n",
    "#     var_z = m2_z / (count - 1)\n",
    "#     var_val = m2_val / (count - 1)\n",
    "#     cov_z /= (count - 1)\n",
    "#     cov_val /= (count - 1)\n",
    "\n",
    "#     std_z = np.sqrt(var_z + 1e-8)\n",
    "#     std_val = np.sqrt(var_val + 1e-8)\n",
    "\n",
    "#     corr_matrix = cov_z / (std_z[:, None] * std_z[None, :])\n",
    "#     corr_value = cov_val / (std_z * std_val)\n",
    "\n",
    "#     return corr_matrix, corr_value\n",
    "\n",
    "\n",
    "# print(\"\\n=== Computing latent correlation matrix... ===\")\n",
    "# corr_matrix, corr_value = analyze_latent_correlation_matrix_streaming(model, val_loader, device=device)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_correlation_matrix(corr_matrix, title=\"Latent Dimension Correlation Matrix\"):\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     plt.imshow(corr_matrix, cmap='coolwarm', interpolation='nearest', vmin=-1, vmax=1)\n",
    "#     plt.colorbar(shrink=0.5)\n",
    "#     plt.title(title)\n",
    "#     plt.xlabel(\"Latent Dim\")\n",
    "#     plt.ylabel(\"Latent Dim\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# plot_correlation_matrix(corr_matrix)\n",
    "\n",
    "# top_k = 128\n",
    "\n",
    "# def print_top_latent_correlations(corr_matrix, top_k=top_k):\n",
    "#     dim = corr_matrix.shape[0]\n",
    "#     pairs = []\n",
    "\n",
    "#     for i in range(dim):\n",
    "#         for j in range(i + 1, dim):\n",
    "#             corr = corr_matrix[i, j]\n",
    "#             pairs.append(((i, j), corr))\n",
    "\n",
    "#     top_corrs = sorted(pairs, key=lambda x: -abs(x[1]))[:top_k]\n",
    "\n",
    "#     print(f\"\\nTop {top_k} most correlated latent dimension pairs:\")\n",
    "#     for (i, j), corr in top_corrs:\n",
    "#         print(f\"z[{i:03d}] â†” z[{j:03d}]: corr = {corr:.4f}\")\n",
    "\n",
    "\n",
    "# print_top_latent_correlations(corr_matrix, top_k=top_k)\n",
    "\n",
    "\n",
    "# top_dims = sorted(enumerate(corr_value), key=lambda x: -abs(x[1]))[:top_k]\n",
    "# print(\"\\nTop latent dimensions most correlated with scaled value:\")\n",
    "# for i, c in top_dims:\n",
    "#     print(f\"z[{i:03d}]: corr = {c:.4f}\")\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(12, 4))\n",
    "# plt.plot(np.sort(np.abs(corr_value))[::-1], marker='o')\n",
    "# plt.title(\"Absolute Correlation of Latent Dims with Value\")\n",
    "# plt.xlabel(\"Sorted Latent Dimension\")\n",
    "# plt.ylabel(\"Absolute Correlation\")\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
