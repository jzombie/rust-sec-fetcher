{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6ff429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from simd_r_drive import DataStore\n",
    "from models.pytorch.narrative_stack.common import UsGaapStore\n",
    "\n",
    "from simd_r_drive_ws_client import DataStoreWsClient\n",
    "\n",
    "WEBSOCKET_ADDRESS = \"127.0.0.1:58274\"\n",
    "data_store = DataStoreWsClient(WEBSOCKET_ADDRESS)\n",
    "us_gaap_store = UsGaapStore(data_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_embedding = us_gaap_store.lookup_by_index(0)[\"embedding\"]\n",
    "\n",
    "sample_embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98055128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Move to preprocessing\n",
    "\n",
    "# # UMAP visualization\n",
    "# umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, metric=\"cosine\")\n",
    "# umap_2d = umap_model.fit_transform(compressed)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(umap_2d[:, 0], umap_2d[:, 1], c=labels, cmap=\"tab10\", s=5)\n",
    "# plt.title(\"Concept/UOM Embeddings Clustered\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e52bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({\n",
    "#     \"concept\": [c for c, _ in concept_unit_pairs],\n",
    "#     \"unit\": [u for _, u in concept_unit_pairs],\n",
    "#     \"cluster\": labels\n",
    "# })\n",
    "# grouped = df.groupby(\"cluster\")\n",
    "\n",
    "# for cluster_id, group in grouped:\n",
    "#     print(f\"\\nCluster {cluster_id} ({len(group)} items):\")\n",
    "#     print(group.head(10).to_string(index=False))\n",
    "\n",
    "# noise = df[df[\"cluster\"] == -1]\n",
    "\n",
    "# print(f\"Noise points: {len(noise)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_points = df[df[\"cluster\"] == -1][[\"concept\", \"unit\"]].reset_index(drop=True)\n",
    "\n",
    "# noise_points.to_csv(\"noise_points.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a051cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Save both embeddings and tuples\n",
    "# np.savez_compressed(\n",
    "#     \"data/stage1_latents.npz\",\n",
    "#     keys=np.array([f\"{c}::{u}\" for c, u in concept_unit_pairs]),\n",
    "#     embeddings=compressed,\n",
    "#     concept_unit_value_tuples=np.array(concept_unit_value_tuples, dtype=object)\n",
    "# )\n",
    "\n",
    "# print(f\"✅ Saved {len(concept_unit_value_tuples):,} tuples and {len(compressed):,} embeddings to 'stage1_latents.npz'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b489720e-de8d-4a98-b6ba-65e24051c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Move\n",
    "\n",
    "# # Debug scaled stats\n",
    "\n",
    "# from collections import defaultdict\n",
    "# import numpy as np\n",
    "\n",
    "# # Reconstruct per (concept, unit) from scaled_tuples\n",
    "# reconstructed = defaultdict(list)\n",
    "# all_vals = []\n",
    "\n",
    "# for concept, unit, val in scaled_tuples:\n",
    "#     key = (concept, unit)\n",
    "#     reconstructed[key].append(val)\n",
    "#     all_vals.append(val)\n",
    "\n",
    "# total_items = 0\n",
    "# total_no_variance_items = 0\n",
    "\n",
    "# # Analyze per group\n",
    "# for key, vals in reconstructed.items():\n",
    "#     vals_np = np.array(vals)\n",
    "\n",
    "#     if not np.all(np.isfinite(vals_np)):\n",
    "#         print(f\"[BAD SCALE] {key} has non-finite scaled values!\")\n",
    "#         print(\"  Sample:\", vals_np[:5])\n",
    "#         continue\n",
    "\n",
    "#     max_val = np.max(vals_np)\n",
    "#     min_val = np.min(vals_np)\n",
    "#     mean_val = np.mean(vals_np)\n",
    "\n",
    "#     if max_val > 1e6 or min_val < -1e6:\n",
    "#         print(f\"[OUTLIER SCALE] {key} range is extreme:\")\n",
    "#         print(\"  min:\", min_val, \"max:\", max_val, \"mean:\", mean_val)\n",
    "\n",
    "#     if np.allclose(max_val, min_val):\n",
    "#         # print(f\"[FLAT SCALE] {key} has no variance ({len(vals)} item(s)):\")\n",
    "#         # print(\"  value:\", max_val)\n",
    "#         total_no_variance_items += len(vals)\n",
    "\n",
    "#     total_items += len(vals)\n",
    "\n",
    "# # Global stats\n",
    "# all_vals_np = np.array(all_vals)\n",
    "\n",
    "# print(\"\\n=== GLOBAL STATS ===\")\n",
    "# print(f\"Global max: {np.max(all_vals_np)}\")\n",
    "# print(f\"Global min: {np.min(all_vals_np)}\")\n",
    "# print(f\"Global mean: {np.mean(all_vals_np)}\")\n",
    "# print(f\"Global std: {np.std(all_vals_np)}\")\n",
    "\n",
    "# print(f\"Total items: {total_items}\")\n",
    "# print(f\"Total no variance items: {total_no_variance_items}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be6e26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pytorch.narrative_stack.stage1.dataset import IterableConceptValueDataset, collate_with_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b584bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# from torchmetrics.regression import R2Score\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class AggregateStats:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self._eps = 1e-8\n",
    "        self._per_tag = defaultdict(\n",
    "            lambda: {\n",
    "                \"mae_sum\": 0.0,\n",
    "                \"abs_sum\": 0.0,\n",
    "                # old: \"r2\": R2Score().to(device),\n",
    "                \"n\": 0,\n",
    "                \"sum_y_true\": 0.0,\n",
    "                \"sum_y_pred\": 0.0,\n",
    "                \"sum_y_true2\": 0.0,\n",
    "                \"sum_y_pred2\": 0.0,\n",
    "                \"sum_y_true_y_pred\": 0.0,\n",
    "            }\n",
    "        )\n",
    "        self.z_sum = 0.0\n",
    "        self.z_sq_sum = 0.0\n",
    "        self.z_count = 0\n",
    "\n",
    "    # TODO: Remove old, slower update method\n",
    "    # def update(self, tags, y_pred_batch, y_true_batch, z_norm_batch):\n",
    "    #     \"\"\"\n",
    "    #     tags: List[Tuple[str, str]]\n",
    "    #     y_pred_batch, y_true_batch, z_norm_batch: Tensors of shape [B]\n",
    "    #     \"\"\"\n",
    "    #     y_pred_batch = y_pred_batch.detach().cpu()\n",
    "    #     y_true_batch = y_true_batch.detach().cpu()\n",
    "    #     z_norm_batch = z_norm_batch.detach().cpu()\n",
    "\n",
    "    #     for i, tag in enumerate(tags):\n",
    "    #         stats = self._per_tag[tag]\n",
    "    #         abs_err = torch.abs(y_pred_batch[i] - y_true_batch[i]).item()\n",
    "    #         abs_target = torch.abs(y_true_batch[i]).item()\n",
    "\n",
    "    #         stats[\"mae_sum\"] += abs_err\n",
    "    #         stats[\"abs_sum\"] += abs_target\n",
    "\n",
    "    #         # Manual R² computation is used here instead of torchmetrics.R2Score\n",
    "    #         # to address performance issues specific to this use case—\n",
    "    #         # namely, avoiding per-sample `.update()` overhead and GPU sync stalls\n",
    "    #         # during large-scale per-tag aggregation. This optimization is targeted\n",
    "    #         # and does not imply that torchmetrics.R2Score is unsuitable\n",
    "    #         # for other scenarios or tasks.\n",
    "\n",
    "    #         # stats[\"r2\"].update(y_pred_batch[i].unsqueeze(0), y_true_batch[i].unsqueeze(0))\n",
    "    #         stats[\"n\"] += 1\n",
    "    #         yt = y_true_batch[i].item()\n",
    "    #         yp = y_pred_batch[i].item()\n",
    "    #         stats[\"sum_y_true\"] += yt\n",
    "    #         stats[\"sum_y_pred\"] += yp\n",
    "    #         stats[\"sum_y_true2\"] += yt * yt\n",
    "    #         stats[\"sum_y_pred2\"] += yp * yp\n",
    "    #         stats[\"sum_y_true_y_pred\"] += yt * yp\n",
    "\n",
    "    #     self.z_sum += z_norm_batch.sum().item()\n",
    "    #     self.z_sq_sum += (z_norm_batch ** 2).sum().item()\n",
    "    #     self.z_count += z_norm_batch.size(0)\n",
    "\n",
    "    def update(self, tags, y_pred_batch, y_true_batch, z_norm_batch):\n",
    "        \"\"\"\n",
    "        tags: List[Tuple[str, str]]\n",
    "        y_pred_batch, y_true_batch, z_norm_batch: Tensors of shape [B]\n",
    "        \"\"\"\n",
    "        y_pred = y_pred_batch.detach().cpu().numpy().astype(np.float32)\n",
    "        y_true = y_true_batch.detach().cpu().numpy().astype(np.float32)\n",
    "        z_norm = z_norm_batch.detach().cpu().numpy().astype(np.float32)\n",
    "        tags = np.array(tags)\n",
    "\n",
    "        # Group indices by unique tag\n",
    "        unique_tags, tag_indices = np.unique(tags, return_inverse=True)\n",
    "        # num_samples = len(y_true)\n",
    "\n",
    "        for i, tag in enumerate(unique_tags):\n",
    "            idxs = np.where(tag_indices == i)[0]\n",
    "            yp = y_pred[idxs]\n",
    "            yt = y_true[idxs]\n",
    "\n",
    "            mae_sum = np.abs(yp - yt).sum()\n",
    "            abs_sum = np.abs(yt).sum()\n",
    "\n",
    "            stats = self._per_tag[tuple(tag)]\n",
    "            stats[\"mae_sum\"] += mae_sum\n",
    "            stats[\"abs_sum\"] += abs_sum\n",
    "\n",
    "            # Manual R² computation is used here instead of torchmetrics.R2Score\n",
    "            # to address performance issues specific to this use case—\n",
    "            # namely, avoiding per-sample `.update()` overhead and GPU sync stalls\n",
    "            # during large-scale per-tag aggregation. This optimization is targeted\n",
    "            # and does not imply that torchmetrics.R2Score is unsuitable\n",
    "            # for other scenarios or tasks.\n",
    "\n",
    "            stats[\"n\"] += len(idxs)\n",
    "            stats[\"sum_y_true\"] += yt.sum()\n",
    "            stats[\"sum_y_pred\"] += yp.sum()\n",
    "            # stats[\"sum_y_true2\"] += np.sum(yt ** 2) # TODO: Fix potential RuntimeWarning: overflow encountered in square\n",
    "            # stats[\"sum_y_pred2\"] += np.sum(yp ** 2) # TODO: Fix potential RuntimeWarning: overflow encountered in square\n",
    "            stats[\"sum_y_true2\"] += np.sum(np.square(yt.astype(np.float64)))\n",
    "            stats[\"sum_y_pred2\"] += np.sum(np.square(yp.astype(np.float64)))\n",
    "\n",
    "            # TODO: Fix: RuntimeWarning: overflow encountered in multiply:  stats[\"sum_y_true_y_pred\"] += np.sum(yt * yp)\n",
    "             # Solution with clamping:\n",
    "            # 1. Perform multiplication in float64 to avoid overflow during product calculation\n",
    "            # 2. Clamp the result to FLOAT32_MAX (or any desired max value)\n",
    "            # 3. Sum the clamped values\n",
    "            # product_yt_yp_float64 = yt.astype(np.float64) * yp.astype(np.float64)\n",
    "            # clamped_product = np.clip(product_yt_yp_float64, a_min=None, a_max=FLOAT32_MAX)\n",
    "            # stats[\"sum_y_true_y_pred\"] += np.sum(clamped_product)\n",
    "            \n",
    "            stats[\"sum_y_true_y_pred\"] += np.sum(yt * yp)\n",
    "\n",
    "        self.z_sum += z_norm.sum()\n",
    "        self.z_sq_sum += np.sum(z_norm**2)\n",
    "        self.z_count += z_norm.shape[0]\n",
    "\n",
    "    def median_relative_mae(self):\n",
    "        vals = []\n",
    "        for v in self._per_tag.values():\n",
    "            if v[\"abs_sum\"] > 0:\n",
    "                vals.append(v[\"mae_sum\"] / (v[\"abs_sum\"] + self._eps))\n",
    "        return float(np.median(vals)) if vals else 0.0\n",
    "\n",
    "    def worst_median_relative_mae(self, top_frac=0.05):\n",
    "        vals = []\n",
    "        for v in self._per_tag.values():\n",
    "            if v[\"abs_sum\"] > 0:\n",
    "                rel_mae = v[\"mae_sum\"] / (v[\"abs_sum\"] + self._eps)\n",
    "                vals.append(rel_mae)\n",
    "\n",
    "        if not vals:\n",
    "            return 0.0\n",
    "\n",
    "        vals.sort(reverse=True)  # higher MAE is worse\n",
    "        k = max(1, int(len(vals) * top_frac))\n",
    "        return float(np.median(vals[:k]))\n",
    "\n",
    "    def _compute_r2_values(self):\n",
    "        \"\"\"\n",
    "        Compute R² values for all tags with at least 2 samples.\n",
    "        Filters out NaN or infinite results caused by overflow or invalid math.\n",
    "        Optimized for speed using NumPy vectorization.\n",
    "        \"\"\"\n",
    "        # 1. Extract data into NumPy arrays from the defaultdict values\n",
    "        # Filter out entries where n < 2 upfront\n",
    "        valid_data_dicts = [v for v in self._per_tag.values() if v[\"n\"] >= 2]\n",
    "\n",
    "        if not valid_data_dicts:\n",
    "            return []\n",
    "\n",
    "        # Create a dictionary of arrays for vectorized operations\n",
    "        # Ensure all data is float64 from the start for calculations\n",
    "        n_fp = np.array([d[\"n\"] for d in valid_data_dicts], dtype=np.float64)\n",
    "        sum_y_true = np.array([d[\"sum_y_true\"] for d in valid_data_dicts], dtype=np.float64)\n",
    "        sum_y_true2 = np.array([d[\"sum_y_true2\"] for d in valid_data_dicts], dtype=np.float64)\n",
    "        sum_y_pred2 = np.array([d[\"sum_y_pred2\"] for d in valid_data_dicts], dtype=np.float64)\n",
    "        sum_y_true_y_pred = np.array([d[\"sum_y_true_y_pred\"] for d in valid_data_dicts], dtype=np.float64)\n",
    "        eps = np.float64(self._eps) # eps remains a scalar\n",
    "\n",
    "        # 2. Perform vectorized calculations\n",
    "        mean_y = sum_y_true / n_fp\n",
    "        mean_y_sq = np.square(mean_y) # Using np.square for consistency and potential overflow handling if values are large\n",
    "\n",
    "        # Initialize a mask for valid computations\n",
    "        valid_mask = np.isfinite(mean_y_sq)\n",
    "\n",
    "        product = n_fp * mean_y_sq\n",
    "        valid_mask &= np.isfinite(product) # Update mask\n",
    "\n",
    "        ss_tot = sum_y_true2 - product\n",
    "        ss_res = sum_y_pred2 - 2 * sum_y_true_y_pred + sum_y_true2\n",
    "\n",
    "        denom = ss_tot + eps\n",
    "\n",
    "        # Apply conditions using boolean indexing\n",
    "        valid_denom_mask = (denom > 0) & np.isfinite(denom)\n",
    "        final_mask = valid_mask & valid_denom_mask\n",
    "\n",
    "        # Apply the combined mask to the arrays before division\n",
    "        ss_res_filtered = ss_res[final_mask]\n",
    "        denom_filtered = denom[final_mask]\n",
    "\n",
    "        # Avoid division by zero by ensuring denom_filtered is not empty if final_mask is True\n",
    "        if denom_filtered.size == 0:\n",
    "            return []\n",
    "\n",
    "        r2_values = 1.0 - (ss_res_filtered / denom_filtered)\n",
    "\n",
    "        # 3. Filter for finite r2 values and return as a list\n",
    "        return r2_values[np.isfinite(r2_values)].tolist()\n",
    "\n",
    "    def median_r2(self):\n",
    "        \"\"\"\n",
    "        Compute the median R² across all valid tags.\n",
    "        \"\"\"\n",
    "        vals = self._compute_r2_values()\n",
    "        return float(np.median(vals)) if vals else 0.0\n",
    "\n",
    "    def worst_median_r2(self, bottom_frac=0.05):\n",
    "        \"\"\"\n",
    "        Compute the median R² among the bottom `bottom_frac` of tags.\n",
    "        \"\"\"\n",
    "        vals = self._compute_r2_values()\n",
    "        if not vals:\n",
    "            return 0.0\n",
    "        vals.sort()\n",
    "        k = max(1, int(len(vals) * bottom_frac))\n",
    "        return float(np.median(vals[:k]))\n",
    "\n",
    "    def z_norm_mean_std(self):\n",
    "        if self.z_count == 0:\n",
    "            return 0.0, 0.0\n",
    "\n",
    "        mean = self.z_sum / max(self.z_count, 1)\n",
    "        mean_sq = self.z_sq_sum / max(self.z_count, 1)\n",
    "        var = max(mean_sq - mean**2, 0.0)\n",
    "        return mean, var**0.5\n",
    "\n",
    "    def reset(self):\n",
    "        self.__init__(self.device)\n",
    "\n",
    "\n",
    "class ValueAttentionModulator(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(1, emb_dim)  # scalar → query\n",
    "        self.key = nn.Linear(emb_dim, emb_dim)  # x_emb → key\n",
    "        self.value = nn.Linear(emb_dim, emb_dim)  # x_emb → value\n",
    "        self.scale = emb_dim**0.5\n",
    "\n",
    "    def forward(self, x_emb, x_val):\n",
    "        q = self.query(x_val)  # [B, D]\n",
    "        k = self.key(x_emb)  # [B, D]\n",
    "        v = self.value(x_emb)  # [B, D]\n",
    "\n",
    "        attn = torch.sum(q * k, dim=-1, keepdim=True) / self.scale  # [B, 1]\n",
    "        attn_weights = torch.sigmoid(attn)  # [B, 1] ∈ (0, 1)\n",
    "\n",
    "        return attn_weights * v + (1 - attn_weights) * x_emb\n",
    "\n",
    "\n",
    "class EncoderWithAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, latent_dim, dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(emb_dim * 4),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "        )\n",
    "        self.attn = ValueAttentionModulator(emb_dim * 4)\n",
    "        self.bottleneck = nn.Linear(emb_dim * 4, latent_dim)\n",
    "\n",
    "    def forward(self, x_emb, x_val):\n",
    "        h = self.expand(x_emb)  # [B, 4D]\n",
    "        h = self.attn(h, x_val)  # scalar-conditioned attention\n",
    "        z = self.bottleneck(h)  # [B, latent_dim]\n",
    "        return z\n",
    "\n",
    "\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, latent_dim, emb_dim, dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(latent_dim * 4),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "        )\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=latent_dim * 4, num_heads=4, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Separate heads\n",
    "        self.emb_head = nn.Sequential(\n",
    "            nn.Linear(latent_dim * 4, emb_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim * 2, emb_dim),\n",
    "        )\n",
    "        self.val_head = nn.Sequential(\n",
    "            nn.Linear(latent_dim * 4, emb_dim), nn.GELU(), nn.Linear(emb_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        z: [B, latent_dim]\n",
    "        Returns:\n",
    "            recon_emb: [B, emb_dim]\n",
    "            recon_val: [B, 1]\n",
    "        \"\"\"\n",
    "        h = self.expand(z)  # [B, 4D]\n",
    "        h_attn, _ = self.attn(h.unsqueeze(1), h.unsqueeze(1), h.unsqueeze(1))\n",
    "        h = h_attn.squeeze(1)\n",
    "\n",
    "        recon_emb = self.emb_head(h)\n",
    "        recon_val = self.val_head(h)\n",
    "        return recon_emb, recon_val\n",
    "\n",
    "\n",
    "# LightningModule\n",
    "class Stage1Autoencoder(pl.LightningModule):\n",
    "    EPSILON = torch.finfo(torch.float32).eps\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=244,\n",
    "        latent_dim=256,\n",
    "        encoder_dropout_rate=0.0,\n",
    "        value_dropout_rate=0.0,\n",
    "        # lr=0.00023072200683712404,\n",
    "        lr=5e-5,\n",
    "        min_lr=1e-6,\n",
    "        # lr=0.00002307,\n",
    "        batch_size=64,\n",
    "        gradient_clip=0.5,\n",
    "        alpha_embed=1.0,\n",
    "        alpha_value=1.0,\n",
    "        # embedding_noise_std=0.0, # 0.02 is roughly ~0.951 cosine sim difference for 243 dimeensions; 0.01 is roughly ~0.99\n",
    "        weight_decay=5.220603379116996e-07,\n",
    "        lr_annealing_epochs=15,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters(ignore=[\"median_scaled_val\", \"mean_emb\"])\n",
    "\n",
    "        # --> Register mean_s and mean_emb as buffers <--\n",
    "        # self.register_buffer(\"median_scaled_val\", median_scaled_val)\n",
    "        # self.register_buffer(\"mean_emb\", mean_emb)\n",
    "\n",
    "        # self.value_proj = nn.Sequential(\n",
    "        #     nn.Linear(1, 32),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(32, self.hparams.latent_dim),\n",
    "        #     nn.LayerNorm(self.hparams.latent_dim)\n",
    "        # )\n",
    "\n",
    "        # May 1, 2025 original\n",
    "        # self.value_proj = nn.Sequential(\n",
    "        #     nn.Linear(1, 32),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(32, latent_dim),\n",
    "        #     nn.LayerNorm(latent_dim)\n",
    "        # )\n",
    "\n",
    "        # self.value_proj = nn.Sequential(\n",
    "        #     nn.Linear(1, 32),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(32, 64),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(64, latent_dim),\n",
    "        #     nn.LayerNorm(latent_dim)\n",
    "        # )\n",
    "\n",
    "        # self.attended_interaction = nn.Sequential(\n",
    "        #     nn.Linear(latent_dim * 2, latent_dim * 2),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(latent_dim * 2, latent_dim),\n",
    "        #     nn.LayerNorm(latent_dim)\n",
    "        # )\n",
    "\n",
    "        # self.encoder = nn.Sequential(\n",
    "        #     nn.Linear(input_dim - 1 + self.hparams.latent_dim, 256),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Dropout(p=dropout_rate),\n",
    "        #     nn.Linear(256, latent_dim)\n",
    "        # )\n",
    "\n",
    "        # self.gate = nn.Sequential(\n",
    "        #     nn.Linear(latent_dim * 2, latent_dim),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(latent_dim, latent_dim),\n",
    "        #     nn.Sigmoid()\n",
    "        # )\n",
    "\n",
    "        # self.fusion_logits = nn.Parameter(torch.zeros(3))\n",
    "        # self.fusion_dim = latent_dim * 3\n",
    "        # self.post_fusion_norm = nn.LayerNorm(self.fusion_dim)\n",
    "\n",
    "        # self.joint_input_dim = (input_dim - 1) * 2\n",
    "        # self.joint_input_norm = nn.LayerNorm(self.joint_input_dim)\n",
    "\n",
    "        # self.encoder = nn.Sequential(\n",
    "        #     nn.Linear(self.joint_input_dim, latent_dim * 4),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Dropout(p=dropout_rate),\n",
    "        #     nn.Linear(latent_dim * 4, latent_dim * 2),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Dropout(p=dropout_rate),\n",
    "        #     nn.Linear(latent_dim * 2, latent_dim)\n",
    "        # )\n",
    "\n",
    "        # self.encoder = nn.Sequential(\n",
    "        #     # nn.Linear(self.joint_input_dim, latent_dim * 4),\n",
    "        #     # nn.GELU(),\n",
    "        #     # nn.LayerNorm(latent_dim * 4),\n",
    "        #     # nn.Dropout(p=encoder_dropout_rate),\n",
    "\n",
    "        #     nn.Linear(input_dim - 1, input_dim * 4),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.LayerNorm(input_dim * 4),\n",
    "        #     nn.Dropout(p=encoder_dropout_rate),\n",
    "\n",
    "        #     nn.Linear(input_dim * 4, latent_dim)  # Bottleneck\n",
    "        # )\n",
    "\n",
    "        self.encoder = EncoderWithAttention(\n",
    "            emb_dim=input_dim - 1,\n",
    "            latent_dim=latent_dim,\n",
    "            dropout_rate=encoder_dropout_rate,\n",
    "        )\n",
    "\n",
    "        self.decoder = DecoderWithAttention(\n",
    "            latent_dim=latent_dim,\n",
    "            emb_dim=input_dim - 1,\n",
    "            dropout_rate=value_dropout_rate,\n",
    "        )\n",
    "\n",
    "        # self.embedding_decoder = DecoderWithAttention(\n",
    "        #     latent_dim=latent_dim,\n",
    "        #     emb_dim=input_dim - 1,\n",
    "        #     dropout_rate=encoder_dropout_rate\n",
    "        # )\n",
    "\n",
    "        # self.embedding_decoder = nn.Sequential(\n",
    "        #     nn.Linear(latent_dim, (input_dim - 1) * 2),\n",
    "        #     nn.GELU(),\n",
    "        #     # nn.Dropout(p=dropout_rate),\n",
    "        #     nn.Linear((input_dim - 1) * 2, input_dim - 1)\n",
    "        # )\n",
    "\n",
    "        # May 1, 2025 original\n",
    "        # self.value_decoder = nn.Sequential(\n",
    "        #     nn.Linear(latent_dim, 32),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Dropout(p=value_dropout_rate),\n",
    "        #     nn.Linear(32, 1)\n",
    "        # )\n",
    "\n",
    "        # self.value_decoder = nn.Sequential(\n",
    "        #     nn.Linear(latent_dim, (input_dim - 1) * 2),\n",
    "        #     nn.GELU(),\n",
    "        #     # nn.Dropout(p=dropout_rate),\n",
    "        #     nn.Linear((input_dim - 1) * 2, 1)\n",
    "        # )\n",
    "\n",
    "        # self.loss_fn = nn.MSELoss()\n",
    "        self.loss_fn = nn.L1Loss()  # MAELoss\n",
    "\n",
    "        self._agg_train_stats = self.create_aggregate_stats()\n",
    "        self._agg_val_stats = self.create_aggregate_stats()\n",
    "\n",
    "    def create_aggregate_stats(self):\n",
    "        return AggregateStats(self.device)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x_emb = x[:, :-1]\n",
    "        x_val = x[:, -1].unsqueeze(1)\n",
    "        z = self.encoder(x_emb, x_val)\n",
    "        return F.normalize(z, p=2, dim=1)\n",
    "\n",
    "    # def encode(self, x):\n",
    "    #     # x shape: [batch_size, input_dim]\n",
    "    #     x_emb = x[:, :-1] # Non-scaled embeddings\n",
    "    #     x_val = x[:, -1].unsqueeze(1) # Scaled values\n",
    "\n",
    "    #     # Inject Gaussian noise into embedding (during training only)\n",
    "    #     # if self.training:\n",
    "    #     #     x_emb = x_emb + torch.randn_like(x_emb) * self.hparams.embedding_noise_std\n",
    "\n",
    "    #     # val_proj = self.value_proj(x_val)\n",
    "    #     value_modulated = self.value_attention(x_emb, x_val)\n",
    "    #     joint_input = torch.cat([x_emb, value_modulated], dim=1)\n",
    "\n",
    "    #     # joint_input = torch.cat([x_emb, value_weighted_x_emb], dim=1)\n",
    "    #     # joint_input = self.joint_input_norm(joint_input)\n",
    "\n",
    "    #     z = self.encoder(joint_input)\n",
    "\n",
    "    #     # Apply L2 normalization along the feature dimension (dim=1)\n",
    "    #     # p=2 is the default for L2 norm, but explicitly stated for clarity\n",
    "    #     z = F.normalize(z, p=2, dim=1)\n",
    "\n",
    "    #     return z\n",
    "\n",
    "    # def decode(self, z):\n",
    "    #     recon_emb = self.embedding_decoder(z)\n",
    "    #     recon_val = self.value_decoder(z)\n",
    "\n",
    "    #     return recon_emb, recon_val\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "\n",
    "        recon_emb, recon_val = self.decode(z)\n",
    "        return recon_emb, recon_val, z\n",
    "\n",
    "    def compute_losses(self, x, target, scaler, concept_units, train=False):\n",
    "        recon_emb, recon_val, z = self(x)\n",
    "\n",
    "        target_emb = target[:, :-1]\n",
    "        target_val = target[:, -1].unsqueeze(1)\n",
    "\n",
    "        # TODO: Figure out how to work out a potentially missing scaler\n",
    "        if scaler and isinstance(scaler, (list, tuple)):\n",
    "            recon_val_np = recon_val.detach().cpu().numpy()\n",
    "            target_val_np = target_val.detach().cpu().numpy()\n",
    "\n",
    "            # Inverse transform per sample\n",
    "            recon_val_orig = np.stack(\n",
    "                [\n",
    "                    s.inverse_transform(r.reshape(-1, 1)).flatten()\n",
    "                    for s, r in zip(scaler, recon_val_np)\n",
    "                ]\n",
    "            )\n",
    "            target_val_orig = np.stack(\n",
    "                [\n",
    "                    s.inverse_transform(t.reshape(-1, 1)).flatten()\n",
    "                    for s, t in zip(scaler, target_val_np)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            recon_val_orig = torch.tensor(\n",
    "                recon_val_orig, dtype=torch.float32, device=recon_val.device\n",
    "            )\n",
    "            target_val_orig = torch.tensor(\n",
    "                target_val_orig, dtype=torch.float32, device=target_val.device\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\"Scaler not implemented\")\n",
    "\n",
    "        # non-scaled\n",
    "        embedding_loss = self.loss_fn(recon_emb, target_emb)\n",
    "\n",
    "        # scaled\n",
    "        value_loss = self.loss_fn(recon_val, target_val)\n",
    "\n",
    "        total_loss = (\n",
    "            self.hparams.alpha_embed * embedding_loss\n",
    "            + self.hparams.alpha_value * value_loss\n",
    "        )\n",
    "\n",
    "        # non-scaled\n",
    "        cos_sim_emb = cosine_similarity(recon_emb, target_emb, dim=1).mean()\n",
    "        euclidean_dist_emb = torch.norm(recon_emb - target_emb, dim=1).mean()\n",
    "\n",
    "        # non-scaled\n",
    "        z_norm = torch.norm(z, dim=1)\n",
    "\n",
    "        agg_stats = self._agg_train_stats if train else self._agg_val_stats\n",
    "        agg_stats.update(\n",
    "            tags=concept_units,\n",
    "            y_pred_batch=recon_val_orig.view(-1),\n",
    "            y_true_batch=target_val_orig.view(-1),\n",
    "            z_norm_batch=z_norm,\n",
    "        )\n",
    "\n",
    "        relative_mae_value = agg_stats.median_relative_mae()\n",
    "        worst_relative_mae_value = agg_stats.worst_median_relative_mae()\n",
    "\n",
    "        r2_value = agg_stats.median_r2()\n",
    "        worst_r2_value = agg_stats.worst_median_r2()\n",
    "\n",
    "        z_norm_mean, z_norm_std = agg_stats.z_norm_mean_std()\n",
    "\n",
    "        return (\n",
    "            total_loss,\n",
    "            embedding_loss,\n",
    "            value_loss,\n",
    "            cos_sim_emb,\n",
    "            euclidean_dist_emb,\n",
    "            relative_mae_value,\n",
    "            worst_relative_mae_value,\n",
    "            r2_value,\n",
    "            worst_r2_value,\n",
    "            z_norm_mean,\n",
    "            z_norm_std,\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        if len(batch) == 4:\n",
    "            x, target, scaler, concept_units = batch\n",
    "        elif len(batch) == 3:\n",
    "            x, target, scaler = batch\n",
    "            concept_units = None\n",
    "        else:\n",
    "            x, target = batch\n",
    "            scaler = None\n",
    "            concept_units = None\n",
    "\n",
    "        (\n",
    "            total_loss,\n",
    "            embedding_loss,\n",
    "            value_loss,\n",
    "            cos_sim_emb,\n",
    "            euclidean_dist_emb,\n",
    "            relative_mae_value,\n",
    "            worst_relative_mae_value,\n",
    "            r2_value,\n",
    "            worst_r2_value,\n",
    "            z_norm_mean,\n",
    "            z_norm_std,\n",
    "        ) = self.compute_losses(x, target, scaler, concept_units, train=True)\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\", total_loss, prog_bar=True, batch_size=self.hparams.batch_size\n",
    "        )\n",
    "        # self.log(\"train_overlap_loss\", overlap_loss, prog_bar=True, batch_size=self.hparams.batch_size)\n",
    "        self.log(\n",
    "            \"train_embedding_loss\", embedding_loss, batch_size=self.hparams.batch_size\n",
    "        )\n",
    "        self.log(\"train_value_loss\", value_loss, batch_size=self.hparams.batch_size)\n",
    "        self.log(\n",
    "            \"train_embedding_cos_sim\", cos_sim_emb, batch_size=self.hparams.batch_size\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_embedding_euclidean\",\n",
    "            euclidean_dist_emb,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_value_relative_mae_running\",\n",
    "            relative_mae_value,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_worst_value_relative_mae_running\",\n",
    "            worst_relative_mae_value,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "        )\n",
    "        self.log(\"train_value_r2_running\", r2_value, batch_size=self.hparams.batch_size)\n",
    "        self.log(\n",
    "            \"train_worst_value_r2_running\",\n",
    "            worst_r2_value,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "        )\n",
    "        self.log(\"train_z_norm_mean\", z_norm_mean, batch_size=self.hparams.batch_size)\n",
    "        self.log(\"train_z_norm_std\", z_norm_std, batch_size=self.hparams.batch_size)\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss_epoch\",\n",
    "            total_loss,\n",
    "            prog_bar=True,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            logger=True,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "        )\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        if len(batch) == 4:\n",
    "            x, target, scaler, concept_units = batch\n",
    "        elif len(batch) == 3:\n",
    "            x, target, scaler = batch\n",
    "            concept_units = None\n",
    "        else:\n",
    "            x, target = batch\n",
    "            scaler = None\n",
    "            concept_units = None\n",
    "\n",
    "        (\n",
    "            total_loss,\n",
    "            embedding_loss,\n",
    "            value_loss,\n",
    "            cos_sim_emb,\n",
    "            euclidean_dist_emb,\n",
    "            relative_mae_value,\n",
    "            worst_relative_mae_value,\n",
    "            r2_value,\n",
    "            worst_r2_value,\n",
    "            z_norm_mean,\n",
    "            z_norm_std,\n",
    "        ) = self.compute_losses(x, target, scaler, concept_units, train=False)\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\", total_loss, prog_bar=True, batch_size=self.hparams.batch_size\n",
    "        )\n",
    "        # self.log(\"val_overlap_loss\", overlap_loss, prog_bar=True, batch_size=self.hparams.batch_size)\n",
    "        self.log(\n",
    "            \"val_embedding_loss\", embedding_loss, batch_size=self.hparams.batch_size\n",
    "        )\n",
    "        self.log(\"val_value_loss\", value_loss, batch_size=self.hparams.batch_size)\n",
    "        self.log(\n",
    "            \"val_embedding_cos_sim\", cos_sim_emb, batch_size=self.hparams.batch_size\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_embedding_euclidean\",\n",
    "            euclidean_dist_emb,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_value_relative_mae_running\",\n",
    "            relative_mae_value,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_worst_value_relative_mae_running\",\n",
    "            worst_relative_mae_value,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "        )\n",
    "        self.log(\"val_value_r2_running\", r2_value, batch_size=self.hparams.batch_size)\n",
    "        self.log(\n",
    "            \"val_worst_value_r2_running\",\n",
    "            worst_r2_value,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "        )\n",
    "        self.log(\"val_z_norm_mean\", z_norm_mean, batch_size=self.hparams.batch_size)\n",
    "        self.log(\"val_z_norm_std\", z_norm_std, batch_size=self.hparams.batch_size)\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss_epoch\",\n",
    "            total_loss,\n",
    "            prog_bar=True,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            logger=True,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "        )\n",
    "        return total_loss\n",
    "\n",
    "    # Note: PyTorch Lightning doesn't support logging from `on_train_epoch_start`. Use `on_train_epoch_end` for logging, instead.\n",
    "    def on_train_epoch_start(self):\n",
    "        self._agg_train_stats.reset()\n",
    "\n",
    "        print(\"Current LR: \", self.get_current_lr())\n",
    "\n",
    "    def on_validation_start(self):\n",
    "        self._agg_val_stats.reset()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        # Log learning rate of first param group\n",
    "        current_lr = self.get_current_lr()\n",
    "        self.log(\"lr_adjusted\", current_lr, prog_bar=True)\n",
    "\n",
    "    def get_current_lr(self):\n",
    "        current_lr = self.trainer.optimizers[0].param_groups[0][\"lr\"]\n",
    "        return current_lr\n",
    "\n",
    "    # def configure_optimizers(self):\n",
    "    #     return torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "        )\n",
    "\n",
    "        # Use CosineAnnealingLR with T_max=15 and eta_min=1e-6 (matches your 15 epochs)\n",
    "        scheduler = CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=self.hparams.lr_annealing_epochs,\n",
    "            eta_min=self.hparams.min_lr,\n",
    "        )\n",
    "\n",
    "        # TODO: Replace scheduler with CosineAnnealingWarmRestarts(optimizer, T_0=15, T_mult=1)?\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccdb991-b887-4847-968b-8d9fb4b9517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning\n",
    "\n",
    "# import os\n",
    "# import optuna\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "# from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from torch.utils.data import DataLoader\n",
    "# from utils.pytorch import get_device\n",
    "\n",
    "# device = get_device()\n",
    "\n",
    "# # === CONFIG ===\n",
    "# OUTPUT_PATH = \"data/stage1\"\n",
    "# os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "# OPTUNA_DB_PATH = os.path.join(OUTPUT_PATH, \"optuna_study.db\")\n",
    "# EPOCHS = 3\n",
    "# PATIENCE = 5\n",
    "# VAL_SPLIT = 0.2\n",
    "\n",
    "# def objective(trial):\n",
    "#     batch_size = trial.suggest_int(\"batch_size\", 8, 64, step=8)\n",
    "#     lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "#     latent_dim = trial.suggest_int(\"latent_dim\", 32, 128, step=32)\n",
    "#     dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.2, step=0.1)\n",
    "#     weight_decay = trial.suggest_float(\"weight_decay\", 1e-8, 1e-4, log=True)\n",
    "#     gradient_clip = trial.suggest_float(\"gradient_clip\", 0.0, 1.0, step=0.1)\n",
    "\n",
    "#     # # 80/20 Train/Val Split\n",
    "#     # split = int(len(scaled_tuples) * (1 - VAL_SPLIT))\n",
    "#     # train_data = scaled_tuples[:split]\n",
    "#     # val_data = scaled_tuples[split:]\n",
    "\n",
    "#      # === Sample Subset for Faster Debugging ===\n",
    "#     SAMPLE_SIZE = 500_000\n",
    "#     subset = scaled_tuples[:SAMPLE_SIZE]\n",
    "\n",
    "#     # 80/20 Train/Val Split\n",
    "#     split = int(len(subset) * (1 - VAL_SPLIT))\n",
    "#     train_data = subset[:split]\n",
    "#     val_data = subset[split:]\n",
    "\n",
    "#     train_loader = DataLoader(\n",
    "#         ConceptValueDataset(train_data, embedding_map, device=device, value_noise_std=0.005, train=True),\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=True\n",
    "#     )\n",
    "\n",
    "#     val_loader = DataLoader(\n",
    "#         ConceptValueDataset(val_data, embedding_map, device=device, value_noise_std=0.00, train=False),\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=False\n",
    "#     )\n",
    "\n",
    "#     input_dim = len(next(iter(embedding_map.values()))) + 1\n",
    "\n",
    "#     model = Stage1Autoencoder(\n",
    "#         input_dim=input_dim,\n",
    "#         latent_dim=latent_dim,\n",
    "#         dropout_rate=dropout_rate,\n",
    "#         lr=lr,\n",
    "#         batch_size=batch_size,\n",
    "#         weight_decay=weight_decay,\n",
    "#         gradient_clip=gradient_clip\n",
    "#     )\n",
    "\n",
    "#     early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, verbose=True, mode=\"min\")\n",
    "\n",
    "#     model_checkpoint = ModelCheckpoint(\n",
    "#         dirpath=OUTPUT_PATH,\n",
    "#         filename=\"best_model_trial_{trial.number}\",\n",
    "#         monitor=\"val_loss\",\n",
    "#         mode=\"min\",\n",
    "#         save_top_k=1,\n",
    "#         verbose=True\n",
    "#     )\n",
    "\n",
    "#     trainer = pl.Trainer(\n",
    "#         max_epochs=EPOCHS,\n",
    "#         logger=TensorBoardLogger(OUTPUT_PATH, name=\"stage1_autoencoder\"),\n",
    "#         callbacks=[early_stop_callback, model_checkpoint],\n",
    "#         accelerator=\"auto\",\n",
    "#         devices=1,\n",
    "#         gradient_clip_val=gradient_clip\n",
    "#     )\n",
    "\n",
    "#     trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "#     return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "# # === Optuna Study ===\n",
    "# study = optuna.create_study(direction=\"minimize\",\n",
    "#                             storage=f\"sqlite:///{OPTUNA_DB_PATH}\",\n",
    "#                             load_if_exists=True)\n",
    "# study.optimize(objective, n_trials=25)\n",
    "\n",
    "# print(\"Best params:\", study.best_params)\n",
    "# print(\"Best trial value:\", study.best_trial.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c278a0d0-19d7-47ca-95ba-5d972553b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Instantiate dataset\n",
    "# dataset = ConceptValueDataset(scaled_tuples, embedding_map)\n",
    "\n",
    "# # Sample inspection\n",
    "# sample_x, sample_y = dataset[0]\n",
    "# print(\"Sample input:\", sample_x)\n",
    "# print(\"Min:\", sample_x.min().item(), \"Max:\", sample_x.max().item())\n",
    "# print(\"Mean:\", sample_x.mean().item(), \"Std:\", sample_x.std().item())\n",
    "# print(\"Input dim:\", sample_x.shape[0], \"Target dim:\", sample_y.shape[0])\n",
    "\n",
    "# # Optional: test batch loading\n",
    "# loader = DataLoader(dataset, batch_size=4)\n",
    "# for xb, yb in loader:\n",
    "#     print(\"Batch shape:\", xb.shape)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c250f332-069a-4a69-af69-fc73b521529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = list(scaled_tuples)\n",
    "# val_data = train_data\n",
    "\n",
    "# # TODO: Get rid of most of this and just evaluate on the full train_data set\n",
    "\n",
    "# from collections import defaultdict\n",
    "# from sklearn.utils import shuffle\n",
    "# from sklearn.model_selection import train_test_split # Make sure this is imported\n",
    "# import numpy as np # For potential use, though train_test_split handles counts\n",
    "\n",
    "# # Target fraction for the validation set from each group\n",
    "# VALIDATION_FRACTION = 0.25\n",
    "# RANDOM_STATE = 42\n",
    "\n",
    "# # Assume `scaled_tuples` is the list of (concept, unit, scaled_value)\n",
    "# # generated by your previous preprocessing script. It contains ALL your scaled data.\n",
    "\n",
    "# # 1. Training data will be 100% of scaled_tuples.\n",
    "# # We create a list copy for clarity, though direct use is also possible.\n",
    "# # The DataLoader for training will typically shuffle this.\n",
    "# train_data = list(scaled_tuples)\n",
    "\n",
    "# # 2. Validation data will be a stratified sample, aiming for ~25% of each group's items.\n",
    "# val_data = []\n",
    "\n",
    "# # Group all tuples by (concept, unit) to perform stratified sampling for the validation set.\n",
    "# # This grouping is based on the full scaled_tuples list.\n",
    "# grouped_for_val_sampling = defaultdict(list)\n",
    "# for concept, unit, value in scaled_tuples: # Iterate over the full dataset\n",
    "#     grouped_for_val_sampling[(concept, unit)].append(value)\n",
    "\n",
    "# # For each group, select a VALIDATION_FRACTION sample of its values to contribute to val_data.\n",
    "# # These samples will also be part of the 100% train_data.\n",
    "# for (concept, unit), values_in_group in grouped_for_val_sampling.items():\n",
    "#     if not values_in_group: # Skip if group is empty\n",
    "#         continue\n",
    "\n",
    "#     num_samples_in_group = len(values_in_group)\n",
    "#     group_val_values = [] # To store validation values from this specific group\n",
    "\n",
    "#     if num_samples_in_group == 1:\n",
    "#         # If a group has only one sample, and we want validation data (VALIDATION_FRACTION > 0),\n",
    "#         # we include this single sample in the validation set.\n",
    "#         # This means all single-sample groups will be represented in this stratified validation set.\n",
    "#         if VALIDATION_FRACTION > 0:\n",
    "#             group_val_values = list(values_in_group) # Take the single value\n",
    "#     elif num_samples_in_group > 1:\n",
    "#         # For groups with more than one sample, use train_test_split to get\n",
    "#         # the desired fraction for the validation set from this group.\n",
    "#         # The '_group_train_dummy' part is not used for constructing val_data here.\n",
    "#         # We ensure shuffling before split if not done by train_test_split, but it shuffles by default.\n",
    "#         _group_train_dummy, sampled_val_values_for_group = train_test_split(\n",
    "#             values_in_group,       # Values from the current (concept, unit) group\n",
    "#             test_size=VALIDATION_FRACTION,\n",
    "#             random_state=RANDOM_STATE,\n",
    "#             shuffle=True          # Ensure shuffling for random sampling within the group\n",
    "#         )\n",
    "#         group_val_values = sampled_val_values_for_group\n",
    "\n",
    "#     # Extend the global val_data list with the (concept, unit, value) tuples from this group's sample\n",
    "#     if group_val_values: # If any values were selected for validation from this group\n",
    "#         val_data.extend([(concept, unit, v) for v in group_val_values])\n",
    "\n",
    "# print(f\"Total items for training (100%): {len(train_data)}\")\n",
    "# print(f\"Total items for validation (stratified ~25% sample): {len(val_data)}\")\n",
    "\n",
    "# # Now you have:\n",
    "# # - `train_data`: A list of all your (concept, unit, scaled_value) tuples (100%).\n",
    "# # - `val_data`: A list containing a stratified sample, where each (concept, unit) group\n",
    "# #               contributes approximately 25% of its items to this validation set.\n",
    "# #               The total size will be roughly 25% of the original dataset, but may vary\n",
    "# #               slightly due to per-group integer rounding and handling of small groups.\n",
    "\n",
    "# # You would then use `train_data` and `val_data` in your PyTorch DataLoaders.\n",
    "# # The `train_loader` will use `train_data` (100%) with shuffle=True.\n",
    "# # The `val_loader` will use `val_data` (the ~25% stratified sample) with shuffle=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c5e738-ff3a-4e99-bdfb-0479288faed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging, only\n",
    "\n",
    "# Example: keep only 10,000 training and 2,000 validation samples\n",
    "# train_data = train_data[:100_000]\n",
    "# val_data = val_data[:2_000]\n",
    "# val_data = train_data\n",
    "\n",
    "# print(f\"Truncated train_data: {len(train_data)}\")\n",
    "# print(f\"Truncated val_data: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd4f758-e1d6-4e8b-bbb2-0060ffbdadc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from utils.pytorch import get_device # Assuming you have this\n",
    "\n",
    "# device = get_device() # Make sure device is defined\n",
    "\n",
    "# # Define Median Scaled Value (which is 0.0 after RobustScaler)\n",
    "# median_scaled_val_tensor = torch.tensor(0.0, dtype=torch.float32, device=device)\n",
    "# print(f\"Using median_scaled_val: {median_scaled_val_tensor.item()}\")\n",
    "\n",
    "# # === Calculate Mean Embedding (mean_emb) ===\n",
    "# # (Calculation for mean_emb_tensor remains the same as before)\n",
    "# # Get all unique (concept, unit) keys from train_data\n",
    "# train_keys = set((item[0], item[1]) for item in train_data)\n",
    "# # Get the corresponding embeddings\n",
    "# train_embeddings = [embedding_map[key] for key in train_keys if key in embedding_map]\n",
    "\n",
    "# if not train_embeddings:\n",
    "#     raise ValueError(\"No valid embeddings found for training data keys!\")\n",
    "\n",
    "# # Stack embeddings into a numpy array and calculate the mean vector\n",
    "# train_embeddings_np = np.stack(train_embeddings)\n",
    "# mean_emb_numpy = np.mean(train_embeddings_np, axis=0)\n",
    "# # Convert to a tensor on the correct device\n",
    "# mean_emb_tensor = torch.tensor(mean_emb_numpy, dtype=torch.float32, device=device)\n",
    "# print(f\"Calculated mean_emb shape: {mean_emb_tensor.shape}\")\n",
    "# print(f\"Calculated mean_emb value (first 10 elements): {mean_emb_tensor[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47484ee4-1c73-48be-8024-e368d977c014",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:150\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:320\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    319\u001b[39m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m     batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:192\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m result = closure.consume_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:270\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/pytorch_lightning/core/module.py:1302\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1278\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1279\u001b[39m \u001b[33;03mthe optimizer.\u001b[39;00m\n\u001b[32m   1280\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1300\u001b[39m \n\u001b[32m   1301\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1302\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py:154\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:239\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py:123\u001b[39m, in \u001b[36mPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m closure = partial(\u001b[38;5;28mself\u001b[39m._wrap_closure, model, optimizer, closure)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:140\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    139\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/torch/optim/adam.py:244\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    235\u001b[39m         group,\n\u001b[32m    236\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    241\u001b[39m         state_steps,\n\u001b[32m    242\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/torch/optim/adam.py:876\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    874\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m876\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/torch/optim/adam.py:425\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[39m\n\u001b[32m    423\u001b[39m exp_avg.lerp_(grad, \u001b[32m1\u001b[39m - device_beta1)\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m \u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     68\u001b[39m model_checkpoint = ModelCheckpoint(\n\u001b[32m     69\u001b[39m     dirpath=OUTPUT_PATH,\n\u001b[32m     70\u001b[39m     filename=\u001b[33m\"\u001b[39m\u001b[33mstage1_resume\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     74\u001b[39m     verbose=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     75\u001b[39m )\n\u001b[32m     77\u001b[39m trainer = pl.Trainer(\n\u001b[32m     78\u001b[39m     max_epochs=EPOCHS,\n\u001b[32m     79\u001b[39m     logger=TensorBoardLogger(OUTPUT_PATH, name=\u001b[33m\"\u001b[39m\u001b[33mstage1_autoencoder\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     83\u001b[39m     gradient_clip_val=gradient_clip,\n\u001b[32m     84\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ckpt_path=ckpt_path # TODO: Uncomment if resuming training AND wanting to restore existing model configuration\u001b[39;49;00m\n\u001b[32m     92\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rust-sec-fetcher/python/narrative_stack/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:65\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[32m     64\u001b[39m         launcher.kill(_get_sigkill_signal())\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mexit\u001b[49m(\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n",
      "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "import os\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.pytorch import get_device\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "# === CONFIG ===\n",
    "OUTPUT_PATH = \"data/stage1_23_(no_pre_dedupe)\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "OPTUNA_DB_PATH = os.path.join(OUTPUT_PATH, \"optuna_study.db\")\n",
    "EPOCHS = 1000\n",
    "PATIENCE = 20  # Compensate for long annealing period + some\n",
    "\n",
    "# ckpt_path = f\"{OUTPUT_PATH}/stage1_resume-v8.ckpt\"\n",
    "# ckpt_path = f\"{OUTPUT_PATH}/manual_resumed_checkpoint.ckpt\"\n",
    "ckpt_path = None\n",
    "\n",
    "# model = Stage1Autoencoder.load_from_checkpoint(ckpt_path,\n",
    "#     # lr=5e-5,\n",
    "#     lr=2.5e-6,\n",
    "#     # min_lr=1e-6,\n",
    "#     min_lr=1e-7\n",
    "# )\n",
    "model = Stage1Autoencoder()\n",
    "\n",
    "batch_size = model.hparams.batch_size\n",
    "gradient_clip = model.hparams.gradient_clip\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     ConceptValueDataset(train_data, embedding_map, device=device, value_noise_std=0.005, train=True),\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# val_loader = DataLoader(\n",
    "#     ConceptValueDataset(val_data, embedding_map, device=device, value_noise_std=0.00, train=False),\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False\n",
    "# )\n",
    "train_loader = DataLoader(\n",
    "    IterableConceptValueDataset(WEBSOCKET_ADDRESS, internal_batch_size=batch_size, return_scaler=True, shuffle=True),\n",
    "    batch_size=batch_size,\n",
    "    # shuffle=True,\n",
    "    collate_fn=collate_with_scaler,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    IterableConceptValueDataset(WEBSOCKET_ADDRESS, internal_batch_size=batch_size, return_scaler=True, shuffle=False),\n",
    "    batch_size=batch_size,\n",
    "    # shuffle=False,\n",
    "    collate_fn=collate_with_scaler,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "# input_dim = len(next(iter(embedding_map.values()))) + 1\n",
    "sample_embedding = us_gaap_store.lookup_by_index(0)[\"embedding\"]\n",
    "input_dim = sample_embedding.shape[0]\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss_epoch\", patience=PATIENCE, verbose=True, mode=\"min\"\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    dirpath=OUTPUT_PATH,\n",
    "    filename=\"stage1_resume\",\n",
    "    monitor=\"val_loss_epoch\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    logger=TensorBoardLogger(OUTPUT_PATH, name=\"stage1_autoencoder\"),\n",
    "    callbacks=[early_stop_callback, model_checkpoint],\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    gradient_clip_val=gradient_clip,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    #\n",
    "    # ckpt_path=ckpt_path # TODO: Uncomment if resuming training AND wanting to restore existing model configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5769c4-97ea-4638-a2bc-de631ce1f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_checkpoint(f\"{OUTPUT_PATH}/manual_resumed_checkpoint.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca87a09-65c9-42ee-9310-42558e6df0dc",
   "metadata": {},
   "source": [
    "# Conceptual Draft\n",
    "\n",
    "Stage 1 learns semantic+quantitative embeddings for individual concept/unit/value triplets.\n",
    "\n",
    "Stage 2 learns how to aggregate and contextualize those embeddings into higher-order units (i.e., financial statements).\n",
    "\n",
    "Stage 3 learns how to model temporal dynamics and structural evolution across filings — a full hierarchy of understanding.\n",
    "\n",
    "This pipeline could encode an entire company's financial narrative into vector space.\n",
    "\n",
    "It’s structured like language modeling, but for accounting — and that’s what makes it potentially groundbreaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70426775-f40b-4b32-ac8f-2957ae841798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# # Where correlation matrix is on the full z, and the `corr_value` is derived specifically from the input value dimension\n",
    "# # IMPORTANT: This should only be used with this \"stage 1\" model\n",
    "# def analyze_latent_correlation_matrix_streaming(model, val_loader, device):\n",
    "#     model.eval()\n",
    "#     model.to(device)\n",
    "\n",
    "#     latent_dim = model.hparams.latent_dim\n",
    "\n",
    "#     count = 0\n",
    "#     mean_z = np.zeros(latent_dim)\n",
    "#     m2_z = np.zeros(latent_dim)\n",
    "#     cov_z = np.zeros((latent_dim, latent_dim))\n",
    "\n",
    "#     # For value correlation\n",
    "#     mean_val = 0.0\n",
    "#     m2_val = 0.0\n",
    "#     cov_val = np.zeros(latent_dim)\n",
    "\n",
    "#     for batch in val_loader:\n",
    "#         x, y, _ = batch\n",
    "#         x = x.to(device)\n",
    "#         y = y.to(device)\n",
    "\n",
    "#         z = model.encode(x).detach().cpu().numpy()  # [B, D]\n",
    "#         v = y[:, -1].detach().cpu().numpy()         # [B]\n",
    "\n",
    "#         for zi, vi in zip(z, v):\n",
    "#             count += 1\n",
    "\n",
    "#             # === Update latent stats (Welford) ===\n",
    "#             delta_z = zi - mean_z\n",
    "#             mean_z += delta_z / count\n",
    "#             m2_z += delta_z * (zi - mean_z)\n",
    "\n",
    "#             # === Update cov_z (outer product) ===\n",
    "#             cov_z += np.outer(delta_z, zi - mean_z)\n",
    "\n",
    "#             # === Update value stats ===\n",
    "#             delta_v = vi - mean_val\n",
    "#             mean_val += delta_v / count\n",
    "#             m2_val += delta_v * (vi - mean_val)\n",
    "\n",
    "#             # === Update cov_val ===\n",
    "#             cov_val += delta_z * (vi - mean_val)\n",
    "\n",
    "#         # break\n",
    "\n",
    "#     var_z = m2_z / (count - 1)\n",
    "#     var_val = m2_val / (count - 1)\n",
    "#     cov_z /= (count - 1)\n",
    "#     cov_val /= (count - 1)\n",
    "\n",
    "#     std_z = np.sqrt(var_z + 1e-8)\n",
    "#     std_val = np.sqrt(var_val + 1e-8)\n",
    "\n",
    "#     corr_matrix = cov_z / (std_z[:, None] * std_z[None, :])\n",
    "#     corr_value = cov_val / (std_z * std_val)\n",
    "\n",
    "#     return corr_matrix, corr_value\n",
    "\n",
    "\n",
    "# print(\"\\n=== Computing latent correlation matrix... ===\")\n",
    "# corr_matrix, corr_value = analyze_latent_correlation_matrix_streaming(model, val_loader, device=device)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_correlation_matrix(corr_matrix, title=\"Latent Dimension Correlation Matrix\"):\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     plt.imshow(corr_matrix, cmap='coolwarm', interpolation='nearest', vmin=-1, vmax=1)\n",
    "#     plt.colorbar(shrink=0.5)\n",
    "#     plt.title(title)\n",
    "#     plt.xlabel(\"Latent Dim\")\n",
    "#     plt.ylabel(\"Latent Dim\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# plot_correlation_matrix(corr_matrix)\n",
    "\n",
    "# top_k = 128\n",
    "\n",
    "# def print_top_latent_correlations(corr_matrix, top_k=top_k):\n",
    "#     dim = corr_matrix.shape[0]\n",
    "#     pairs = []\n",
    "\n",
    "#     for i in range(dim):\n",
    "#         for j in range(i + 1, dim):\n",
    "#             corr = corr_matrix[i, j]\n",
    "#             pairs.append(((i, j), corr))\n",
    "\n",
    "#     top_corrs = sorted(pairs, key=lambda x: -abs(x[1]))[:top_k]\n",
    "\n",
    "#     print(f\"\\nTop {top_k} most correlated latent dimension pairs:\")\n",
    "#     for (i, j), corr in top_corrs:\n",
    "#         print(f\"z[{i:03d}] ↔ z[{j:03d}]: corr = {corr:.4f}\")\n",
    "\n",
    "\n",
    "# print_top_latent_correlations(corr_matrix, top_k=top_k)\n",
    "\n",
    "\n",
    "# top_dims = sorted(enumerate(corr_value), key=lambda x: -abs(x[1]))[:top_k]\n",
    "# print(\"\\nTop latent dimensions most correlated with scaled value:\")\n",
    "# for i, c in top_dims:\n",
    "#     print(f\"z[{i:03d}]: corr = {c:.4f}\")\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(12, 4))\n",
    "# plt.plot(np.sort(np.abs(corr_value))[::-1], marker='o')\n",
    "# plt.title(\"Absolute Correlation of Latent Dims with Value\")\n",
    "# plt.xlabel(\"Sorted Latent Dimension\")\n",
    "# plt.ylabel(\"Absolute Correlation\")\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e1ffb0-31c4-4e71-9fc0-3411adf06d74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
