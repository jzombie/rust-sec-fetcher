{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08735a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import simd_r_drive_server_config\n",
    "from us_gaap_store import UsGaapStore\n",
    "\n",
    "from simd_r_drive_ws_client import DataStoreWsClient\n",
    "\n",
    "data_store = DataStoreWsClient(simd_r_drive_server_config.host, simd_r_drive_server_config.port)\n",
    "us_gaap_store = UsGaapStore(data_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10d741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_store.file_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5154e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pytorch.narrative_stack.stage1.dataset import IterableConceptValueDataset, collate_with_scaler\n",
    "from models.pytorch.narrative_stack.stage1 import Stage1Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df92beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Iterator, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm  # purely for a progress bar\n",
    "from utils.pytorch import get_device\n",
    "\n",
    "# ── project imports ────────────────────────────────────────────────────────────\n",
    "from config import project_paths, simd_r_drive_server_config\n",
    "from models.pytorch.narrative_stack.stage1.dataset import IterableConceptValueDataset, collate_with_scaler\n",
    "from models.pytorch.narrative_stack.stage1 import Stage1Autoencoder\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    ")\n",
    "\n",
    "# ── constants ─────────────────────────────────────────────────────────────────\n",
    "CKPT = (\n",
    "    Path(project_paths.python_data)\n",
    "    / \"stage1_23_(no_pre_dedupe)\"\n",
    "    / \"stage1_resume-v10.ckpt\"\n",
    ")\n",
    "BATCH_SIZE_OVERRIDE = None  # e.g. 128 to force smaller GPU batches\n",
    "NUM_WORKERS = 2\n",
    "DEVICE = get_device()\n",
    "\n",
    "\n",
    "# ── helpers ───────────────────────────────────────────────────────────────────\n",
    "def inverse_batch(\n",
    "    scaled_vals: torch.Tensor, scalers: List  # list[sklearn Scaler]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Per-sample inverse_transform; vectorized enough to stay fast.\n",
    "    \"\"\"\n",
    "    scaled_np = scaled_vals.detach().cpu().numpy()\n",
    "    return np.stack(\n",
    "        [s.inverse_transform(v.reshape(-1, 1)).ravel() for s, v in zip(scalers, scaled_np)]\n",
    "    )\n",
    "\n",
    "\n",
    "def build_loader() -> DataLoader:\n",
    "    ds = IterableConceptValueDataset(\n",
    "        simd_r_drive_server_config,\n",
    "        internal_batch_size=64,\n",
    "        return_scaler=True,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    bs = BATCH_SIZE_OVERRIDE or Stage1Autoencoder.load_from_checkpoint(CKPT).hparams.batch_size\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=bs,\n",
    "        collate_fn=collate_with_scaler,\n",
    "        pin_memory=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def inference_stream() -> Iterator[List[dict]]:\n",
    "    \"\"\"\n",
    "    Yields one *batch* (list of dicts) at a time.\n",
    "    \"\"\"\n",
    "    model = Stage1Autoencoder.load_from_checkpoint(CKPT).to(DEVICE).eval()\n",
    "    loader = build_loader()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, scalers, concept_units in tqdm(loader, desc=\"infer\"):\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            recon_emb, recon_val, z = model(x)\n",
    "\n",
    "            # Inverse‑scale predicted & true values\n",
    "            pred_orig = inverse_batch(recon_val, scalers)\n",
    "            true_orig = inverse_batch(y[:, -1:], scalers)\n",
    "\n",
    "            # Move to CPU once, keep contiguous\n",
    "            z_cpu       = z.cpu().numpy()\n",
    "            pred_scaled = recon_val.cpu().numpy()\n",
    "            true_scaled = y[:, -1:].cpu().numpy()\n",
    "\n",
    "            batch_records = [\n",
    "                {\n",
    "                    \"concept\": concept,\n",
    "                    \"uom\": uom,\n",
    "                    \"pred_val_scaled\": float(pred_scaled[i, 0]),\n",
    "                    \"true_val_scaled\": float(true_scaled[i, 0]),\n",
    "                    \"pred_val_orig\": float(pred_orig[i, 0]),\n",
    "                    \"true_val_orig\": float(true_orig[i, 0]),\n",
    "                    \"latent\": z_cpu[i],\n",
    "                }\n",
    "                for i, (concept, uom) in enumerate(concept_units)\n",
    "            ]\n",
    "\n",
    "            yield batch_records\n",
    "\n",
    "\n",
    "# ── main ──────────────────────────────────────────────────────────────────────\n",
    "# if __name__ == \"__main__\":\n",
    "for batch in inference_stream():\n",
    "    print(batch)\n",
    "\n",
    "    # TODO: Remove\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48471d0c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
